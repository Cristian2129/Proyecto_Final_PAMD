{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a00643f-51d0-49ec-9aaa-0a6ac15581a1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Fase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28ef8e94-16fd-4be8-bf98-8cd5a51ae9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f489ff4e-b661-45da-a176-ee1ac13ef7fc;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.3 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "\tfound io.delta#delta-spark_2.12;3.0.0 in central\n",
      "\tfound io.delta#delta-storage;3.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 380ms :: artifacts dl 15ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tio.delta#delta-spark_2.12;3.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.0.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   14  |   0   |   0   |   0   ||   14  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-f489ff4e-b661-45da-a176-ee1ac13ef7fc\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 14 already retrieved (0kB/9ms)\n",
      "25/12/11 20:29:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/11 20:29:37 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Spark 3.5.1 iniciado\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FASE 2 - OPTIMIZADO PARA SPARK 3.5.1 + DELTA LAKE 3.0\n",
    "# ============================================================================\n",
    "\n",
    "# PASO 0: REINICIAR SPARK CON VERSIONES CORRECTAS\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "import time\n",
    "time.sleep(3)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, lower, regexp_replace, translate, length, trim\n",
    "\n",
    "from pyspark.ml.feature import (\n",
    "    Tokenizer, StopWordsRemover, Word2Vec, \n",
    "    StringIndexer, OneHotEncoder, VectorAssembler,\n",
    "    StandardScaler, PCA\n",
    ")\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.stat import Correlation\n",
    "import numpy as np\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Bronze_to_Silver_Optimized\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1,\"\n",
    "            \"io.delta:delta-spark_2.12:3.0.0\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config(\"spark.executor.memory\", \"4g\")\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"50\")\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "print(f\" Spark {spark.version} iniciado\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fd17a2e-563a-48c1-8f15-3c3ec7900ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PASO 1: LECTURA DE KAFKA\n",
      "================================================================================\n",
      "\n",
      "Leyendo Kafka...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 52:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mensajes: 100,698\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 1. LECTURA DE KAFKA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 1: LECTURA DE KAFKA\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "contract_schema = StructType([\n",
    "    StructField(\"id_contrato\", StringType()),\n",
    "    StructField(\"objeto_contrato\", StringType()),\n",
    "    StructField(\"entidad\", StringType()),\n",
    "    StructField(\"departamento\", StringType()),\n",
    "    StructField(\"municipio\", StringType()),\n",
    "    StructField(\"region\", StringType()),\n",
    "    StructField(\"codigo_unspsc\", StringType()),\n",
    "    StructField(\"descripcion_categoria\", StringType()),\n",
    "    StructField(\"valor_contrato\", DoubleType()),\n",
    "    StructField(\"duracion_dias\", IntegerType()),\n",
    "    StructField(\"fecha_firma\", StringType()),\n",
    "    StructField(\"tipo_contrato\", StringType()),\n",
    "    StructField(\"estado_contrato\", StringType()),\n",
    "    StructField(\"modalidad\", StringType()),\n",
    "    StructField(\"anno\", IntegerType()),\n",
    "    StructField(\"id_interno_sistema\", StringType()),\n",
    "    StructField(\"campo_vacio\", StringType()),\n",
    "    StructField(\"constante_1\", StringType()),\n",
    "    StructField(\"constante_2\", IntegerType()),\n",
    "    StructField(\"duplicate_id\", StringType()),\n",
    "    StructField(\"timestamp_carga\", StringType())\n",
    "])\n",
    "\n",
    "print(\"Leyendo Kafka...\")\n",
    "\n",
    "df_kafka = spark.read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", \"contratos-publicos\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "df_bronze = df_kafka.select(\n",
    "    from_json(col(\"value\").cast(\"string\"), contract_schema).alias(\"data\")\n",
    ").select(\"data.*\")\n",
    "\n",
    "df_bronze = df_bronze.cache()\n",
    "total_kafka = df_bronze.count()\n",
    "\n",
    "print(f\" Mensajes: {total_kafka:,}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f952457-ec33-4c40-b285-f2d1e68c3357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PASO 2: ELIMINAR REDUNDANTES Y PREPARAR DATOS\n",
      "================================================================================\n",
      "\n",
      " Eliminando 6 columnas redundantes...\n",
      "Columnas restantes: 15\n",
      "\n",
      " Preparando campo fecha_firma...\n",
      "   Formato recibido: ISO timestamp (2024-01-04T00:00:00.000)\n",
      "   Convirtiendo a: date (2024-01-04)\n",
      " Fecha convertida correctamente\n",
      "\n",
      " Liberando memoria de df_bronze...\n",
      " Memoria liberada\n",
      "\n",
      "================================================================================\n",
      " Dataset preparado: 15 columnas\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 2. ELIMINAR REDUNDANTES Y PREPARAR DATOS\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 2: ELIMINAR REDUNDANTES Y PREPARAR DATOS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Columnas redundantes a eliminar\n",
    "redundant_columns = [\n",
    "    \"id_interno_sistema\",\n",
    "    \"campo_vacio\",\n",
    "    \"constante_1\",\n",
    "    \"constante_2\",\n",
    "    \"duplicate_id\",\n",
    "    \"timestamp_carga\"\n",
    "]\n",
    "\n",
    "print(f\" Eliminando {len(redundant_columns)} columnas redundantes...\")\n",
    "df_cleaned = df_bronze.drop(*redundant_columns)\n",
    "\n",
    "print(f\"Columnas restantes: {len(df_cleaned.columns)}\")\n",
    "print()\n",
    "\n",
    "print(\" Preparando campo fecha_firma...\")\n",
    "print(\"   Formato recibido: ISO timestamp (2024-01-04T00:00:00.000)\")\n",
    "print(\"   Convirtiendo a: date (2024-01-04)\")\n",
    "\n",
    "df_cleaned = (\n",
    "    df_cleaned\n",
    "    .withColumn(\"fecha_firma_temp\", to_timestamp(col(\"fecha_firma\")))\n",
    "    .withColumn(\"fecha_firma\", to_date(col(\"fecha_firma_temp\")))\n",
    "    .drop(\"fecha_firma_temp\")\n",
    ")\n",
    "\n",
    "print(\" Fecha convertida correctamente\\n\")\n",
    "\n",
    "# Liberar bronze ahora que ya no lo necesitamos\n",
    "print(\" Liberando memoria de df_bronze...\")\n",
    "df_bronze.unpersist()\n",
    "print(\" Memoria liberada\\n\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\" Dataset preparado: {len(df_cleaned.columns)} columnas\")\n",
    "print(\"=\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "816b1e7b-d82b-439c-acbf-057230ec288d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PASO 3: LIMPIEZA - PREPARACI√ìN\n",
      "================================================================================\n",
      "\n",
      "Cacheando datos para an√°lisis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 56:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Registros totales: 100,698\n",
      "\n",
      " Columnas: 15\n",
      " Datos cacheados¬†en¬†memoria\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELDA 1: PREPARACI√ìN Y CONTEO INICIAL\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 3: LIMPIEZA - PREPARACI√ìN\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"Cacheando datos para an√°lisis...\")\n",
    "df_cleaned = df_cleaned.cache()\n",
    "total_cleaned = df_cleaned.count()\n",
    "\n",
    "print(f\" Registros totales: {total_cleaned:,}\\n\")\n",
    "print(f\" Columnas: {len(df_cleaned.columns)}\")\n",
    "print(f\" Datos cacheados¬†en¬†memoria\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82037c71-8bfc-4e99-b7fe-f3f24287fe73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "AN√ÅLISIS DE CALIDAD DE DATOS\n",
      "================================================================================\n",
      "\n",
      "Analizando valores nulos en columnas cr√≠ticas...\n",
      "üìä Valores nulos en columnas cr√≠ticas:\n",
      "\n",
      "   ‚ö†  fecha_firma: 695 (0.7%)\n",
      "   ‚ö†  duracion_dias: 50,350 (50.0%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELDA 2: AN√ÅLISIS DE NULOS (OPTIMIZADO)\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"AN√ÅLISIS DE CALIDAD DE DATOS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"Analizando valores nulos en columnas cr√≠ticas...\")\n",
    "\n",
    "# Solo analizar columnas cr√≠ticas para ahorrar memoria\n",
    "critical_columns = [\n",
    "    \"id_contrato\",\n",
    "    \"objeto_contrato\", \n",
    "    \"valor_contrato\",\n",
    "    \"fecha_firma\",\n",
    "    \"entidad\",\n",
    "    \"departamento\",\n",
    "    \"duracion_dias\"\n",
    "]\n",
    "\n",
    "# An√°lisis optimizado solo de columnas cr√≠ticas\n",
    "null_analysis = df_cleaned.select([\n",
    "    count(when(col(c).isNull(), c)).alias(c) \n",
    "    for c in critical_columns if c in df_cleaned.columns\n",
    "]).collect()[0].asDict()\n",
    "\n",
    "print(\"üìä Valores nulos en columnas cr√≠ticas:\\n\")\n",
    "has_nulls = False\n",
    "for col_name in critical_columns:\n",
    "    if col_name in null_analysis:\n",
    "        null_count = null_analysis[col_name]\n",
    "        if null_count > 0:\n",
    "            has_nulls = True\n",
    "            pct = (null_count / total_cleaned) * 100\n",
    "            print(f\"   ‚ö†  {col_name}: {null_count:,} ({pct:.1f}%)\")\n",
    "\n",
    "if not has_nulls:\n",
    "    print(\"   ‚úÖ No hay valores nulos en columnas cr√≠ticas\")\n",
    "\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af77614e-c5ff-4b86-91e2-23ce1d5a253d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "APLICANDO FILTROS DE CALIDAD\n",
      "================================================================================\n",
      "\n",
      "Aplicando reglas de limpieza:\n",
      "  ‚úì id_contrato no nulo\n",
      "  ‚úì objeto_contrato no nulo\n",
      "  ‚úì valor_contrato no nulo y > 0\n",
      "  ‚úì fecha_firma no nula\n",
      "\n",
      "‚úÖ Filtros aplicados correctamente\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELDA 3: APLICAR FILTROS DE LIMPIEZA\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"APLICANDO FILTROS DE CALIDAD\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"Aplicando reglas de limpieza:\")\n",
    "print(\"  ‚úì id_contrato no nulo\")\n",
    "print(\"  ‚úì objeto_contrato no nulo\")\n",
    "print(\"  ‚úì valor_contrato no nulo y > 0\")\n",
    "print(\"  ‚úì fecha_firma no nula\")\n",
    "print()\n",
    "\n",
    "# Aplicar filtros paso a paso\n",
    "# NOTA: fecha_firma ya fue convertida a date en el Paso 2\n",
    "df_silver = df_cleaned \\\n",
    "    .filter(col(\"id_contrato\").isNotNull()) \\\n",
    "    .filter(col(\"objeto_contrato\").isNotNull()) \\\n",
    "    .filter(col(\"valor_contrato\").isNotNull()) \\\n",
    "    .filter(col(\"valor_contrato\") > 0) \\\n",
    "    .filter(col(\"fecha_firma\").isNotNull())\n",
    "\n",
    "print(\"‚úÖ Filtros aplicados correctamente\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b60ee146-c2f4-4f23-ae53-f98ded14fb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.1\n"
     ]
    }
   ],
   "source": [
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "244fd81e-3b09-4646-99b4-d88f61d8abd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FINALIZANDO LIMPIEZA\n",
      "================================================================================\n",
      "\n",
      "Cacheando datos limpios...\n",
      "\n",
      "================================================================================\n",
      "üìä RESUMEN DE LIMPIEZA\n",
      "================================================================================\n",
      "  Registros iniciales:    100,698\n",
      "  Registros finales:      99,458 (98.8%)\n",
      "  Registros descartados:  1,240 (1.2%)\n",
      "================================================================================\n",
      "\n",
      "Liberando memoria del cache anterior...\n",
      "‚úÖ Limpieza¬†completada\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELDA 4: CACHEAR RESULTADOS Y GENERAR REPORTE\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"FINALIZANDO LIMPIEZA\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"Cacheando datos limpios...\")\n",
    "df_silver = df_silver.cache()\n",
    "total_silver = df_silver.count()\n",
    "\n",
    "# Calcular estad√≠sticas\n",
    "registros_descartados = total_cleaned - total_silver\n",
    "pct_retenido = (total_silver / total_cleaned) * 100 if total_cleaned > 0 else 0\n",
    "pct_descartado = (registros_descartados / total_cleaned) * 100 if total_cleaned > 0 else 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä RESUMEN DE LIMPIEZA\")\n",
    "print(\"=\"*80)\n",
    "print(f\"  Registros iniciales:    {total_cleaned:,}\")\n",
    "print(f\"  Registros finales:      {total_silver:,} ({pct_retenido:.1f}%)\")\n",
    "print(f\"  Registros descartados:  {registros_descartados:,} ({pct_descartado:.1f}%)\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Liberar memoria del DataFrame anterior\n",
    "print(\"Liberando memoria del cache anterior...\")\n",
    "df_cleaned.unpersist()\n",
    "print(\"‚úÖ Limpieza¬†completada\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b4f2fef-6d09-419f-baae-b612a0c8eef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 entidades:\n",
      "+-------------------------------------------------+-----+\n",
      "|entidad                                          |count|\n",
      "+-------------------------------------------------+-----+\n",
      "|MUNICIPIO DE SOACHA.                             |6356 |\n",
      "|ALCALD√çA MUNICIPAL COTA                          |3988 |\n",
      "|ESE MUNICIPAL DE SOACHA JULIO CESAR PE√ëALOZA*    |3822 |\n",
      "|CUNDINAMARCA-ALCALDIA MUNICIPIO MOSQUERA         |3759 |\n",
      "|empresa social del estado regi√≥n de salud soacha.|3152 |\n",
      "+-------------------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Top 5 departamentos:\n",
      "+------------+-----+\n",
      "|departamento|count|\n",
      "+------------+-----+\n",
      "|Cundinamarca|99458|\n",
      "+------------+-----+\n",
      "\n",
      "\n",
      "Distribuci√≥n por regi√≥n:\n",
      "+--------------+-----+\n",
      "|region        |count|\n",
      "+--------------+-----+\n",
      "|Centro-Oriente|99458|\n",
      "+--------------+-----+\n",
      "\n",
      "\n",
      "Top 10 c√≥digos UNSPSC:\n",
      "+-------------+-----+\n",
      "|codigo_unspsc|count|\n",
      "+-------------+-----+\n",
      "|             |50058|\n",
      "|V1.80111600  |11391|\n",
      "|V1.80111701  |4329 |\n",
      "|V1.85101600  |2605 |\n",
      "|V1.80111620  |2158 |\n",
      "|V1.80111601  |2092 |\n",
      "|V1.86101710  |1134 |\n",
      "|UNSPECIFIED  |1081 |\n",
      "|V1.80161500  |891  |\n",
      "|V1.80111504  |866  |\n",
      "+-------------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "Top 10 categor√≠as UNSPSC:\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|descripcion_categoria                                                                                                                                                                                                                                                                                       |count|\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|                                                                                                                                                                                                                                                                                                            |50058|\n",
      "|PRESTACI√ìN DE SERVICIOS DE APOYO A LA GESTI√ìN PARA REALIZAR ACTIVIDADES DE ATENCI√ìN PRIMARIA EN SALUD EN EL MUNICIPIO DE SOACHA EN EL MARCO DE LAS NORMAS VIGENTES; MEDIANTE LA IMPLEMENTACI√ìN DE EQUIPOS B√ÅSICOS DE SALUD EN EL CONTEXTO DE LAS ACCIONES DE PROMOCI√ìN MANTENIMIENTO DE LA SALUD Y LA ATENCI|318  |\n",
      "|PRESTACI√ìN DE SERVICIOS PROFESIONALES PARA REALIZAR ACTIVIDADES DE ATENCI√ìN PRIMARIA EN SALUD EN EL MUNICIPIO DE SOACHA EN EL MARCO DE LAS NORMAS VIGENTES; MEDIANTE LA IMPLEMENTACI√ìN DE EQUIPOS B√ÅSICOS DE SALUD EN EL CONTEXTO DE LAS ACCIONES DE PROMOCI√ìN MANTENIMIENTO DE LA SALUD Y LA ATENCI√ìN INDIV|248  |\n",
      "|Contratar la prestaci√≥n de los servicios profesionales de manera aut√≥noma e independiente para impartir formaci√≥n profesional integral en las competencias t√©cnicas; claves y transversales a trav√©s de herramientas pedag√≥gicas en el programa y modalidad asignada garantizando el cumplimiento del proces|173  |\n",
      "|PRESTACI√ìN DE SERVICIOS T√âCNICO ASISTENCIAL PARA REALIZAR ACTIVIDADES DE ATENCI√ìN PRIMARIA EN SALUD EN EL MUNICIPIO DE SOACHA EN EL MARCO DE LAS NORMAS VIGENTES; MEDIANTE LA IMPLEMENTACI√ìN DE EQUIPOS B√ÅSICOS EN SALUD EN EL CONTEXTO DE LAS ACCIONES DE PROMOCI√ìN Y MANTENIMIENTO DE LA SALUD Y LA ATENCI|120  |\n",
      "|Prestar servicios personales de car√°cter temporal para impartir formaci√≥n profesional integral; de forma presencial y/o virtual; as√≠ como otras actividades que se deriven de los diferentes programas; niveles y especialidades impartidas por el centro de tecnolog√≠as del transporte del Sena - Regional |92   |\n",
      "|Prestaci√≥n de servicios profesionales; tecn√≥logo y/o t√©cnico para el desarrollo de las actividades de formaci√≥n del nivel t√©cnico; tecnol√≥gico y/o complementar√≠a del programa Regular; atendiendo las pol√≠ticas institucionales y la normatividad vigente; de acuerdo con las necesidades; la programaci√≥n |87   |\n",
      "|AUNAR ESFUERZOS ENTRE EL DEPARTAMENTO DE CUNDINAMARCA - SECRETAR√çA DE EDUCACI√ìN Y LOS MUNICIPIOS FOCALIZADOS PARA LA ESTRATEGIA DE TRANSPORTE ESCOLAR; DIRIGIDO A ESTUDIANTES DE LAS IED OFICIALES; ASEGURANDO LAS CONDICIONES NECESARIAS PARA SU PERMANENCIA EN EL   SISTEMA EDUCATIVO (SEG√öN ANEXO DISTRIB|72   |\n",
      "|PRESTACI√ìN DE SERVICIOS COMO AUXILIAR DE ENFERMERIA EN EL PROCESO DE EQUIPOS BASICOS EN SALUD DE LA SUBGERENCIA COMUNITARIA DE LA EMPRESA SOCIAL DEL ESTADO REGION DE SALUD SOACHA DE ACUERDO AL REQUERIMIENTO INSTITUCIONAL EN DESARROLLO DE LA RESOLUCION 1032 DE 2024 EMITIDA POR EL MINISTERIO DE SALUD |72   |\n",
      "|PRESTACI√ìN DE SERVICIOS COMO AUXILIAR DE APOYO ADMINISTRATIVO A LA GESTI√ìN Y SEGUIMIENTO DE LAS ACTIVIDADES DE FACTURACI√ìN EN EL √ÅREA ADMINISTRATIVA DENTRO DE LOS DIFERENTES PROCESOS Y PROCEDIMIENTOS DE LA EMPRESA SOCIAL DEL ESTADO REGI√ìN DE SALUD SOACHA DE ACUERDO AL REQUERIMIENTO INSTITUCIONAL.   |66   |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "Distribuci√≥n por tipo de contrato:\n",
      "+------------------------------+-----+\n",
      "|tipo_contrato                 |count|\n",
      "+------------------------------+-----+\n",
      "|Prestaci√≥n de servicios       |81169|\n",
      "|Decreto 092 de 2017           |7814 |\n",
      "|Suministros                   |3413 |\n",
      "|Compraventa                   |2352 |\n",
      "|Otro                          |2319 |\n",
      "|Arrendamiento de inmuebles    |749  |\n",
      "|Obra                          |738  |\n",
      "|Seguros                       |475  |\n",
      "|Consultor√≠a                   |186  |\n",
      "|Interventor√≠a                 |104  |\n",
      "|Arrendamiento de muebles      |56   |\n",
      "|Operaciones de Cr√©dito P√∫blico|23   |\n",
      "|No Especificado               |20   |\n",
      "|Servicios financieros         |19   |\n",
      "|Comodato                      |9    |\n",
      "|Venta muebles                 |6    |\n",
      "|Asociaci√≥n P√∫blico Privada    |4    |\n",
      "|Concesi√≥n                     |2    |\n",
      "+------------------------------+-----+\n",
      "\n",
      "\n",
      "Distribuci√≥n del estado del contrato:\n",
      "+---------------+-----+\n",
      "|estado_contrato|count|\n",
      "+---------------+-----+\n",
      "|En ejecuci√≥n   |37005|\n",
      "|Cerrado        |27760|\n",
      "|Modificado     |21922|\n",
      "|terminado      |11707|\n",
      "|Aprobado       |688  |\n",
      "|cedido         |278  |\n",
      "|Suspendido     |98   |\n",
      "+---------------+-----+\n",
      "\n",
      "\n",
      "Top 10 modalidades de contrataci√≥n:\n",
      "+-------------------------------------------+-----+\n",
      "|modalidad                                  |count|\n",
      "+-------------------------------------------+-----+\n",
      "|Contrataci√≥n directa                       |69957|\n",
      "|Contrataci√≥n r√©gimen especial              |17569|\n",
      "|M√≠nima cuant√≠a                             |6772 |\n",
      "|Selecci√≥n Abreviada de Menor Cuant√≠a       |1607 |\n",
      "|Selecci√≥n abreviada subasta inversa        |964  |\n",
      "|Contrataci√≥n r√©gimen especial (con ofertas)|832  |\n",
      "|Contrataci√≥n Directa (con ofertas)         |745  |\n",
      "|Licitaci√≥n p√∫blica                         |724  |\n",
      "|Concurso de m√©ritos abierto                |186  |\n",
      "|Licitaci√≥n p√∫blica Obra Publica            |58   |\n",
      "+-------------------------------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Entidades\n",
    "print(\"Top 5 entidades:\")\n",
    "df_silver.groupBy(\"entidad\").count().orderBy(desc(\"count\")).show(5, truncate=False)\n",
    "\n",
    "# 2. Departamentos\n",
    "print(\"\\nTop 5 departamentos:\")\n",
    "df_silver.groupBy(\"departamento\").count().orderBy(desc(\"count\")).show(5, truncate=False)\n",
    "\n",
    "# 3. Regi√≥n\n",
    "print(\"\\nDistribuci√≥n por regi√≥n:\")\n",
    "df_silver.groupBy(\"region\").count().orderBy(desc(\"count\")).show(truncate=False)\n",
    "\n",
    "# 4. C√≥digo UNSPSC\n",
    "print(\"\\nTop 10 c√≥digos UNSPSC:\")\n",
    "df_silver.groupBy(\"codigo_unspsc\").count().orderBy(desc(\"count\")).show(10, truncate=False)\n",
    "\n",
    "# 5. Categor√≠a UNSPSC\n",
    "print(\"\\nTop 10 categor√≠as UNSPSC:\")\n",
    "df_silver.groupBy(\"descripcion_categoria\").count().orderBy(desc(\"count\")).show(10, truncate=False)\n",
    "\n",
    "# 6. Tipo de contrato\n",
    "print(\"\\nDistribuci√≥n por tipo de contrato:\")\n",
    "df_silver.groupBy(\"tipo_contrato\").count().orderBy(desc(\"count\")).show(truncate=False)\n",
    "\n",
    "# 7. Estado del contrato\n",
    "print(\"\\nDistribuci√≥n del estado del contrato:\")\n",
    "df_silver.groupBy(\"estado_contrato\").count().orderBy(desc(\"count\")).show(truncate=False)\n",
    "\n",
    "# 8. Modalidad de contrataci√≥n\n",
    "print(\"\\nTop 10 modalidades de contrataci√≥n:\")\n",
    "df_silver.groupBy(\"modalidad\").count().orderBy(desc(\"count\")).show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0131185-211a-4265-9356-7b567e112143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Estad√≠sticas de valor_contrato:\n",
      "+---+----------------+-------------------+--------------------+\n",
      "|min|             max|               mean|                 std|\n",
      "+---+----------------+-------------------+--------------------+\n",
      "|1.0|1.50838540149E11|9.941466321590018E7|1.1521186504414532E9|\n",
      "+---+----------------+-------------------+--------------------+\n",
      "\n",
      "\n",
      "Percentiles de valor_contrato:\n",
      "\n",
      "Estad√≠sticas de duracion_dias:\n",
      "+---+----+-----------------+------------------+\n",
      "|min| max|             mean|               std|\n",
      "+---+----+-----------------+------------------+\n",
      "|  0|4297|82.47422012591348|101.20091534465666|\n",
      "+---+----+-----------------+------------------+\n",
      "\n",
      "\n",
      "Percentiles de duracion_dias:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0, 6.0, 40.0, 125.0, 4297.0]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max, avg, stddev, expr\n",
    "\n",
    "# 10. Valor del contrato\n",
    "print(\"\\nEstad√≠sticas de valor_contrato:\")\n",
    "df_silver.select(\n",
    "    min(\"valor_contrato\").alias(\"min\"),\n",
    "    max(\"valor_contrato\").alias(\"max\"),\n",
    "    avg(\"valor_contrato\").alias(\"mean\"),\n",
    "    stddev(\"valor_contrato\").alias(\"std\")\n",
    ").show()\n",
    "\n",
    "# Percentiles\n",
    "print(\"\\nPercentiles de valor_contrato:\")\n",
    "df_silver.approxQuantile(\"valor_contrato\", [0.01, 0.25, 0.5, 0.75, 0.99], 0.01)\n",
    "\n",
    "# 11. Duraci√≥n en d√≠as\n",
    "print(\"\\nEstad√≠sticas de duracion_dias:\")\n",
    "df_silver.select(\n",
    "    min(\"duracion_dias\").alias(\"min\"),\n",
    "    max(\"duracion_dias\").alias(\"max\"),\n",
    "    avg(\"duracion_dias\").alias(\"mean\"),\n",
    "    stddev(\"duracion_dias\").alias(\"std\")\n",
    ").show()\n",
    "\n",
    "print(\"\\nPercentiles de duracion_dias:\")\n",
    "df_silver.approxQuantile(\"duracion_dias\", [0.01, 0.25, 0.5, 0.75, 0.99], 0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be7ceb3f-a49f-454d-9040-eadd4a513895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top a√±os:\n",
      "+----+-----+\n",
      "|anno|count|\n",
      "+----+-----+\n",
      "|2025|68   |\n",
      "|2024|95797|\n",
      "|2023|3029 |\n",
      "|2022|564  |\n",
      "+----+-----+\n",
      "\n",
      "\n",
      "Contratos por a√±o:\n",
      "+----+-----+\n",
      "|anno|count|\n",
      "+----+-----+\n",
      "|2024|95797|\n",
      "|2023|3029 |\n",
      "|2022|564  |\n",
      "|2025|68   |\n",
      "+----+-----+\n",
      "\n",
      "\n",
      "Top fechas de firma:\n",
      "+-----------+-----+\n",
      "|fecha_firma|count|\n",
      "+-----------+-----+\n",
      "|2024-02-01 |1230 |\n",
      "|2024-03-01 |1119 |\n",
      "|2024-02-02 |860  |\n",
      "|2024-02-05 |815  |\n",
      "|2024-03-22 |794  |\n",
      "|2024-02-09 |789  |\n",
      "|2024-02-16 |744  |\n",
      "|2024-02-06 |723  |\n",
      "|2024-02-12 |715  |\n",
      "|2024-09-02 |686  |\n",
      "+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTop a√±os:\")\n",
    "df_silver.groupBy(\"anno\").count().orderBy(desc(\"anno\")).show(10, truncate=False)\n",
    "\n",
    "print(\"\\nContratos por a√±o:\")\n",
    "df_silver.groupBy(\"anno\").count().orderBy(desc(\"count\")).show(10, truncate=False)\n",
    "\n",
    "print(\"\\nTop fechas de firma:\")\n",
    "df_silver.groupBy(\"fecha_firma\").count().orderBy(desc(\"count\")).show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9095b5d1-039d-4c44-81ad-731451dc1ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PASO 5: GUARDAR EN DELTA LAKE\n",
      "================================================================================\n",
      "\n",
      "üíæ Guardando en: /app/notebooks/delta_lake/silver_contracts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Guardado exitosamente\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 5. GUARDAR EN DELTA LAKE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 5: GUARDAR EN DELTA LAKE\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "DELTA_PATH = \"/app/notebooks/delta_lake/silver_contracts\"\n",
    "\n",
    "print(f\"üíæ Guardando en: {DELTA_PATH}\")\n",
    "\n",
    "df_silver.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .save(DELTA_PATH)\n",
    "\n",
    "print(\"‚úÖ Guardado exitosamente\\n\")\n",
    "\n",
    "# ‚ö†Ô∏è LIBERAR todo\n",
    "df_silver.unpersist()\n",
    "spark.catalog.clearCache()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fa9b8d-dcf7-4962-8755-3993314ee402",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Fase 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f353f8e1-7564-40cc-ae3f-bc39a496b980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PASO 1: CARGAR DATOS DESDE SILVER\n",
      "================================================================================\n",
      "\n",
      "üìä Cargando: /app/notebooks/delta_lake/silver_contracts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:============================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Registros: 99,458\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PASO 1: CARGAR DATOS DESDE SILVER\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "SILVER_PATH = \"/app/notebooks/delta_lake/silver_contracts\"\n",
    "print(f\"üìä Cargando: {SILVER_PATH}\")\n",
    "\n",
    "df_silver = spark.read.format(\"delta\").load(SILVER_PATH)\n",
    "df_silver = df_silver.cache()\n",
    "total_records = df_silver.count()\n",
    "\n",
    "print(f\"‚úì Registros: {total_records:,}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18f418b5-05f7-4432-a3e9-ca4bc34e5141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PASO 2: LIMPIEZA DE TEXTO\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:===========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Registros despu√©s de limpieza: 99,458\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PASO 2: LIMPIEZA DE TEXTO\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 2: LIMPIEZA DE TEXTO\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "src_chars = \"√°√©√≠√≥√∫√º√±\"\n",
    "dst_chars = \"aeiouun\"\n",
    "\n",
    "df_prepared = df_silver.withColumn(\n",
    "    \"objeto_limpio\",\n",
    "    trim(\n",
    "        regexp_replace(\n",
    "            regexp_replace(\n",
    "                translate(\n",
    "                    lower(col(\"objeto_contrato\")),\n",
    "                    src_chars,\n",
    "                    dst_chars\n",
    "                ),\n",
    "                \"[^a-z0-9\\\\s]\", \" \"\n",
    "            ),\n",
    "            \"\\\\s+\", \" \"\n",
    "        )\n",
    "    )\n",
    ").filter(length(col(\"objeto_limpio\")) >= 10)\n",
    "\n",
    "print(f\"‚úì Registros despu√©s de limpieza: {df_prepared.count():,}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb8b89d1-bb12-449c-b504-a243d1b8e33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PASO 3: TOKENIZACI√ìN\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:===========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Registros despu√©s de filtrado: 99,458\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PASO 3: TOKENIZACI√ìN Y STOPWORDS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 3: TOKENIZACI√ìN\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "stopwords_es = [\n",
    "    \"el\", \"la\", \"de\", \"que\", \"y\", \"a\", \"en\", \"un\", \"ser\", \"se\", \"no\",\n",
    "    \"por\", \"con\", \"su\", \"para\", \"como\", \"estar\", \"tener\", \"le\", \"lo\",\n",
    "    \"pero\", \"hacer\", \"o\", \"este\", \"otro\", \"ese\", \"si\", \"ya\", \"ver\",\n",
    "    \"dar\", \"muy\", \"sin\", \"sobre\", \"tambi√©n\", \"hasta\", \"a√±o\", \"entre\",\n",
    "    \"del\", \"al\", \"los\", \"las\", \"uno\", \"una\", \"unos\", \"unas\",\n",
    "    \"contrato\", \"contratos\", \"objeto\", \"prestacion\", \"prestaci√≥n\",\n",
    "    \"servicio\", \"servicios\", \"suministro\", \"ejecucion\", \"ejecuci√≥n\"\n",
    "]\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"objeto_limpio\", outputCol=\"palabras\")\n",
    "df_tokenized = tokenizer.transform(df_prepared)\n",
    "\n",
    "remover = StopWordsRemover(\n",
    "    inputCol=\"palabras\",\n",
    "    outputCol=\"palabras_sin_stopwords\",\n",
    "    stopWords=stopwords_es\n",
    ")\n",
    "df_filtered_words = remover.transform(df_tokenized)\n",
    "\n",
    "# Filtrar palabras cortas\n",
    "def clean_words(words):\n",
    "    if not words:\n",
    "        return []\n",
    "    return [w for w in words if len(w) >= 3]\n",
    "\n",
    "clean_udf = udf(clean_words, ArrayType(StringType()))\n",
    "\n",
    "df_filtered = df_filtered_words.withColumn(\n",
    "    \"palabras_filtradas\",\n",
    "    clean_udf(col(\"palabras_sin_stopwords\"))\n",
    ").filter(size(col(\"palabras_filtradas\")) > 0)\n",
    "\n",
    "print(f\"‚úì Registros despu√©s de filtrado: {df_filtered.count():,}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bc2043b-4721-4da7-a242-b0e4bf879ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PASO 4: WORD2VEC\n",
      "================================================================================\n",
      "\n",
      "‚è≥ Entrenando Word2Vec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Vocabulario: 14,465 palabras\n",
      "‚úì Embeddings¬†generados\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PASO 4: WORD2VEC\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 4: WORD2VEC\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "word2vec = Word2Vec(\n",
    "    vectorSize=100,\n",
    "    minCount=2,\n",
    "    maxIter=10,\n",
    "    seed=42,\n",
    "    inputCol=\"palabras_filtradas\",\n",
    "    outputCol=\"embedding_raw\"\n",
    ")\n",
    "\n",
    "print(\"‚è≥ Entrenando Word2Vec...\")\n",
    "word2vec_model = word2vec.fit(df_filtered)\n",
    "df_embeddings = word2vec_model.transform(df_filtered)\n",
    "\n",
    "vocab_size = len(word2vec_model.getVectors().collect())\n",
    "print(f\"‚úì Vocabulario: {vocab_size:,} palabras\")\n",
    "print(f\"‚úì Embeddings¬†generados\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbde6876-943f-4de2-8206-dd5afc874ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Transformaciones categ√≥ricas (sin target)...\n",
      "   OneHot para tipo_contrato...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   OneHot para estado_contrato...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   OneHot para modalidad...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   Frequency Encoding para 'entidad'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# 3. Transformaciones INDEPENDIENTES del target\n",
    "# ----------------------------------------------------------------\n",
    "print(\"\\n2. Transformaciones categ√≥ricas (sin target)...\")\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# A. OneHot para variables de baja cardinalidad\n",
    "low_card_cols = [\"tipo_contrato\", \"estado_contrato\", \"modalidad\"]\n",
    "\n",
    "for col_name in low_card_cols:\n",
    "    print(f\"   OneHot para {col_name}...\")\n",
    "    \n",
    "    indexer = StringIndexer(\n",
    "        inputCol=col_name,\n",
    "        outputCol=f\"{col_name}_idx\",\n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "    df_embeddings = indexer.fit(df_embeddings).transform(df_embeddings)\n",
    "    \n",
    "    encoder = OneHotEncoder(\n",
    "        inputCol=f\"{col_name}_idx\",\n",
    "        outputCol=f\"{col_name}_ohe\",\n",
    "        dropLast=True\n",
    "    )\n",
    "    df_embeddings = encoder.fit(df_embeddings).transform(df_embeddings)\n",
    "\n",
    "# B. Frequency Encoding para 'entidad'\n",
    "print(\"\\n   Frequency Encoding para 'entidad'...\")\n",
    "entidad_freq = df_embeddings.groupBy(\"entidad\").count()\n",
    "total_count = df_embeddings.count()\n",
    "entidad_freq = entidad_freq.withColumn(\n",
    "    \"entidad_freq\",\n",
    "    col(\"count\") / total_count\n",
    ").select(\"entidad\", \"entidad_freq\")\n",
    "\n",
    "df_embeddings = df_embeddings.join(entidad_freq, \"entidad\", \"left\")\n",
    "\n",
    "# C. Eliminar variables sin varianza\n",
    "df_embeddings = df_embeddings.drop(\"departamento\", \"region\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd6d8a14-d303-454a-ae88-91f3c95a00b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Divisi√≥n temporal train/test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Fecha de corte: 2024-09-27 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Train: 79,298 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 67:===========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Test:  20,160 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# 4. DIVISI√ìN TEMPORAL (80/20)\n",
    "# ----------------------------------------------------------------\n",
    "print(\"\\n3. Divisi√≥n temporal train/test...\")\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# Convertir fecha para calcular percentil\n",
    "df_temp = df_embeddings.withColumn(\n",
    "    \"fecha_num\",\n",
    "    col(\"fecha_firma\").cast(\"timestamp\").cast(\"long\")\n",
    ")\n",
    "\n",
    "# Calcular percentil 80\n",
    "q = df_temp.approxQuantile(\"fecha_num\", [0.8], 0.01)\n",
    "split_ts = q[0]\n",
    "split_date = datetime.utcfromtimestamp(split_ts)\n",
    "\n",
    "print(f\"   Fecha de corte: {split_date}\")\n",
    "\n",
    "# Crear datasets de train y test\n",
    "df_train_raw = df_embeddings.filter(col(\"fecha_firma\") <= split_date)\n",
    "df_test_raw = df_embeddings.filter(col(\"fecha_firma\") > split_date)\n",
    "\n",
    "print(f\"   Train: {df_train_raw.count():,} registros\")\n",
    "print(f\"   Test:  {df_test_raw.count():,} registros\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f280937-d4cd-4c08-8970-e427d5537aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. Guardando datasets en Delta Lake...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úì Train guardado en: /app/notebooks/delta_lake/train_raw_v3\n",
      "   ‚úì Test guardado en:  /app/notebooks/delta_lake/test_raw_v3\n",
      "\n",
      "5. Guardando modelos de transformaci√≥n...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelos guardados\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# 5. Guardar datasets en Delta Lake\n",
    "# ----------------------------------------------------------------\n",
    "print(\"\\n4. Guardando datasets en Delta Lake...\")\n",
    "\n",
    "# Ruta para los datasets preprocesados\n",
    "TRAIN_RAW_PATH = \"/app/notebooks/delta_lake/train_raw_v3\"\n",
    "TEST_RAW_PATH = \"/app/notebooks/delta_lake/test_raw_v3\"\n",
    "\n",
    "# Guardar train\n",
    "df_train_raw.write.format(\"delta\").mode(\"overwrite\").save(TRAIN_RAW_PATH)\n",
    "\n",
    "# Guardar test\n",
    "df_test_raw.write.format(\"delta\").mode(\"overwrite\").save(TEST_RAW_PATH)\n",
    "\n",
    "print(f\"   ‚úì Train guardado en: {TRAIN_RAW_PATH}\")\n",
    "print(f\"   ‚úì Test guardado en:  {TEST_RAW_PATH}\")\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 6. Guardar tambi√©n los modelos de transformaci√≥n\n",
    "# ----------------------------------------------------------------\n",
    "print(\"\\n5. Guardando modelos de transformaci√≥n...\")\n",
    "\n",
    "MODELS_PATH = \"/app/notebooks/models_v3\"\n",
    "\n",
    "# Guardar Word2Vec model (si lo tienes)\n",
    "if 'word2vec_model' in locals():\n",
    "    word2vec_model.save(f\"{MODELS_PATH}/word2vec_model\")\n",
    "\n",
    "# Guardar StringIndexer models para referencia\n",
    "for col_name in low_card_cols:\n",
    "    indexer_model_path = f\"{MODELS_PATH}/indexer_{col_name}\"\n",
    "    # Necesitar√≠as extraer el modelo del pipeline o guardar los mapeos\n",
    "\n",
    "print(\"Modelos guardados\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02bcc194-9db1-4aac-9f0e-06077ca5a1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RESUMEN FASE 3\n",
      "================================================================================\n",
      "‚úÖ Preprocesamiento completado\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Train: 79,298 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 99:===========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Test:  20,160 registros\n",
      "\n",
      "üéØ Variables disponibles:\n",
      "  - embedding_raw: Word2Vec embeddings\n",
      "  - tipo_contrato_ohe: OneHot encoded\n",
      "  - estado_contrato_ohe: OneHot encoded\n",
      "  - modalidad_ohe: OneHot encoded\n",
      "  - entidad_freq: Frequency encoding\n",
      "  - valor_contrato: Target variable\n",
      "  - duracion_dias: Variable num√©rica\n",
      "\n",
      "üìà Listo para Fase 4: Target Encoding y Modelado\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# 7. Informe final\n",
    "# ----------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESUMEN FASE 3\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"‚úÖ Preprocesamiento completado\")\n",
    "print(f\"üìä Train: {df_train_raw.count():,} registros\")\n",
    "print(f\"üìä Test:  {df_test_raw.count():,} registros\")\n",
    "print()\n",
    "print(\"üéØ Variables disponibles:\")\n",
    "print(f\"  - embedding_raw: Word2Vec embeddings\")\n",
    "for col in low_card_cols:\n",
    "    print(f\"  - {col}_ohe: OneHot encoded\")\n",
    "print(f\"  - entidad_freq: Frequency encoding\")\n",
    "print(f\"  - valor_contrato: Target variable\")\n",
    "print(f\"  - duracion_dias: Variable num√©rica\")\n",
    "print()\n",
    "print(\"üìà Listo para Fase 4: Target Encoding y Modelado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81211544-f4a9-481c-87a2-b1b3a4b98958",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Fase 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07f621bb-6131-43ed-8375-261de8517517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import builtins  # <-- IMPORTANTE: Importar builtins\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor, GBTRegressor, DecisionTreeRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d89ed5d-9cda-4234-9cef-3322c5bfe2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FASE 4: MODELADO CON TRANSFORMACI√ìN LOGAR√çTMICA\n",
      "================================================================================\n",
      "1. Cargando datasets preprocesados...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:===================================================>     (9 + 1) / 10]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úì Train: 79,298 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:==============================================>           (4 + 1) / 5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úì Test:  20,160 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# FASE 4: MODELADO Y REGISTRO EN MLFLOW (CON LOG TRANSFORM)\n",
    "# ================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FASE 4: MODELADO CON TRANSFORMACI√ìN LOGAR√çTMICA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 1. Cargar datasets preprocesados\n",
    "# ----------------------------------------------------------------\n",
    "print(\"1. Cargando datasets preprocesados...\")\n",
    "\n",
    "TRAIN_RAW_PATH = \"/app/notebooks/delta_lake/train_raw_v3\"\n",
    "TEST_RAW_PATH = \"/app/notebooks/delta_lake/test_raw_v3\"\n",
    "\n",
    "train_raw = spark.read.format(\"delta\").load(TRAIN_RAW_PATH).cache()\n",
    "test_raw = spark.read.format(\"delta\").load(TEST_RAW_PATH).cache()\n",
    "\n",
    "print(f\"   ‚úì Train: {train_raw.count():,} registros\")\n",
    "print(f\"   ‚úì Test:  {test_raw.count():,} registros\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb6d7eca-ebcc-4e37-8dfe-bae7e5850657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Aplicando transformaci√≥n logar√≠tmica al target...\n",
      "   Estad√≠sticas del target original vs logar√≠tmico:\n",
      "   Original: mean=$81,402,743.06, std=$981,240,307.25\n",
      "   Log: mean=16.72, std=1.12\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# 2. TRANSFORMACI√ìN LOGAR√çTMICA DEL TARGET\n",
    "# ----------------------------------------------------------------\n",
    "import pyspark.sql.functions as F\n",
    "print(\"\\n2. Aplicando transformaci√≥n logar√≠tmica al target...\")\n",
    "\n",
    "# Usamos log1p = log(1 + x) para evitar problemas con valores peque√±os\n",
    "train_data = train_raw.withColumn(\"log_valor_contrato\", F.log1p(col(\"valor_contrato\")))\n",
    "test_data = test_raw.withColumn(\"log_valor_contrato\", F.log1p(col(\"valor_contrato\")))\n",
    "\n",
    "# Verificar estad√≠sticas antes/despu√©s\n",
    "print(\"   Estad√≠sticas del target original vs logar√≠tmico:\")\n",
    "train_stats = train_data.select(\n",
    "    F.mean(\"valor_contrato\").alias(\"mean_original\"),\n",
    "    F.stddev(\"valor_contrato\").alias(\"std_original\"),\n",
    "    F.mean(\"log_valor_contrato\").alias(\"mean_log\"),\n",
    "    F.stddev(\"log_valor_contrato\").alias(\"std_log\")\n",
    ").first()\n",
    "\n",
    "print(f\"   Original: mean=${train_stats['mean_original']:,.2f}, std=${train_stats['std_original']:,.2f}\")\n",
    "print(f\"   Log: mean={train_stats['mean_log']:.2f}, std={train_stats['std_log']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86e24322-a3fd-4171-b2ae-7c069a62e034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Target Encoding para 'codigo_unspsc' (usando target log)...\n",
      "   ‚úì codigo_unspsc_te_log creado (en escala log)\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# 3. Target Encoding usando el TARGET LOGAR√çTMICO\n",
    "# ----------------------------------------------------------------\n",
    "print(\"\\n3. Target Encoding para 'codigo_unspsc' (usando target log)...\")\n",
    "\n",
    "def safe_target_encoding_log(train_df, test_df, cat_col, target_log_col=\"log_valor_contrato\", m=50):\n",
    "    \"\"\"\n",
    "    Target encoding usando el target en escala logar√≠tmica\n",
    "    \"\"\"\n",
    "    # Calcular media global del LOG en train\n",
    "    global_mean_log = train_df.agg(F.mean(target_log_col)).first()[0]\n",
    "    \n",
    "    # Calcular estad√≠sticas por categor√≠a en train (usando LOG)\n",
    "    stats = train_df.groupBy(cat_col).agg(\n",
    "        F.mean(target_log_col).alias(\"cat_mean_log\"),\n",
    "        F.count(target_log_col).alias(\"cat_count\")\n",
    "    )\n",
    "    \n",
    "    # Aplicar smoothing en escala logar√≠tmica\n",
    "    stats = stats.withColumn(\n",
    "        f\"{cat_col}_te_log\",\n",
    "        (F.col(\"cat_count\") * F.col(\"cat_mean_log\") + m * global_mean_log) / \n",
    "        (F.col(\"cat_count\") + m)\n",
    "    ).select(cat_col, f\"{cat_col}_te_log\")\n",
    "    \n",
    "    # Aplicar a train\n",
    "    train_encoded = train_df.join(stats, cat_col, \"left\")\n",
    "    \n",
    "    # Aplicar a test\n",
    "    test_encoded = test_df.join(stats, cat_col, \"left\")\n",
    "    \n",
    "    # Para categor√≠as no vistas en train, usar global mean log\n",
    "    test_encoded = test_encoded.fillna({f\"{cat_col}_te_log\": global_mean_log})\n",
    "    \n",
    "    return train_encoded, test_encoded\n",
    "\n",
    "# Aplicar target encoding con target logar√≠tmico\n",
    "train_data, test_data = safe_target_encoding_log(\n",
    "    train_data, test_data,\n",
    "    cat_col=\"codigo_unspsc\",\n",
    "    target_log_col=\"log_valor_contrato\",\n",
    "    m=50\n",
    ")\n",
    "\n",
    "print(\"   ‚úì codigo_unspsc_te_log creado (en escala log)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01c10bf4-32e6-4fed-ae19-23f56f58c816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. Ensamblando features...\n",
      "   Features: 7 dimensiones\n",
      "   - embedding_raw\n",
      "   - tipo_contrato_ohe\n",
      "   - estado_contrato_ohe\n",
      "   - modalidad_ohe\n",
      "   - entidad_freq\n",
      "   - codigo_unspsc_te_log\n",
      "   - duracion_dias\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# 4. Ensamblar features (incluyendo el target encoding logar√≠tmico)\n",
    "# ----------------------------------------------------------------\n",
    "print(\"\\n4. Ensamblando features...\")\n",
    "\n",
    "feature_cols = [\n",
    "    \"embedding_raw\",\n",
    "    \"tipo_contrato_ohe\",\n",
    "    \"estado_contrato_ohe\", \n",
    "    \"modalidad_ohe\",\n",
    "    \"entidad_freq\",\n",
    "    \"codigo_unspsc_te_log\",  # ¬°Usamos la versi√≥n logar√≠tmica!\n",
    "]\n",
    "\n",
    "if \"duracion_dias\" in train_data.columns:\n",
    "    train_data = train_data.fillna({\"duracion_dias\": 0})\n",
    "    test_data = test_data.fillna({\"duracion_dias\": 0})\n",
    "    feature_cols.append(\"duracion_dias\")\n",
    "\n",
    "print(f\"   Features: {len(feature_cols)} dimensiones\")\n",
    "for feat in feature_cols:\n",
    "    print(f\"   - {feat}\")\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features_raw\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "\n",
    "train_features = assembler.transform(train_data)\n",
    "test_features = assembler.transform(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "216023ac-4227-46c8-8777-09c2df8ff6f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. Normalizando y validando...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# 5. Normalizaci√≥n (CON VALIDACI√ìN)\n",
    "# ----------------------------------------------------------------\n",
    "print(\"\\n5. Normalizando y validando...\")\n",
    "\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features_raw\",\n",
    "    outputCol=\"features_scaled\",\n",
    "    withStd=True,\n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "scaler_model = scaler.fit(train_features)\n",
    "train_scaled = scaler_model.transform(train_features)\n",
    "test_scaled = scaler_model.transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df5b0f10-7080-4f3f-a666-c1f97faeb902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6. Omitiendo PCA - usando features escaladas directamente\n",
      "   Dimensiones finales: 141\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[codigo_unspsc: string, entidad: string, id_contrato: string, objeto_contrato: string, municipio: string, descripcion_categoria: string, valor_contrato: double, duracion_dias: int, fecha_firma: date, tipo_contrato: string, estado_contrato: string, modalidad: string, anno: int, objeto_limpio: string, palabras: array<string>, palabras_sin_stopwords: array<string>, palabras_filtradas: array<string>, embedding_raw: vector, tipo_contrato_idx: double, tipo_contrato_ohe: vector, estado_contrato_idx: double, estado_contrato_ohe: vector, modalidad_idx: double, modalidad_ohe: vector, entidad_freq: double, log_valor_contrato: double, codigo_unspsc_te_log: double, features_raw: vector, features_scaled: vector]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# 6. OMITIR PCA\n",
    "# ----------------------------------------------------------------\n",
    "print(\"\\n6. Omitiendo PCA - usando features escaladas directamente\")\n",
    "\n",
    "train_final = train_scaled.select(\n",
    "    col(\"log_valor_contrato\").alias(\"label_log\"),\n",
    "    col(\"features_scaled\").alias(\"features\"),\n",
    "    col(\"valor_contrato\")\n",
    ").cache()\n",
    "\n",
    "test_final = test_scaled.select(\n",
    "    col(\"log_valor_contrato\").alias(\"label_log\"),\n",
    "    col(\"features_scaled\").alias(\"features\"),\n",
    "    col(\"valor_contrato\")\n",
    ").cache()\n",
    "\n",
    "dimensiones = len(train_final.select(\"features\").first()[0])\n",
    "print(f\"   Dimensiones finales: {dimensiones}\")\n",
    "\n",
    "# Liberar DataFrames intermedios que ya no se usan\n",
    "train_scaled.unpersist()\n",
    "test_scaled.unpersist()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1215ffd-de37-4046-82c5-84aab30c315b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2. RANDOM FOREST (escala logar√≠tmica)\n",
      "\n",
      "üîß Entrenando Random Forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Modelo entrenado correctamente\n",
      "\n",
      "üìê Calculando œÉ en TRAIN set (escala logar√≠tmica)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   œÉ (escala log): 0.7763\n",
      "   Threshold 2.8œÉ: 2.1736\n",
      "   ‚úì Sigma calculado y listo para MLflow\n",
      "\n",
      "üîÆ Generando predicciones en TEST...\n",
      "   ‚úì Predicciones generadas\n",
      "\n",
      "üìä Calculando m√©tricas en escala LOGAR√çTMICA...\n",
      "\n",
      "üìä RESULTADOS RANDOM FOREST (escala logar√≠tmica):\n",
      "   R¬≤:    0.4395\n",
      "   RMSE:  1.2620\n",
      "   MAE:   0.9908\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# 2. RANDOM FOREST (entrenado y evaluado en escala logar√≠tmica)\n",
    "# ----------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"2. RANDOM FOREST (escala logar√≠tmica)\")\n",
    "\n",
    "rf = RandomForestRegressor(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label_log\",\n",
    "    numTrees=30,\n",
    "    maxDepth=8,\n",
    "    maxBins=32,\n",
    "    subsamplingRate=0.8,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "try:\n",
    "    # ----------------------------------------------------------------\n",
    "    # ENTRENAR MODELO\n",
    "    # ----------------------------------------------------------------\n",
    "    print(\"\\nüîß Entrenando Random Forest...\")\n",
    "    rf_model = rf.fit(train_final)\n",
    "    print(\"   ‚úÖ Modelo entrenado correctamente\")\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # CALCULAR SIGMA EN TRAIN SET (ESCALA LOGAR√çTMICA)\n",
    "    # ----------------------------------------------------------------\n",
    "    print(\"\\nüìê Calculando œÉ en TRAIN set (escala logar√≠tmica)...\")\n",
    "\n",
    "    train_predictions = rf_model.transform(train_final)\n",
    "\n",
    "    train_with_residuals = train_predictions.withColumn(\n",
    "        \"residual_log\",\n",
    "        F.col(\"label_log\") - F.col(\"prediction\")\n",
    "    )\n",
    "\n",
    "    sigma_log = train_with_residuals.agg(\n",
    "        F.stddev(\"residual_log\").alias(\"sigma\")\n",
    "    ).first()[\"sigma\"]\n",
    "\n",
    "    threshold_log = 2.8 * sigma_log\n",
    "\n",
    "    print(f\"   œÉ (escala log): {sigma_log:.4f}\")\n",
    "    print(f\"   Threshold 2.8œÉ: {threshold_log:.4f}\")\n",
    "    print(\"   ‚úì Sigma calculado y listo para MLflow\")\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # PREDECIR EN TEST SET\n",
    "    # ----------------------------------------------------------------\n",
    "    print(\"\\nüîÆ Generando predicciones en TEST...\")\n",
    "    rf_predictions = rf_model.transform(test_final)\n",
    "    print(\"   ‚úì Predicciones generadas\")\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # M√âTRICAS EN ESCALA LOGAR√çTMICA\n",
    "    # ----------------------------------------------------------------\n",
    "    print(\"\\nüìä Calculando m√©tricas en escala LOGAR√çTMICA...\")\n",
    "\n",
    "    evaluator_r2 = RegressionEvaluator(\n",
    "        labelCol=\"label_log\",\n",
    "        predictionCol=\"prediction\",\n",
    "        metricName=\"r2\"\n",
    "    )\n",
    "    evaluator_rmse = RegressionEvaluator(\n",
    "        labelCol=\"label_log\",\n",
    "        predictionCol=\"prediction\",\n",
    "        metricName=\"rmse\"\n",
    "    )\n",
    "    evaluator_mae = RegressionEvaluator(\n",
    "        labelCol=\"label_log\",\n",
    "        predictionCol=\"prediction\",\n",
    "        metricName=\"mae\"\n",
    "    )\n",
    "\n",
    "    rf_r2_log = evaluator_r2.evaluate(rf_predictions)\n",
    "    rf_rmse_log = evaluator_rmse.evaluate(rf_predictions)\n",
    "    rf_mae_log  = evaluator_mae.evaluate(rf_predictions)\n",
    "\n",
    "    print(f\"\\nüìä RESULTADOS RANDOM FOREST (escala logar√≠tmica):\")\n",
    "    print(f\"   R¬≤:    {rf_r2_log:.4f}\")\n",
    "    print(f\"   RMSE:  {rf_rmse_log:.4f}\")\n",
    "    print(f\"   MAE:   {rf_mae_log:.4f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è Error con Random Forest: {str(e)[:120]}\")\n",
    "    print(\"   ‚Üí Probable falta de memoria o problema con DF.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4dc727e-587c-4e34-af8d-4f323d62229d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "REGISTRO EN MLFLOW - RANDOM FOREST\n",
      "================================================================================\n",
      "üìù Registrando par√°metros...\n",
      "   ‚úì Par√°metros registrados\n",
      "üìà Registrando m√©tricas...\n",
      "   ‚úì M√©tricas registradas\n",
      "   ‚úì sigma_log_train: 0.776302 ‚Üê GUARDADO EN MLFLOW\n",
      "   ‚úì anomaly_threshold_log: 2.173647 ‚Üê GUARDADO EN MLFLOW\n",
      "üíæ Registrando modelo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'contract_value_predictor_rf_log_v1' already exists. Creating a new version of this model...\n",
      "2025/12/11 20:45:44 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: contract_value_predictor_rf_log_v1, version 4\n",
      "Created version '4' of model 'contract_value_predictor_rf_log_v1'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úì Modelo registrado\n",
      "üìé Registrando artifacts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úì Artifacts registrados\n",
      "üè∑Ô∏è  Registrando tags...\n",
      "   ‚úì Tags registrados\n",
      "\n",
      "‚úÖ RUN COMPLETADO - RANDOM FOREST:\n",
      "   Run ID: 74280a268f2a4be8b98d49bd88277335\n",
      "   Experiment ID: 3\n",
      "   MLflow UI: http://172.17.0.1:5000\n",
      "\n",
      "   üìä M√©tricas (escala logar√≠tmica):\n",
      "      R¬≤:   0.4395\n",
      "      RMSE: 1.2620\n",
      "      MAE:  0.9908\n",
      "\n",
      "   üéØ Detecci√≥n de anomal√≠as:\n",
      "      œÉ (log, train): 0.7763\n",
      "      Threshold 2.8œÉ: 2.1736\n",
      "üèÉ View run random_forest_log_20251211_204524 at: http://172.17.0.1:5000/#/experiments/3/runs/74280a268f2a4be8b98d49bd88277335\n",
      "üß™ View experiment at: http://172.17.0.1:5000/#/experiments/3\n",
      "\n",
      "‚úÖ Variables guardadas para detecci√≥n de anomal√≠as:\n",
      "   - rf_model: Modelo entrenado\n",
      "   - rf_predictions: Predicciones en test set\n",
      "   - sigma_log: 0.7763\n",
      "   - threshold_log: 2.1736\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# REGISTRAR EN MLFLOW\n",
    "# ----------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"REGISTRO EN MLFLOW - RANDOM FOREST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Configurar MLflow\n",
    "MLFLOW_TRACKING_URI = \"http://172.17.0.1:5000\"\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "mlflow.set_experiment(\"contract_value_regression_log\")\n",
    "\n",
    "# Iniciar run de MLflow\n",
    "with mlflow.start_run(run_name=f\"random_forest_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"):\n",
    "\n",
    "    # ========== Registrar par√°metros ==========\n",
    "    print(\"üìù Registrando par√°metros...\")\n",
    "\n",
    "    mlflow.log_param(\"model_type\", \"RandomForestRegressor\")\n",
    "    mlflow.log_param(\"target_transform\", \"log1p\")\n",
    "    mlflow.log_param(\"numTrees\", 30)\n",
    "    mlflow.log_param(\"maxDepth\", 8)\n",
    "    mlflow.log_param(\"maxBins\", 32)\n",
    "    mlflow.log_param(\"subsamplingRate\", 0.8)\n",
    "    mlflow.log_param(\"seed\", 42)\n",
    "    mlflow.log_param(\"train_size\", train_final.count())\n",
    "    mlflow.log_param(\"test_size\", test_final.count())\n",
    "    mlflow.log_param(\"features_count\", len(feature_cols))\n",
    "    mlflow.log_param(\"target_encoding_smoothing\", 50)\n",
    "    mlflow.log_param(\"target_encoding_scale\", \"logarithmic\")\n",
    "\n",
    "    print(\"   ‚úì Par√°metros registrados\")\n",
    "\n",
    "    # ========== Registrar m√©tricas (SOLO ESCALA LOGAR√çTMICA) ==========\n",
    "    print(\"üìà Registrando m√©tricas...\")\n",
    "\n",
    "    mlflow.log_metric(\"test_r2_log\", rf_r2_log)\n",
    "    mlflow.log_metric(\"test_rmse_log\", rf_rmse_log)\n",
    "    mlflow.log_metric(\"test_mae_log\", rf_mae_log)\n",
    "\n",
    "    # ‚≠ê M√©tricas para detecci√≥n de anomal√≠as\n",
    "    mlflow.log_metric(\"sigma_log_train\", sigma_log)\n",
    "    mlflow.log_metric(\"anomaly_threshold_log\", threshold_log)\n",
    "\n",
    "    print(f\"   ‚úì M√©tricas registradas\")\n",
    "    print(f\"   ‚úì sigma_log_train: {sigma_log:.6f} ‚Üê GUARDADO EN MLFLOW\")\n",
    "    print(f\"   ‚úì anomaly_threshold_log: {threshold_log:.6f} ‚Üê GUARDADO EN MLFLOW\")\n",
    "\n",
    "    # ========== Registrar modelo ==========\n",
    "    print(\"üíæ Registrando modelo...\")\n",
    "\n",
    "    mlflow.spark.log_model(\n",
    "        spark_model=rf_model,\n",
    "        artifact_path=\"model_log\",\n",
    "        registered_model_name=\"contract_value_predictor_rf_log_v1\"\n",
    "    )\n",
    "\n",
    "    print(\"   ‚úì Modelo registrado\")\n",
    "\n",
    "    # ========== Registrar artifacts ==========\n",
    "    print(\"üìé Registrando artifacts...\")\n",
    "\n",
    "    metrics_dict = {\n",
    "        \"model_type\": \"RandomForestRegressor\",\n",
    "        \"target_transform\": \"log1p\",\n",
    "        \"hyperparameters\": {\n",
    "            \"numTrees\": 30,\n",
    "            \"maxDepth\": 8,\n",
    "            \"maxBins\": 32,\n",
    "            \"subsamplingRate\": 0.8,\n",
    "            \"seed\": 42\n",
    "        },\n",
    "        \"metrics_log\": {\n",
    "            \"r2\": float(rf_r2_log),\n",
    "            \"rmse\": float(rf_rmse_log),\n",
    "            \"mae\": float(rf_mae_log)\n",
    "        },\n",
    "        \"anomaly_detection\": {\n",
    "            \"sigma_log_train\": float(sigma_log),\n",
    "            \"threshold_2.8sigma_log\": float(threshold_log),\n",
    "            \"method\": \"log_scale_residuals\"\n",
    "        },\n",
    "        \"data_info\": {\n",
    "            \"train_size\": int(train_final.count()),\n",
    "            \"test_size\": int(test_final.count()),\n",
    "            \"features_count\": len(feature_cols)\n",
    "        },\n",
    "        \"run_timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "    metrics_path = \"/tmp/rf_model_metrics_log.json\"\n",
    "    with open(metrics_path, 'w') as f:\n",
    "        json.dump(metrics_dict, f, indent=2)\n",
    "\n",
    "    mlflow.log_artifact(metrics_path, \"metrics\")\n",
    "\n",
    "    print(\"   ‚úì Artifacts registrados\")\n",
    "\n",
    "    # ========== Registrar tags ==========\n",
    "    print(\"üè∑Ô∏è  Registrando tags...\")\n",
    "\n",
    "    mlflow.set_tag(\"framework\", \"PySpark\")\n",
    "    mlflow.set_tag(\"spark_version\", spark.version)\n",
    "    mlflow.set_tag(\"model_version\", \"v1.0_rf_log\")\n",
    "    mlflow.set_tag(\"data_source\", \"contratos_publicos\")\n",
    "    mlflow.set_tag(\"target_variable\", \"valor_contrato\")\n",
    "    mlflow.set_tag(\"target_transform\", \"log1p\")\n",
    "    mlflow.set_tag(\"encoding_strategy\", \"hybrid_log\")\n",
    "    mlflow.set_tag(\"algorithm\", \"RandomForest\")\n",
    "    mlflow.set_tag(\"anomaly_detection_method\", \"sigma_log_train\")\n",
    "\n",
    "    print(\"   ‚úì Tags registrados\")\n",
    "\n",
    "    # ========== Informaci√≥n del run ==========\n",
    "    run_id = mlflow.active_run().info.run_id\n",
    "    experiment_id = mlflow.active_run().info.experiment_id\n",
    "\n",
    "    print(f\"\\n‚úÖ RUN COMPLETADO - RANDOM FOREST:\")\n",
    "    print(f\"   Run ID: {run_id}\")\n",
    "    print(f\"   Experiment ID: {experiment_id}\")\n",
    "    print(f\"   MLflow UI: {MLFLOW_TRACKING_URI}\")\n",
    "    print(f\"\\n   üìä M√©tricas (escala logar√≠tmica):\")\n",
    "    print(f\"      R¬≤:   {rf_r2_log:.4f}\")\n",
    "    print(f\"      RMSE: {rf_rmse_log:.4f}\")\n",
    "    print(f\"      MAE:  {rf_mae_log:.4f}\")\n",
    "    print(f\"\\n   üéØ Detecci√≥n de anomal√≠as:\")\n",
    "    print(f\"      œÉ (log, train): {sigma_log:.4f}\")\n",
    "    print(f\"      Threshold 2.8œÉ: {threshold_log:.4f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Variables guardadas para detecci√≥n de anomal√≠as:\")\n",
    "print(f\"   - rf_model: Modelo entrenado\")\n",
    "print(f\"   - rf_predictions: Predicciones en test set\")\n",
    "print(f\"   - sigma_log: {sigma_log:.4f}\")\n",
    "print(f\"   - threshold_log: {threshold_log:.4f}\")\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a98ca4-2ec4-4ceb-9aab-0789fca74b73",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Fase 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eecab113-75aa-48ee-b80c-46f6825b90ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FASE 5: DETECCI√ìN DE AT√çPICOS (REGLA DE NEGOCIO)\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£ Verificando sigma calculado...\n",
      "   ‚úÖ sigma_log disponible: 0.776302\n",
      "   ‚úÖ Threshold 2.8œÉ: 2.173647\n",
      "\n",
      "2Ô∏è‚É£ Aplicando regla de detecci√≥n...\n",
      "   ‚úÖ Regla aplicada: Desviaci√≥n > 2.1736 ‚Üí AT√çPICO\n",
      "\n",
      "3Ô∏è‚É£ Calculando estad√≠sticas...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä RESULTADOS DE DETECCI√ìN:\n",
      "   Total contratos analizados: 20,160\n",
      "   Contratos AT√çPICOS: 622 (3.09%)\n",
      "   Contratos LIBRES: 19,538 (96.91%)\n",
      "\n",
      "4Ô∏è‚É£ Enriqueciendo datos...\n",
      "   ‚úÖ Columnas agregadas:\n",
      "      - z_score: Cu√°ntos sigmas se desv√≠a del predicho\n",
      "      - severity: Clasificaci√≥n de severidad (NORMAL, LEVE, MODERADO, SEVERO)\n",
      "      - detection_timestamp: Timestamp de detecci√≥n\n",
      "\n",
      "5Ô∏è‚É£ Guardando en tabla Delta...\n",
      "   Guardando en: s3a://gold/anomalies/contract_anomalies\n",
      "   ‚ö†Ô∏è  Error al guardar tabla Delta: An error occurred while calling o2145.save.\n",
      ": java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n",
      "\tat org.apache.hadoop.conf.Configurat\n",
      "   Intentando guardar en ruta alternativa...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 199:================>                                     (15 + 11) / 50]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Tabla guardada en: /tmp/gold_anomalies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# FASE 5: DETECCI√ìN DE AT√çPICOS (REGLA DE NEGOCIO)\n",
    "# ================================================================\n",
    "# Implementaci√≥n de la regla del tablero para detectar corrupci√≥n \n",
    "# o sobrecostos en contratos p√∫blicos\n",
    "# ================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FASE 5: DETECCI√ìN DE AT√çPICOS (REGLA DE NEGOCIO)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 1. VERIFICAR QUE TENEMOS SIGMA_LOG CALCULADO\n",
    "# ----------------------------------------------------------------\n",
    "print(\"\\n1Ô∏è‚É£ Verificando sigma calculado...\")\n",
    "\n",
    "if 'sigma_log' not in locals():\n",
    "    raise ValueError(\"‚ùå sigma_log no est√° definido. Aseg√∫rate de haber ejecutado el entrenamiento del modelo primero.\")\n",
    "\n",
    "if 'rf_predictions' not in locals():\n",
    "    raise ValueError(\"‚ùå rf_predictions no est√° definido. Aseg√∫rate de haber ejecutado las predicciones primero.\")\n",
    "\n",
    "print(f\"   ‚úÖ sigma_log disponible: {sigma_log:.6f}\")\n",
    "print(f\"   ‚úÖ Threshold 2.8œÉ: {threshold_log:.6f}\")\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 2. APLICAR REGLA DE DETECCI√ìN EN TEST SET\n",
    "# ----------------------------------------------------------------\n",
    "print(\"\\n2Ô∏è‚É£ Aplicando regla de detecci√≥n...\")\n",
    "\n",
    "# Calcular desviaci√≥n en ESCALA LOGAR√çTMICA\n",
    "# Desviaci√≥n = ValorReal - ValorPredicho\n",
    "test_with_deviation = rf_predictions.withColumn(\n",
    "    \"desviacion_log\",\n",
    "    F.col(\"label_log\") - F.col(\"prediction\")  # Real - Predicho (en escala log)\n",
    ")\n",
    "\n",
    "# Aplicar regla del tablero:\n",
    "# Si ValorReal > (ValorPredicho + 2.8œÉ) ‚Üí AT√çPICO\n",
    "# Equivalente a: Si Desviaci√≥n > 2.8œÉ ‚Üí AT√çPICO\n",
    "test_with_anomalies = test_with_deviation.withColumn(\n",
    "    \"anomaly_flag\",\n",
    "    F.when(F.col(\"desviacion_log\") > threshold_log, \"ATIPICO\")\n",
    "     .otherwise(\"LIBRE\")\n",
    ")\n",
    "\n",
    "print(f\"   ‚úÖ Regla aplicada: Desviaci√≥n > {threshold_log:.4f} ‚Üí AT√çPICO\")\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 3. CALCULAR ESTAD√çSTICAS DE DETECCI√ìN\n",
    "# ----------------------------------------------------------------\n",
    "print(\"\\n3Ô∏è‚É£ Calculando estad√≠sticas...\")\n",
    "\n",
    "# Contar at√≠picos y libres\n",
    "anomaly_counts = test_with_anomalies.groupBy(\"anomaly_flag\").count().collect()\n",
    "anomaly_dict = {row[\"anomaly_flag\"]: row[\"count\"] for row in anomaly_counts}\n",
    "\n",
    "total_contratos = test_with_anomalies.count()\n",
    "atipicos = anomaly_dict.get(\"ATIPICO\", 0)\n",
    "libres = anomaly_dict.get(\"LIBRE\", 0)\n",
    "pct_atipicos = (atipicos / total_contratos * 100) if total_contratos > 0 else 0\n",
    "\n",
    "print(f\"\\nüìä RESULTADOS DE DETECCI√ìN:\")\n",
    "print(f\"   Total contratos analizados: {total_contratos:,}\")\n",
    "print(f\"   Contratos AT√çPICOS: {atipicos:,} ({pct_atipicos:.2f}%)\")\n",
    "print(f\"   Contratos LIBRES: {libres:,} ({100-pct_atipicos:.2f}%)\")\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 4. AGREGAR COLUMNAS ADICIONALES PARA AN√ÅLISIS\n",
    "# ----------------------------------------------------------------\n",
    "print(\"\\n4Ô∏è‚É£ Enriqueciendo datos...\")\n",
    "\n",
    "# Calcular Z-score (cu√°ntos sigmas se desv√≠a)\n",
    "gold_anomalies = test_with_anomalies.withColumn(\n",
    "    \"z_score\",\n",
    "    F.col(\"desviacion_log\") / sigma_log\n",
    ").withColumn(\n",
    "    \"sigma_threshold\",\n",
    "    F.lit(threshold_log)\n",
    ").withColumn(\n",
    "    \"detection_timestamp\",\n",
    "    F.current_timestamp()\n",
    ").withColumn(\n",
    "    \"model_version\",\n",
    "    F.lit(\"rf_log_v1\")\n",
    ")\n",
    "\n",
    "# Agregar explicaci√≥n del z-score\n",
    "gold_anomalies = gold_anomalies.withColumn(\n",
    "    \"severity\",\n",
    "    F.when(F.col(\"z_score\") <= 2.8, \"NORMAL\")\n",
    "     .when((F.col(\"z_score\") > 2.8) & (F.col(\"z_score\") <= 3.5), \"LEVE\")\n",
    "     .when((F.col(\"z_score\") > 3.5) & (F.col(\"z_score\") <= 4.5), \"MODERADO\")\n",
    "     .when(F.col(\"z_score\") > 4.5, \"SEVERO\")\n",
    "     .otherwise(\"NORMAL\")\n",
    ")\n",
    "\n",
    "print(\"   ‚úÖ Columnas agregadas:\")\n",
    "print(\"      - z_score: Cu√°ntos sigmas se desv√≠a del predicho\")\n",
    "print(\"      - severity: Clasificaci√≥n de severidad (NORMAL, LEVE, MODERADO, SEVERO)\")\n",
    "print(\"      - detection_timestamp: Timestamp de detecci√≥n\")\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 5. GUARDAR EN TABLA DELTA: gold_anomalies\n",
    "# ----------------------------------------------------------------\n",
    "print(\"\\n5Ô∏è‚É£ Guardando en tabla Delta...\")\n",
    "\n",
    "# Definir ruta de salida\n",
    "output_path = \"s3a://gold/anomalies/contract_anomalies\"\n",
    "\n",
    "# Seleccionar columnas finales\n",
    "gold_anomalies_final = gold_anomalies.select(\n",
    "    \"valor_contrato\",\n",
    "    F.col(\"label_log\").alias(\"valor_log\"),\n",
    "    F.col(\"prediction\").alias(\"valor_predicho_log\"),\n",
    "    \"desviacion_log\",\n",
    "    \"z_score\",\n",
    "    \"sigma_threshold\",\n",
    "    \"anomaly_flag\",\n",
    "    \"severity\",\n",
    "    \"detection_timestamp\",\n",
    "    \"model_version\"\n",
    ")\n",
    "\n",
    "print(f\"   Guardando en: {output_path}\")\n",
    "\n",
    "try:\n",
    "    # Guardar como tabla Delta\n",
    "    gold_anomalies_final.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .save(output_path)\n",
    "    \n",
    "    print(\"   ‚úÖ Tabla Delta guardada exitosamente\")\n",
    "    \n",
    "    # Registrar en el cat√°logo de Spark\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS gold.anomalies\n",
    "        USING DELTA\n",
    "        LOCATION '{output_path}'\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"   ‚úÖ Tabla registrada en cat√°logo: gold.anomalies\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è  Error al guardar tabla Delta: {str(e)[:200]}\")\n",
    "    print(\"   Intentando guardar en ruta alternativa...\")\n",
    "    \n",
    "    # Ruta alternativa local\n",
    "    output_path_local = \"/tmp/gold_anomalies\"\n",
    "    gold_anomalies_final.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .save(output_path_local)\n",
    "    \n",
    "    print(f\"   ‚úÖ Tabla guardada en: {output_path_local}\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e51860-8eb5-48ee-bfc1-b9ce94addbe0",
   "metadata": {},
   "source": [
    "## Fase 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad88a16-ffd9-4187-8ebf-8814b1c53126",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
