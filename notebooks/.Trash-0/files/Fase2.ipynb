{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df0fd561-397a-4b98-bd8d-03e90bea6454",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Fase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28ef8e94-16fd-4be8-bf98-8cd5a51ae9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-bda52606-2bf2-4a26-a4d2-aef53be004ae;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.3 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "\tfound io.delta#delta-spark_2.12;3.0.0 in central\n",
      "\tfound io.delta#delta-storage;3.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.5.1/spark-sql-kafka-0-10_2.12-3.5.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1!spark-sql-kafka-0-10_2.12.jar (120ms)\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-spark_2.12/3.0.0/delta-spark_2.12-3.0.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-spark_2.12;3.0.0!delta-spark_2.12.jar (351ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.5.1/spark-token-provider-kafka-0-10_2.12-3.5.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1!spark-token-provider-kafka-0-10_2.12.jar (103ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.4.1/kafka-clients-3.4.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kafka#kafka-clients;3.4.1!kafka-clients.jar (444ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar ...\n",
      "\t[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.0!jsr305.jar (68ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-pool2;2.11.1!commons-pool2.jar (87ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.4/hadoop-client-runtime-3.3.4.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-runtime;3.3.4!hadoop-client-runtime.jar (1740ms)\n",
      "downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.8.0/lz4-java-1.8.0.jar ...\n",
      "\t[SUCCESSFUL ] org.lz4#lz4-java;1.8.0!lz4-java.jar (101ms)\n",
      "downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.10.3/snappy-java-1.1.10.3.jar ...\n",
      "\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.10.3!snappy-java.jar(bundle) (142ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/2.0.7/slf4j-api-2.0.7.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;2.0.7!slf4j-api.jar (145ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.4/hadoop-client-api-3.3.4.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-api;3.3.4!hadoop-client-api.jar (1177ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar ...\n",
      "\t[SUCCESSFUL ] commons-logging#commons-logging;1.1.3!commons-logging.jar (127ms)\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-storage/3.0.0/delta-storage-3.0.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-storage;3.0.0!delta-storage.jar (156ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr4-runtime/4.9.3/antlr4-runtime-4.9.3.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr4-runtime;4.9.3!antlr4-runtime.jar (88ms)\n",
      ":: resolution report :: resolve 9365ms :: artifacts dl 4892ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tio.delta#delta-spark_2.12;3.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.0.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   14  |   14  |   14  |   0   ||   14  |   14  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-bda52606-2bf2-4a26-a4d2-aef53be004ae\n",
      "\tconfs: [default]\n",
      "\t14 artifacts copied, 0 already retrieved (61997kB/2161ms)\n",
      "25/12/09 00:11:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Spark 3.5.1 iniciado\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FASE 2 - OPTIMIZADO PARA SPARK 3.5.1 + DELTA LAKE 3.0\n",
    "# ============================================================================\n",
    "\n",
    "# PASO 0: REINICIAR SPARK CON VERSIONES CORRECTAS\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "import time\n",
    "time.sleep(3)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, lower, regexp_replace, translate, length, trim\n",
    "\n",
    "from pyspark.ml.feature import (\n",
    "    Tokenizer, StopWordsRemover, Word2Vec, \n",
    "    StringIndexer, OneHotEncoder, VectorAssembler,\n",
    "    StandardScaler, PCA\n",
    ")\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.stat import Correlation\n",
    "import numpy as np\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Bronze_to_Silver_Optimized\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1,\"\n",
    "            \"io.delta:delta-spark_2.12:3.0.0\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config(\"spark.executor.memory\", \"4g\")\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"50\")\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "print(f\" Spark {spark.version} iniciado\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fd17a2e-563a-48c1-8f15-3c3ec7900ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PASO 1: LECTURA DE KAFKA\n",
      "================================================================================\n",
      "\n",
      "Leyendo Kafka...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mensajes: 50,349\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 1. LECTURA DE KAFKA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 1: LECTURA DE KAFKA\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "contract_schema = StructType([\n",
    "    StructField(\"id_contrato\", StringType()),\n",
    "    StructField(\"objeto_contrato\", StringType()),\n",
    "    StructField(\"entidad\", StringType()),\n",
    "    StructField(\"departamento\", StringType()),\n",
    "    StructField(\"municipio\", StringType()),\n",
    "    StructField(\"region\", StringType()),\n",
    "    StructField(\"codigo_unspsc\", StringType()),\n",
    "    StructField(\"descripcion_categoria\", StringType()),\n",
    "    StructField(\"valor_contrato\", DoubleType()),\n",
    "    StructField(\"duracion_dias\", IntegerType()),\n",
    "    StructField(\"fecha_firma\", StringType()),\n",
    "    StructField(\"tipo_contrato\", StringType()),\n",
    "    StructField(\"estado_contrato\", StringType()),\n",
    "    StructField(\"modalidad\", StringType()),\n",
    "    StructField(\"anno\", IntegerType()),\n",
    "    StructField(\"id_interno_sistema\", StringType()),\n",
    "    StructField(\"campo_vacio\", StringType()),\n",
    "    StructField(\"constante_1\", StringType()),\n",
    "    StructField(\"constante_2\", IntegerType()),\n",
    "    StructField(\"duplicate_id\", StringType()),\n",
    "    StructField(\"timestamp_carga\", StringType())\n",
    "])\n",
    "\n",
    "print(\"Leyendo Kafka...\")\n",
    "\n",
    "df_kafka = spark.read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", \"contratos-publicos\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "df_bronze = df_kafka.select(\n",
    "    from_json(col(\"value\").cast(\"string\"), contract_schema).alias(\"data\")\n",
    ").select(\"data.*\")\n",
    "\n",
    "df_bronze = df_bronze.cache()\n",
    "total_kafka = df_bronze.count()\n",
    "\n",
    "print(f\" Mensajes: {total_kafka:,}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f952457-ec33-4c40-b285-f2d1e68c3357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PASO 2: ELIMINAR REDUNDANTES\n",
      "================================================================================\n",
      "\n",
      " Eliminando 6 columnas redundantes\n",
      " Columnas restantes: 15\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[id_contrato: string, objeto_contrato: string, entidad: string, departamento: string, municipio: string, region: string, codigo_unspsc: string, descripcion_categoria: string, valor_contrato: double, duracion_dias: int, fecha_firma: string, tipo_contrato: string, estado_contrato: string, modalidad: string, anno: int, id_interno_sistema: string, campo_vacio: string, constante_1: string, constante_2: int, duplicate_id: string, timestamp_carga: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 2. ELIMINAR REDUNDANTES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 2: ELIMINAR REDUNDANTES\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "redundant_columns = [\n",
    "    \"id_interno_sistema\", \"campo_vacio\", \"constante_1\",\n",
    "    \"constante_2\", \"duplicate_id\", \"timestamp_carga\"\n",
    "]\n",
    "\n",
    "print(f\" Eliminando {len(redundant_columns)} columnas redundantes\")\n",
    "\n",
    "df_cleaned = df_bronze.drop(*redundant_columns)\n",
    "\n",
    "print(f\" Columnas restantes: {len(df_cleaned.columns)}\\n\")\n",
    "\n",
    "#  LIBERAR bronze, ya no lo necesitamos\n",
    "df_bronze.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "816b1e7b-d82b-439c-acbf-057230ec288d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PASO 3: LIMPIEZA\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Registros: 50,349\n",
      "\n",
      "Columnas con nulos:\n",
      "   duracion_dias: 50,349 (100.0%)\n",
      "\n",
      " Limpieza:\n",
      "   Antes: 50,349\n",
      "   DespuÃ©s: 50,058\n",
      "   Descartados: 291\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[id_contrato: string, objeto_contrato: string, entidad: string, departamento: string, municipio: string, region: string, codigo_unspsc: string, descripcion_categoria: string, valor_contrato: double, duracion_dias: int, fecha_firma: string, tipo_contrato: string, estado_contrato: string, modalidad: string, anno: int]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 3. LIMPIEZA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 3: LIMPIEZA\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "df_cleaned = df_cleaned.cache()\n",
    "total_cleaned = df_cleaned.count()\n",
    "\n",
    "print(f\" Registros: {total_cleaned:,}\\n\")\n",
    "\n",
    "# AnÃ¡lisis de nulos optimizado\n",
    "null_counts = df_cleaned.select([\n",
    "    count(when(col(c).isNull(), c)).alias(c) \n",
    "    for c in df_cleaned.columns\n",
    "]).collect()[0].asDict()\n",
    "\n",
    "print(\"Columnas con nulos:\")\n",
    "for col_name, null_count in sorted(null_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    if null_count > 0:\n",
    "        pct = (null_count / total_cleaned) * 100\n",
    "        print(f\"   {col_name}: {null_count:,} ({pct:.1f}%)\")\n",
    "\n",
    "# Limpieza\n",
    "df_silver = df_cleaned \\\n",
    "    .filter(col(\"id_contrato\").isNotNull()) \\\n",
    "    .filter(col(\"objeto_contrato\").isNotNull()) \\\n",
    "    .filter(col(\"valor_contrato\").isNotNull()) \\\n",
    "    .filter(col(\"valor_contrato\") > 0) \\\n",
    "    .filter(col(\"fecha_firma\").isNotNull()) \\\n",
    "    .withColumn(\"fecha_firma\", to_date(col(\"fecha_firma\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "df_silver = df_silver.cache()\n",
    "total_silver = df_silver.count()\n",
    "\n",
    "print(f\"\\n Limpieza:\")\n",
    "print(f\"   Antes: {total_cleaned:,}\")\n",
    "print(f\"   DespuÃ©s: {total_silver:,}\")\n",
    "print(f\"   Descartados: {total_cleaned - total_silver:,}\\n\")\n",
    "\n",
    "df_cleaned.unpersist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82037c71-8bfc-4e99-b7fe-f3f24287fe73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PASO 4: ESTADÃSTICAS\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Por regiÃ³n:\n",
      "+--------------+-----+\n",
      "|        region|count|\n",
      "+--------------+-----+\n",
      "|Centro-Oriente|50058|\n",
      "+--------------+-----+\n",
      "\n",
      "\n",
      "ðŸ“Š Top 5 entidades:\n",
      "+-------------------------------------------------+-----+\n",
      "|entidad                                          |count|\n",
      "+-------------------------------------------------+-----+\n",
      "|MUNICIPIO DE SOACHA.                             |3184 |\n",
      "|ALCALDÃA MUNICIPAL COTA                          |1995 |\n",
      "|ESE MUNICIPAL DE SOACHA JULIO CESAR PEÃ‘ALOZA*    |1919 |\n",
      "|CUNDINAMARCA-ALCALDIA MUNICIPIO MOSQUERA         |1879 |\n",
      "|empresa social del estado regiÃ³n de salud soacha.|1579 |\n",
      "+-------------------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 4. ESTADÃSTICAS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 4: ESTADÃSTICAS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"ðŸ“Š Por regiÃ³n:\")\n",
    "df_silver.groupBy(\"region\").count().orderBy(desc(\"count\")).show(5)\n",
    "\n",
    "print(\"\\nðŸ“Š Top 5 entidades:\")\n",
    "df_silver.groupBy(\"entidad\").count().orderBy(desc(\"count\")).show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af77614e-c5ff-4b86-91e2-23ce1d5a253d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PASO 5: GUARDAR EN DELTA LAKE\n",
      "================================================================================\n",
      "\n",
      "ðŸ’¾ Guardando en: /app/notebooks/delta_lake/silver_contracts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Guardado exitosamente\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 5. GUARDAR EN DELTA LAKE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 5: GUARDAR EN DELTA LAKE\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "DELTA_PATH = \"/app/notebooks/delta_lake/silver_contracts\"\n",
    "\n",
    "print(f\"ðŸ’¾ Guardando en: {DELTA_PATH}\")\n",
    "\n",
    "df_silver.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .save(DELTA_PATH)\n",
    "\n",
    "print(\"âœ… Guardado exitosamente\\n\")\n",
    "\n",
    "# âš ï¸ LIBERAR todo\n",
    "df_silver.unpersist()\n",
    "spark.catalog.clearCache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b60ee146-c2f4-4f23-ae53-f98ded14fb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.1\n"
     ]
    }
   ],
   "source": [
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "244fd81e-3b09-4646-99b4-d88f61d8abd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "VERIFICACIÃ“N FINAL\n",
      "================================================================================\n",
      "\n",
      "âœ… Registros verificados: 50,058\n",
      "+------------+----------------------------------------------------+--------------+-----------+\n",
      "|id_contrato |entidad                                             |valor_contrato|fecha_firma|\n",
      "+------------+----------------------------------------------------+--------------+-----------+\n",
      "|011-2024    |ESE HOSPITAL SALAZAR DE VILLETA                     |2.9991E7      |2024-01-01 |\n",
      "|CPS 012-2024|E.S.E HOSPITAL NUESTRA SEÃ‘ORA DEL CARMEN DEL COLEGIO|1.07844E7     |2024-01-01 |\n",
      "|CPS 017-2024|E.S.E HOSPITAL NUESTRA SEÃ‘ORA DEL CARMEN DEL COLEGIO|1.07844E7     |2024-01-01 |\n",
      "|004-2024    |ESE HOSPITAL SALAZAR DE VILLETA                     |2.016E7       |2024-01-01 |\n",
      "|CPS-042-2024|empresa social del estado regiÃ³n de salud soacha.   |7.5323616E7   |2024-01-01 |\n",
      "+------------+----------------------------------------------------+--------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "ðŸŽ¯ Fase 2 completada. Siguiente: Fase 3 - Embeddings\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 6. VERIFICACIÃ“N\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"VERIFICACIÃ“N FINAL\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "df_verify = spark.read.format(\"delta\").load(DELTA_PATH)\n",
    "print(f\"âœ… Registros verificados: {df_verify.count():,}\")\n",
    "\n",
    "df_verify.select(\"id_contrato\", \"entidad\", \"valor_contrato\", \"fecha_firma\") \\\n",
    "    .show(5, truncate=False)\n",
    "\n",
    "print(\"\\nðŸŽ¯ Fase 2 completada. Siguiente: Fase 3 - Embeddings\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb79ca8-dc51-4ddd-9465-fcf1d636394b",
   "metadata": {},
   "source": [
    "## Fase 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f353f8e1-7564-40cc-ae3f-bc39a496b980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PASO 1: CARGAR DATOS DESDE SILVER\n",
      "================================================================================\n",
      "\n",
      "ðŸ“– Cargando datos desde: /app/notebooks/delta_lake/silver_contracts\n",
      "âœ… Registros cargados: 50,058\n",
      "\n",
      "ðŸ“‹ Esquema de datos:\n",
      "root\n",
      " |-- id_contrato: string (nullable = true)\n",
      " |-- objeto_contrato: string (nullable = true)\n",
      " |-- entidad: string (nullable = true)\n",
      " |-- departamento: string (nullable = true)\n",
      " |-- municipio: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- codigo_unspsc: string (nullable = true)\n",
      " |-- descripcion_categoria: string (nullable = true)\n",
      " |-- valor_contrato: double (nullable = true)\n",
      " |-- duracion_dias: integer (nullable = true)\n",
      " |-- fecha_firma: date (nullable = true)\n",
      " |-- tipo_contrato: string (nullable = true)\n",
      " |-- estado_contrato: string (nullable = true)\n",
      " |-- modalidad: string (nullable = true)\n",
      " |-- anno: integer (nullable = true)\n",
      "\n",
      "\n",
      "ðŸ“Š Muestra de datos:\n",
      "+------------+---------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------+-------------+--------------+-------------+\n",
      "|id_contrato |objeto_contrato                                                                                                                        |entidad                                             |codigo_unspsc|valor_contrato|duracion_dias|\n",
      "+------------+---------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------+-------------+--------------+-------------+\n",
      "|011-2024    |PRESTACION DE SERVICIOS ASISTENCIALES COMO MEDICO GENERAL EN LOS PROCESOS Y SUBPROCESOS DE LA ESE HOSPITAL SALAZAR DE VILLETA          |ESE HOSPITAL SALAZAR DE VILLETA                     |             |2.9991E7      |NULL         |\n",
      "|CPS 012-2024|PRESTAR APOYO AL PROCESO ASISTENCIAL EN EL ÃREA DE AUXILIAR DE ENFERMERÃA A LA E.S.E. HOSPITAL NUESTRA SEÃ‘ORA DEL CARMEN DE EL COLEGIO.|E.S.E HOSPITAL NUESTRA SEÃ‘ORA DEL CARMEN DEL COLEGIO|             |1.07844E7     |NULL         |\n",
      "|CPS 017-2024|PRESTAR APOYO AL PROCESO ASISTENCIAL EN EL ÃREA DE FARMACIA-TÃ‰CNICA PARA LA E.S.E. HOSPITAL NUESTRA SEÃ‘ORA DEL CARMEN DE EL COLEGIO.   |E.S.E HOSPITAL NUESTRA SEÃ‘ORA DEL CARMEN DEL COLEGIO|             |1.07844E7     |NULL         |\n",
      "+------------+---------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------+-------------+--------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 1. CARGAR DATOS DESDE SILVER\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PASO 1: CARGAR DATOS DESDE SILVER\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "SILVER_PATH = \"/app/notebooks/delta_lake/silver_contracts\"\n",
    "\n",
    "print(f\"ðŸ“– Cargando datos desde: {SILVER_PATH}\")\n",
    "\n",
    "df_silver = spark.read.format(\"delta\").load(SILVER_PATH)\n",
    "df_silver = df_silver.cache()\n",
    "\n",
    "total_records = df_silver.count()\n",
    "print(f\"âœ… Registros cargados: {total_records:,}\\n\")\n",
    "\n",
    "print(\"ðŸ“‹ Esquema de datos:\")\n",
    "df_silver.printSchema()\n",
    "\n",
    "print(\"\\nðŸ“Š Muestra de datos:\")\n",
    "df_silver.select(\n",
    "    \"id_contrato\", \"objeto_contrato\", \"entidad\", \n",
    "    \"codigo_unspsc\", \"valor_contrato\", \"duracion_dias\"\n",
    ").show(3, truncate=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18f418b5-05f7-4432-a3e9-ca4bc34e5141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PASO 2: LIMPIEZA Y PREPARACIÃ“N DE TEXTO\n",
      "================================================================================\n",
      "\n",
      "ðŸ§¹ Limpiando columna 'objeto_contrato'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Registros despuÃ©s de limpieza: 50,058\n",
      "\n",
      "ðŸ“‹ Ejemplo de texto limpio:\n",
      "+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+\n",
      "|                                                                 objeto_contrato|                                                                   objeto_limpio|\n",
      "+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+\n",
      "|PRESTACION DE SERVICIOS ASISTENCIALES COMO MEDICO GENERAL EN LOS PROCESOS Y S...|prestacion de servicios asistenciales como medico general en los procesos y s...|\n",
      "|PRESTAR APOYO AL PROCESO ASISTENCIAL EN EL ÃREA DE AUXILIAR DE ENFERMERÃA A L...|prestar apoyo al proceso asistencial en el rea de auxiliar de enfermer a a la...|\n",
      "+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 2. LIMPIEZA Y PREPARACIÃ“N DE TEXTO\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PASO 2: LIMPIEZA Y PREPARACIÃ“N DE TEXTO\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"ðŸ§¹ Limpiando columna 'objeto_contrato'...\")\n",
    "\n",
    "# Limpiar y normalizar texto\n",
    "df_prepared = df_silver.withColumn(\n",
    "    \"objeto_limpio\",\n",
    "    lower(\n",
    "        regexp_replace(\n",
    "            regexp_replace(col(\"objeto_contrato\"), \"[^a-zA-ZÃ¡Ã©Ã­Ã³ÃºÃ±Ã‘0-9\\\\s]\", \" \"),\n",
    "            \"\\\\s+\", \" \"\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# Filtrar textos muy cortos (menos de 10 caracteres)\n",
    "df_prepared = df_prepared.filter(length(col(\"objeto_limpio\")) >= 10)\n",
    "\n",
    "print(f\"âœ… Registros despuÃ©s de limpieza: {df_prepared.count():,}\")\n",
    "\n",
    "print(\"\\nðŸ“‹ Ejemplo de texto limpio:\")\n",
    "df_prepared.select(\"objeto_contrato\", \"objeto_limpio\").show(2, truncate=80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2876373e-947e-437c-9221-2bbf2dd7e24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PASO 3: TOKENIZACIÃ“N Y REMOCIÃ“N DE STOPWORDS\n",
      "================================================================================\n",
      "\n",
      "âœ… TokenizaciÃ³n completada\n",
      "\n",
      "ðŸ“‹ Ejemplo de tokens:\n",
      "+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+\n",
      "|                                                                   objeto_limpio|                                                                        palabras|\n",
      "+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+\n",
      "|prestacion de servicios asistenciales como medico general en los procesos y s...|[prestacion, de, servicios, asistenciales, como, medico, general, en, los, pr...|\n",
      "|prestar apoyo al proceso asistencial en el rea de auxiliar de enfermer a a la...|[prestar, apoyo, al, proceso, asistencial, en, el, rea, de, auxiliar, de, enf...|\n",
      "+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "âœ… Stopwords removidas\n",
      "\n",
      "ðŸ“‹ ComparaciÃ³n antes/despuÃ©s de stopwords:\n",
      "+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+\n",
      "|                                                                        palabras|                                                              palabras_filtradas|\n",
      "+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+\n",
      "|[prestacion, de, servicios, asistenciales, como, medico, general, en, los, pr...|[prestacion, servicios, asistenciales, medico, general, procesos, subprocesos...|\n",
      "|[prestar, apoyo, al, proceso, asistencial, en, el, rea, de, auxiliar, de, enf...|[prestar, apoyo, proceso, asistencial, rea, auxiliar, enfermer, e, s, e, hosp...|\n",
      "+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 50:=============================>                            (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Registros con palabras vÃ¡lidas: 50,058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 3. TOKENIZACIÃ“N Y STOPWORDS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PASO 3: TOKENIZACIÃ“N Y REMOCIÃ“N DE STOPWORDS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# TokenizaciÃ³n\n",
    "tokenizer = Tokenizer(inputCol=\"objeto_limpio\", outputCol=\"palabras\")\n",
    "df_tokenized = tokenizer.transform(df_prepared)\n",
    "\n",
    "print(\"âœ… TokenizaciÃ³n completada\")\n",
    "print(\"\\nðŸ“‹ Ejemplo de tokens:\")\n",
    "df_tokenized.select(\"objeto_limpio\", \"palabras\").show(2, truncate=80)\n",
    "\n",
    "# Stopwords en espaÃ±ol\n",
    "stopwords_es = [\n",
    "    \"el\", \"la\", \"de\", \"que\", \"y\", \"a\", \"en\", \"un\", \"ser\", \"se\", \"no\", \"haber\",\n",
    "    \"por\", \"con\", \"su\", \"para\", \"como\", \"estar\", \"tener\", \"le\", \"lo\", \"todo\",\n",
    "    \"pero\", \"mÃ¡s\", \"hacer\", \"o\", \"poder\", \"decir\", \"este\", \"ir\", \"otro\", \"ese\",\n",
    "    \"si\", \"me\", \"ya\", \"ver\", \"porque\", \"dar\", \"cuando\", \"Ã©l\", \"muy\", \"sin\",\n",
    "    \"vez\", \"mucho\", \"saber\", \"quÃ©\", \"sobre\", \"mi\", \"alguno\", \"mismo\", \"yo\",\n",
    "    \"tambiÃ©n\", \"hasta\", \"aÃ±o\", \"dos\", \"querer\", \"entre\", \"asÃ­\", \"primero\",\n",
    "    \"desde\", \"grande\", \"eso\", \"ni\", \"nos\", \"llegar\", \"pasar\", \"tiempo\", \"ella\",\n",
    "    \"del\", \"al\", \"los\", \"las\", \"uno\", \"una\", \"unos\", \"unas\"\n",
    "]\n",
    "\n",
    "# Remover stopwords\n",
    "remover = StopWordsRemover(\n",
    "    inputCol=\"palabras\", \n",
    "    outputCol=\"palabras_filtradas\",\n",
    "    stopWords=stopwords_es\n",
    ")\n",
    "df_filtered = remover.transform(df_tokenized)\n",
    "\n",
    "print(\"âœ… Stopwords removidas\")\n",
    "print(\"\\nðŸ“‹ ComparaciÃ³n antes/despuÃ©s de stopwords:\")\n",
    "df_filtered.select(\"palabras\", \"palabras_filtradas\").show(2, truncate=80)\n",
    "\n",
    "# Filtrar documentos sin palabras despuÃ©s de stopwords\n",
    "df_filtered = df_filtered.filter(size(col(\"palabras_filtradas\")) > 0)\n",
    "\n",
    "print(f\"\\nðŸ“Š Registros con palabras vÃ¡lidas: {df_filtered.count():,}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6d8a14-d303-454a-ae88-91f3c95a00b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
