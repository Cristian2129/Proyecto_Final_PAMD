{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28ef8e94-16fd-4be8-bf98-8cd5a51ae9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-3c1b76ef-2078-463a-8103-daaea123be2e;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.3 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "\tfound io.delta#delta-spark_2.12;3.0.0 in central\n",
      "\tfound io.delta#delta-storage;3.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.5.1/spark-sql-kafka-0-10_2.12-3.5.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1!spark-sql-kafka-0-10_2.12.jar (228ms)\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-spark_2.12/3.0.0/delta-spark_2.12-3.0.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-spark_2.12;3.0.0!delta-spark_2.12.jar (756ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.5.1/spark-token-provider-kafka-0-10_2.12-3.5.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1!spark-token-provider-kafka-0-10_2.12.jar (176ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.4.1/kafka-clients-3.4.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kafka#kafka-clients;3.4.1!kafka-clients.jar (854ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.4/hadoop-client-runtime-3.3.4.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-runtime;3.3.4!hadoop-client-runtime.jar (4512ms)\n",
      "downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.10.3/snappy-java-1.1.10.3.jar ...\n",
      "\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.10.3!snappy-java.jar(bundle) (319ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/2.0.7/slf4j-api-2.0.7.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;2.0.7!slf4j-api.jar (140ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.4/hadoop-client-api-3.3.4.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-api;3.3.4!hadoop-client-api.jar (2412ms)\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-storage/3.0.0/delta-storage-3.0.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-storage;3.0.0!delta-storage.jar (112ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr4-runtime/4.9.3/antlr4-runtime-4.9.3.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr4-runtime;4.9.3!antlr4-runtime.jar (640ms)\n",
      ":: resolution report :: resolve 6962ms :: artifacts dl 10171ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tio.delta#delta-spark_2.12;3.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.0.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   14  |   10  |   10  |   0   ||   14  |   10  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-3c1b76ef-2078-463a-8103-daaea123be2e\n",
      "\tconfs: [default]\n",
      "\t10 artifacts copied, 4 already retrieved (61095kB/774ms)\n",
      "25/12/06 00:54:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Spark 3.5.1 iniciado\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FASE 2 - OPTIMIZADO PARA SPARK 3.5.1 + DELTA LAKE 3.0\n",
    "# ============================================================================\n",
    "\n",
    "# PASO 0: REINICIAR SPARK CON VERSIONES CORRECTAS\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "import time\n",
    "time.sleep(3)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Bronze_to_Silver_Optimized\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1,\"\n",
    "            \"io.delta:delta-spark_2.12:3.0.0\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config(\"spark.executor.memory\", \"4g\")\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"50\")\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "print(f\" Spark {spark.version} iniciado\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fd17a2e-563a-48c1-8f15-3c3ec7900ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PASO 1: LECTURA DE KAFKA\n",
      "================================================================================\n",
      "\n",
      "Leyendo Kafka...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mensajes: 50,349\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 1. LECTURA DE KAFKA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 1: LECTURA DE KAFKA\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "contract_schema = StructType([\n",
    "    StructField(\"id_contrato\", StringType()),\n",
    "    StructField(\"objeto_contrato\", StringType()),\n",
    "    StructField(\"entidad\", StringType()),\n",
    "    StructField(\"departamento\", StringType()),\n",
    "    StructField(\"municipio\", StringType()),\n",
    "    StructField(\"region\", StringType()),\n",
    "    StructField(\"codigo_unspsc\", StringType()),\n",
    "    StructField(\"descripcion_categoria\", StringType()),\n",
    "    StructField(\"valor_contrato\", DoubleType()),\n",
    "    StructField(\"duracion_dias\", IntegerType()),\n",
    "    StructField(\"fecha_firma\", StringType()),\n",
    "    StructField(\"tipo_contrato\", StringType()),\n",
    "    StructField(\"estado_contrato\", StringType()),\n",
    "    StructField(\"modalidad\", StringType()),\n",
    "    StructField(\"anno\", IntegerType()),\n",
    "    StructField(\"id_interno_sistema\", StringType()),\n",
    "    StructField(\"campo_vacio\", StringType()),\n",
    "    StructField(\"constante_1\", StringType()),\n",
    "    StructField(\"constante_2\", IntegerType()),\n",
    "    StructField(\"duplicate_id\", StringType()),\n",
    "    StructField(\"timestamp_carga\", StringType())\n",
    "])\n",
    "\n",
    "print(\"Leyendo Kafka...\")\n",
    "\n",
    "df_kafka = spark.read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", \"contratos-publicos\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "df_bronze = df_kafka.select(\n",
    "    from_json(col(\"value\").cast(\"string\"), contract_schema).alias(\"data\")\n",
    ").select(\"data.*\")\n",
    "\n",
    "df_bronze = df_bronze.cache()\n",
    "total_kafka = df_bronze.count()\n",
    "\n",
    "print(f\" Mensajes: {total_kafka:,}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f952457-ec33-4c40-b285-f2d1e68c3357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PASO 2: ELIMINAR REDUNDANTES\n",
      "================================================================================\n",
      "\n",
      " Eliminando 6 columnas redundantes\n",
      " Columnas restantes: 15\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[id_contrato: string, objeto_contrato: string, entidad: string, departamento: string, municipio: string, region: string, codigo_unspsc: string, descripcion_categoria: string, valor_contrato: double, duracion_dias: int, fecha_firma: string, tipo_contrato: string, estado_contrato: string, modalidad: string, anno: int, id_interno_sistema: string, campo_vacio: string, constante_1: string, constante_2: int, duplicate_id: string, timestamp_carga: string]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 2. ELIMINAR REDUNDANTES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 2: ELIMINAR REDUNDANTES\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "redundant_columns = [\n",
    "    \"id_interno_sistema\", \"campo_vacio\", \"constante_1\",\n",
    "    \"constante_2\", \"duplicate_id\", \"timestamp_carga\"\n",
    "]\n",
    "\n",
    "print(f\" Eliminando {len(redundant_columns)} columnas redundantes\")\n",
    "\n",
    "df_cleaned = df_bronze.drop(*redundant_columns)\n",
    "\n",
    "print(f\" Columnas restantes: {len(df_cleaned.columns)}\\n\")\n",
    "\n",
    "#  LIBERAR bronze, ya no lo necesitamos\n",
    "df_bronze.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "816b1e7b-d82b-439c-acbf-057230ec288d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PASO 3: LIMPIEZA\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Registros: 50,349\n",
      "\n",
      "Columnas con nulos:\n",
      "   duracion_dias: 50,349 (100.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Limpieza:\n",
      "   Antes: 50,349\n",
      "   Despu√©s: 50,058\n",
      "   Descartados: 291\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[id_contrato: string, objeto_contrato: string, entidad: string, departamento: string, municipio: string, region: string, codigo_unspsc: string, descripcion_categoria: string, valor_contrato: double, duracion_dias: int, fecha_firma: string, tipo_contrato: string, estado_contrato: string, modalidad: string, anno: int]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 3. LIMPIEZA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 3: LIMPIEZA\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "df_cleaned = df_cleaned.cache()\n",
    "total_cleaned = df_cleaned.count()\n",
    "\n",
    "print(f\" Registros: {total_cleaned:,}\\n\")\n",
    "\n",
    "# An√°lisis de nulos optimizado\n",
    "null_counts = df_cleaned.select([\n",
    "    count(when(col(c).isNull(), c)).alias(c) \n",
    "    for c in df_cleaned.columns\n",
    "]).collect()[0].asDict()\n",
    "\n",
    "print(\"Columnas con nulos:\")\n",
    "for col_name, null_count in sorted(null_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    if null_count > 0:\n",
    "        pct = (null_count / total_cleaned) * 100\n",
    "        print(f\"   {col_name}: {null_count:,} ({pct:.1f}%)\")\n",
    "\n",
    "# Limpieza\n",
    "df_silver = df_cleaned \\\n",
    "    .filter(col(\"id_contrato\").isNotNull()) \\\n",
    "    .filter(col(\"objeto_contrato\").isNotNull()) \\\n",
    "    .filter(col(\"valor_contrato\").isNotNull()) \\\n",
    "    .filter(col(\"valor_contrato\") > 0) \\\n",
    "    .filter(col(\"fecha_firma\").isNotNull()) \\\n",
    "    .withColumn(\"fecha_firma\", to_date(col(\"fecha_firma\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "df_silver = df_silver.cache()\n",
    "total_silver = df_silver.count()\n",
    "\n",
    "print(f\"\\n Limpieza:\")\n",
    "print(f\"   Antes: {total_cleaned:,}\")\n",
    "print(f\"   Despu√©s: {total_silver:,}\")\n",
    "print(f\"   Descartados: {total_cleaned - total_silver:,}\\n\")\n",
    "\n",
    "df_cleaned.unpersist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82037c71-8bfc-4e99-b7fe-f3f24287fe73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PASO 4: ESTAD√çSTICAS\n",
      "================================================================================\n",
      "\n",
      "üìä Por regi√≥n:\n",
      "+--------------+-----+\n",
      "|        region|count|\n",
      "+--------------+-----+\n",
      "|Centro-Oriente|50058|\n",
      "+--------------+-----+\n",
      "\n",
      "\n",
      "üìä Top 5 entidades:\n",
      "+-------------------------------------------------+-----+\n",
      "|entidad                                          |count|\n",
      "+-------------------------------------------------+-----+\n",
      "|MUNICIPIO DE SOACHA.                             |3182 |\n",
      "|ALCALD√çA MUNICIPAL COTA                          |1994 |\n",
      "|ESE MUNICIPAL DE SOACHA JULIO CESAR PE√ëALOZA*    |1919 |\n",
      "|CUNDINAMARCA-ALCALDIA MUNICIPIO MOSQUERA         |1883 |\n",
      "|empresa social del estado regi√≥n de salud soacha.|1579 |\n",
      "+-------------------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 4. ESTAD√çSTICAS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 4: ESTAD√çSTICAS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"üìä Por regi√≥n:\")\n",
    "df_silver.groupBy(\"region\").count().orderBy(desc(\"count\")).show(5)\n",
    "\n",
    "print(\"\\nüìä Top 5 entidades:\")\n",
    "df_silver.groupBy(\"entidad\").count().orderBy(desc(\"count\")).show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af77614e-c5ff-4b86-91e2-23ce1d5a253d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PASO 5: GUARDAR EN DELTA LAKE\n",
      "================================================================================\n",
      "\n",
      "üíæ Guardando en: /app/notebooks/delta_lake/silver_contracts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:=============================================>          (41 + 9) / 50]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Guardado exitosamente\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 5. GUARDAR EN DELTA LAKE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 5: GUARDAR EN DELTA LAKE\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "DELTA_PATH = \"/app/notebooks/delta_lake/silver_contracts\"\n",
    "\n",
    "print(f\"üíæ Guardando en: {DELTA_PATH}\")\n",
    "\n",
    "df_silver.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .save(DELTA_PATH)\n",
    "\n",
    "print(\"‚úÖ Guardado exitosamente\\n\")\n",
    "\n",
    "# ‚ö†Ô∏è LIBERAR todo\n",
    "df_silver.unpersist()\n",
    "spark.catalog.clearCache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b60ee146-c2f4-4f23-ae53-f98ded14fb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.1\n"
     ]
    }
   ],
   "source": [
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "244fd81e-3b09-4646-99b4-d88f61d8abd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "VERIFICACI√ìN FINAL\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Registros verificados: 50,058\n",
      "+------------+----------------------------------------------------+--------------+-----------+\n",
      "|id_contrato |entidad                                             |valor_contrato|fecha_firma|\n",
      "+------------+----------------------------------------------------+--------------+-----------+\n",
      "|CPS-045-2024|empresa social del estado regi√≥n de salud soacha.   |7.8624E7      |2024-01-01 |\n",
      "|CPS 018-2024|E.S.E HOSPITAL NUESTRA SE√ëORA DEL CARMEN DEL COLEGIO|1.07844E7     |2024-01-01 |\n",
      "|CPS 012-2024|E.S.E HOSPITAL NUESTRA SE√ëORA DEL CARMEN DEL COLEGIO|1.07844E7     |2024-01-01 |\n",
      "|024-2024    |ESE HOSPITAL SALAZAR DE VILLETA                     |9363575.0     |2024-01-01 |\n",
      "|CPS-060-2024|empresa social del estado regi√≥n de salud soacha.   |8.3279308E7   |2024-01-01 |\n",
      "+------------+----------------------------------------------------+--------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "üéØ Fase 2 completada. Siguiente: Fase 3 - Embeddings\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 6. VERIFICACI√ìN\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"VERIFICACI√ìN FINAL\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "df_verify = spark.read.format(\"delta\").load(DELTA_PATH)\n",
    "print(f\"‚úÖ Registros verificados: {df_verify.count():,}\")\n",
    "\n",
    "df_verify.select(\"id_contrato\", \"entidad\", \"valor_contrato\", \"fecha_firma\") \\\n",
    "    .show(5, truncate=False)\n",
    "\n",
    "print(\"\\nüéØ Fase 2 completada. Siguiente: Fase 3 - Embeddings\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
