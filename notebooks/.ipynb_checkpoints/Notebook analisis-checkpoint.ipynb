{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a00643f-51d0-49ec-9aaa-0a6ac15581a1",
   "metadata": {},
   "source": [
    "## Fase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28ef8e94-16fd-4be8-bf98-8cd5a51ae9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-ae27b7c2-6798-4cc7-89f8-5ef358020b28;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.3 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "\tfound io.delta#delta-spark_2.12;3.0.0 in central\n",
      "\tfound io.delta#delta-storage;3.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 711ms :: artifacts dl 49ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tio.delta#delta-spark_2.12;3.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.0.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   14  |   0   |   0   |   0   ||   14  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-ae27b7c2-6798-4cc7-89f8-5ef358020b28\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 14 already retrieved (0kB/17ms)\n",
      "25/12/11 18:45:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Spark 3.5.1 iniciado\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FASE 2 - OPTIMIZADO PARA SPARK 3.5.1 + DELTA LAKE 3.0\n",
    "# ============================================================================\n",
    "\n",
    "# PASO 0: REINICIAR SPARK CON VERSIONES CORRECTAS\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "import time\n",
    "time.sleep(3)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, lower, regexp_replace, translate, length, trim\n",
    "\n",
    "from pyspark.ml.feature import (\n",
    "    Tokenizer, StopWordsRemover, Word2Vec, \n",
    "    StringIndexer, OneHotEncoder, VectorAssembler,\n",
    "    StandardScaler, PCA\n",
    ")\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.stat import Correlation\n",
    "import numpy as np\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Bronze_to_Silver_Optimized\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1,\"\n",
    "            \"io.delta:delta-spark_2.12:3.0.0\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config(\"spark.executor.memory\", \"4g\")\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"50\")\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "print(f\" Spark {spark.version} iniciado\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fd17a2e-563a-48c1-8f15-3c3ec7900ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PASO 1: LECTURA DE KAFKA\n",
      "================================================================================\n",
      "\n",
      "Leyendo Kafka...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 52:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mensajes: 100,698\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 1. LECTURA DE KAFKA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 1: LECTURA DE KAFKA\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "contract_schema = StructType([\n",
    "    StructField(\"id_contrato\", StringType()),\n",
    "    StructField(\"objeto_contrato\", StringType()),\n",
    "    StructField(\"entidad\", StringType()),\n",
    "    StructField(\"departamento\", StringType()),\n",
    "    StructField(\"municipio\", StringType()),\n",
    "    StructField(\"region\", StringType()),\n",
    "    StructField(\"codigo_unspsc\", StringType()),\n",
    "    StructField(\"descripcion_categoria\", StringType()),\n",
    "    StructField(\"valor_contrato\", DoubleType()),\n",
    "    StructField(\"duracion_dias\", IntegerType()),\n",
    "    StructField(\"fecha_firma\", StringType()),\n",
    "    StructField(\"tipo_contrato\", StringType()),\n",
    "    StructField(\"estado_contrato\", StringType()),\n",
    "    StructField(\"modalidad\", StringType()),\n",
    "    StructField(\"anno\", IntegerType()),\n",
    "    StructField(\"id_interno_sistema\", StringType()),\n",
    "    StructField(\"campo_vacio\", StringType()),\n",
    "    StructField(\"constante_1\", StringType()),\n",
    "    StructField(\"constante_2\", IntegerType()),\n",
    "    StructField(\"duplicate_id\", StringType()),\n",
    "    StructField(\"timestamp_carga\", StringType())\n",
    "])\n",
    "\n",
    "print(\"Leyendo Kafka...\")\n",
    "\n",
    "df_kafka = spark.read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", \"contratos-publicos\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "df_bronze = df_kafka.select(\n",
    "    from_json(col(\"value\").cast(\"string\"), contract_schema).alias(\"data\")\n",
    ").select(\"data.*\")\n",
    "\n",
    "df_bronze = df_bronze.cache()\n",
    "total_kafka = df_bronze.count()\n",
    "\n",
    "print(f\" Mensajes: {total_kafka:,}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f952457-ec33-4c40-b285-f2d1e68c3357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PASO 2: ELIMINAR REDUNDANTES Y PREPARAR DATOS\n",
      "================================================================================\n",
      "\n",
      " Eliminando 6 columnas redundantes...\n",
      "Columnas restantes: 15\n",
      "\n",
      " Preparando campo fecha_firma...\n",
      "   Formato recibido: ISO timestamp (2024-01-04T00:00:00.000)\n",
      "   Convirtiendo a: date (2024-01-04)\n",
      " Fecha convertida correctamente\n",
      "\n",
      " Liberando memoria de df_bronze...\n",
      " Memoria liberada\n",
      "\n",
      "================================================================================\n",
      " Dataset preparado: 15 columnas\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 2. ELIMINAR REDUNDANTES Y PREPARAR DATOS\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 2: ELIMINAR REDUNDANTES Y PREPARAR DATOS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Columnas redundantes a eliminar\n",
    "redundant_columns = [\n",
    "    \"id_interno_sistema\",\n",
    "    \"campo_vacio\",\n",
    "    \"constante_1\",\n",
    "    \"constante_2\",\n",
    "    \"duplicate_id\",\n",
    "    \"timestamp_carga\"\n",
    "]\n",
    "\n",
    "print(f\" Eliminando {len(redundant_columns)} columnas redundantes...\")\n",
    "df_cleaned = df_bronze.drop(*redundant_columns)\n",
    "\n",
    "print(f\"Columnas restantes: {len(df_cleaned.columns)}\")\n",
    "print()\n",
    "\n",
    "print(\" Preparando campo fecha_firma...\")\n",
    "print(\"   Formato recibido: ISO timestamp (2024-01-04T00:00:00.000)\")\n",
    "print(\"   Convirtiendo a: date (2024-01-04)\")\n",
    "\n",
    "df_cleaned = (\n",
    "    df_cleaned\n",
    "    .withColumn(\"fecha_firma_temp\", to_timestamp(col(\"fecha_firma\")))\n",
    "    .withColumn(\"fecha_firma\", to_date(col(\"fecha_firma_temp\")))\n",
    "    .drop(\"fecha_firma_temp\")\n",
    ")\n",
    "\n",
    "print(\" Fecha convertida correctamente\\n\")\n",
    "\n",
    "# Liberar bronze ahora que ya no lo necesitamos\n",
    "print(\" Liberando memoria de df_bronze...\")\n",
    "df_bronze.unpersist()\n",
    "print(\" Memoria liberada\\n\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\" Dataset preparado: {len(df_cleaned.columns)} columnas\")\n",
    "print(\"=\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "816b1e7b-d82b-439c-acbf-057230ec288d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PASO 3: LIMPIEZA - PREPARACI√ìN\n",
      "================================================================================\n",
      "\n",
      "Cacheando datos para an√°lisis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 56:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Registros totales: 100,698\n",
      "\n",
      " Columnas: 15\n",
      " Datos cacheados¬†en¬†memoria\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELDA 1: PREPARACI√ìN Y CONTEO INICIAL\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 3: LIMPIEZA - PREPARACI√ìN\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"Cacheando datos para an√°lisis...\")\n",
    "df_cleaned = df_cleaned.cache()\n",
    "total_cleaned = df_cleaned.count()\n",
    "\n",
    "print(f\" Registros totales: {total_cleaned:,}\\n\")\n",
    "print(f\" Columnas: {len(df_cleaned.columns)}\")\n",
    "print(f\" Datos cacheados¬†en¬†memoria\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82037c71-8bfc-4e99-b7fe-f3f24287fe73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "AN√ÅLISIS DE CALIDAD DE DATOS\n",
      "================================================================================\n",
      "\n",
      "Analizando valores nulos en columnas cr√≠ticas...\n",
      "üìä Valores nulos en columnas cr√≠ticas:\n",
      "\n",
      "   ‚ö†  fecha_firma: 695 (0.7%)\n",
      "   ‚ö†  duracion_dias: 50,350 (50.0%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELDA 2: AN√ÅLISIS DE NULOS (OPTIMIZADO)\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"AN√ÅLISIS DE CALIDAD DE DATOS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"Analizando valores nulos en columnas cr√≠ticas...\")\n",
    "\n",
    "# Solo analizar columnas cr√≠ticas para ahorrar memoria\n",
    "critical_columns = [\n",
    "    \"id_contrato\",\n",
    "    \"objeto_contrato\", \n",
    "    \"valor_contrato\",\n",
    "    \"fecha_firma\",\n",
    "    \"entidad\",\n",
    "    \"departamento\",\n",
    "    \"duracion_dias\"\n",
    "]\n",
    "\n",
    "# An√°lisis optimizado solo de columnas cr√≠ticas\n",
    "null_analysis = df_cleaned.select([\n",
    "    count(when(col(c).isNull(), c)).alias(c) \n",
    "    for c in critical_columns if c in df_cleaned.columns\n",
    "]).collect()[0].asDict()\n",
    "\n",
    "print(\"üìä Valores nulos en columnas cr√≠ticas:\\n\")\n",
    "has_nulls = False\n",
    "for col_name in critical_columns:\n",
    "    if col_name in null_analysis:\n",
    "        null_count = null_analysis[col_name]\n",
    "        if null_count > 0:\n",
    "            has_nulls = True\n",
    "            pct = (null_count / total_cleaned) * 100\n",
    "            print(f\"   ‚ö†  {col_name}: {null_count:,} ({pct:.1f}%)\")\n",
    "\n",
    "if not has_nulls:\n",
    "    print(\"   ‚úÖ No hay valores nulos en columnas cr√≠ticas\")\n",
    "\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af77614e-c5ff-4b86-91e2-23ce1d5a253d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "APLICANDO FILTROS DE CALIDAD\n",
      "================================================================================\n",
      "\n",
      "Aplicando reglas de limpieza:\n",
      "  ‚úì id_contrato no nulo\n",
      "  ‚úì objeto_contrato no nulo\n",
      "  ‚úì valor_contrato no nulo y > 0\n",
      "  ‚úì fecha_firma no nula\n",
      "\n",
      "‚úÖ Filtros aplicados correctamente\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELDA 3: APLICAR FILTROS DE LIMPIEZA\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"APLICANDO FILTROS DE CALIDAD\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"Aplicando reglas de limpieza:\")\n",
    "print(\"  ‚úì id_contrato no nulo\")\n",
    "print(\"  ‚úì objeto_contrato no nulo\")\n",
    "print(\"  ‚úì valor_contrato no nulo y > 0\")\n",
    "print(\"  ‚úì fecha_firma no nula\")\n",
    "print()\n",
    "\n",
    "# Aplicar filtros paso a paso\n",
    "# NOTA: fecha_firma ya fue convertida a date en el Paso 2\n",
    "df_silver = df_cleaned \\\n",
    "    .filter(col(\"id_contrato\").isNotNull()) \\\n",
    "    .filter(col(\"objeto_contrato\").isNotNull()) \\\n",
    "    .filter(col(\"valor_contrato\").isNotNull()) \\\n",
    "    .filter(col(\"valor_contrato\") > 0) \\\n",
    "    .filter(col(\"fecha_firma\").isNotNull())\n",
    "\n",
    "print(\"‚úÖ Filtros aplicados correctamente\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b60ee146-c2f4-4f23-ae53-f98ded14fb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.1\n"
     ]
    }
   ],
   "source": [
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "244fd81e-3b09-4646-99b4-d88f61d8abd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FINALIZANDO LIMPIEZA\n",
      "================================================================================\n",
      "\n",
      "Cacheando datos limpios...\n",
      "\n",
      "================================================================================\n",
      "üìä RESUMEN DE LIMPIEZA\n",
      "================================================================================\n",
      "  Registros iniciales:    100,698\n",
      "  Registros finales:      99,458 (98.8%)\n",
      "  Registros descartados:  1,240 (1.2%)\n",
      "================================================================================\n",
      "\n",
      "Liberando memoria del cache anterior...\n",
      "‚úÖ Limpieza¬†completada\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELDA 4: CACHEAR RESULTADOS Y GENERAR REPORTE\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"FINALIZANDO LIMPIEZA\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"Cacheando datos limpios...\")\n",
    "df_silver = df_silver.cache()\n",
    "total_silver = df_silver.count()\n",
    "\n",
    "# Calcular estad√≠sticas\n",
    "registros_descartados = total_cleaned - total_silver\n",
    "pct_retenido = (total_silver / total_cleaned) * 100 if total_cleaned > 0 else 0\n",
    "pct_descartado = (registros_descartados / total_cleaned) * 100 if total_cleaned > 0 else 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä RESUMEN DE LIMPIEZA\")\n",
    "print(\"=\"*80)\n",
    "print(f\"  Registros iniciales:    {total_cleaned:,}\")\n",
    "print(f\"  Registros finales:      {total_silver:,} ({pct_retenido:.1f}%)\")\n",
    "print(f\"  Registros descartados:  {registros_descartados:,} ({pct_descartado:.1f}%)\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Liberar memoria del DataFrame anterior\n",
    "print(\"Liberando memoria del cache anterior...\")\n",
    "df_cleaned.unpersist()\n",
    "print(\"‚úÖ Limpieza¬†completada\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b4f2fef-6d09-419f-baae-b612a0c8eef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 entidades:\n",
      "+-------------------------------------------------+-----+\n",
      "|entidad                                          |count|\n",
      "+-------------------------------------------------+-----+\n",
      "|MUNICIPIO DE SOACHA.                             |6356 |\n",
      "|ALCALD√çA MUNICIPAL COTA                          |3988 |\n",
      "|ESE MUNICIPAL DE SOACHA JULIO CESAR PE√ëALOZA*    |3822 |\n",
      "|CUNDINAMARCA-ALCALDIA MUNICIPIO MOSQUERA         |3759 |\n",
      "|empresa social del estado regi√≥n de salud soacha.|3152 |\n",
      "+-------------------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Top 5 departamentos:\n",
      "+------------+-----+\n",
      "|departamento|count|\n",
      "+------------+-----+\n",
      "|Cundinamarca|99458|\n",
      "+------------+-----+\n",
      "\n",
      "\n",
      "Distribuci√≥n por regi√≥n:\n",
      "+--------------+-----+\n",
      "|region        |count|\n",
      "+--------------+-----+\n",
      "|Centro-Oriente|99458|\n",
      "+--------------+-----+\n",
      "\n",
      "\n",
      "Top 10 c√≥digos UNSPSC:\n",
      "+-------------+-----+\n",
      "|codigo_unspsc|count|\n",
      "+-------------+-----+\n",
      "|             |50058|\n",
      "|V1.80111600  |11391|\n",
      "|V1.80111701  |4329 |\n",
      "|V1.85101600  |2605 |\n",
      "|V1.80111620  |2158 |\n",
      "|V1.80111601  |2092 |\n",
      "|V1.86101710  |1134 |\n",
      "|UNSPECIFIED  |1081 |\n",
      "|V1.80161500  |891  |\n",
      "|V1.80111504  |866  |\n",
      "+-------------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "Top 10 categor√≠as UNSPSC:\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|descripcion_categoria                                                                                                                                                                                                                                                                                       |count|\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|                                                                                                                                                                                                                                                                                                            |50058|\n",
      "|PRESTACI√ìN DE SERVICIOS DE APOYO A LA GESTI√ìN PARA REALIZAR ACTIVIDADES DE ATENCI√ìN PRIMARIA EN SALUD EN EL MUNICIPIO DE SOACHA EN EL MARCO DE LAS NORMAS VIGENTES; MEDIANTE LA IMPLEMENTACI√ìN DE EQUIPOS B√ÅSICOS DE SALUD EN EL CONTEXTO DE LAS ACCIONES DE PROMOCI√ìN MANTENIMIENTO DE LA SALUD Y LA ATENCI|318  |\n",
      "|PRESTACI√ìN DE SERVICIOS PROFESIONALES PARA REALIZAR ACTIVIDADES DE ATENCI√ìN PRIMARIA EN SALUD EN EL MUNICIPIO DE SOACHA EN EL MARCO DE LAS NORMAS VIGENTES; MEDIANTE LA IMPLEMENTACI√ìN DE EQUIPOS B√ÅSICOS DE SALUD EN EL CONTEXTO DE LAS ACCIONES DE PROMOCI√ìN MANTENIMIENTO DE LA SALUD Y LA ATENCI√ìN INDIV|248  |\n",
      "|Contratar la prestaci√≥n de los servicios profesionales de manera aut√≥noma e independiente para impartir formaci√≥n profesional integral en las competencias t√©cnicas; claves y transversales a trav√©s de herramientas pedag√≥gicas en el programa y modalidad asignada garantizando el cumplimiento del proces|173  |\n",
      "|PRESTACI√ìN DE SERVICIOS T√âCNICO ASISTENCIAL PARA REALIZAR ACTIVIDADES DE ATENCI√ìN PRIMARIA EN SALUD EN EL MUNICIPIO DE SOACHA EN EL MARCO DE LAS NORMAS VIGENTES; MEDIANTE LA IMPLEMENTACI√ìN DE EQUIPOS B√ÅSICOS EN SALUD EN EL CONTEXTO DE LAS ACCIONES DE PROMOCI√ìN Y MANTENIMIENTO DE LA SALUD Y LA ATENCI|120  |\n",
      "|Prestar servicios personales de car√°cter temporal para impartir formaci√≥n profesional integral; de forma presencial y/o virtual; as√≠ como otras actividades que se deriven de los diferentes programas; niveles y especialidades impartidas por el centro de tecnolog√≠as del transporte del Sena - Regional |92   |\n",
      "|Prestaci√≥n de servicios profesionales; tecn√≥logo y/o t√©cnico para el desarrollo de las actividades de formaci√≥n del nivel t√©cnico; tecnol√≥gico y/o complementar√≠a del programa Regular; atendiendo las pol√≠ticas institucionales y la normatividad vigente; de acuerdo con las necesidades; la programaci√≥n |87   |\n",
      "|AUNAR ESFUERZOS ENTRE EL DEPARTAMENTO DE CUNDINAMARCA - SECRETAR√çA DE EDUCACI√ìN Y LOS MUNICIPIOS FOCALIZADOS PARA LA ESTRATEGIA DE TRANSPORTE ESCOLAR; DIRIGIDO A ESTUDIANTES DE LAS IED OFICIALES; ASEGURANDO LAS CONDICIONES NECESARIAS PARA SU PERMANENCIA EN EL   SISTEMA EDUCATIVO (SEG√öN ANEXO DISTRIB|72   |\n",
      "|PRESTACI√ìN DE SERVICIOS COMO AUXILIAR DE ENFERMERIA EN EL PROCESO DE EQUIPOS BASICOS EN SALUD DE LA SUBGERENCIA COMUNITARIA DE LA EMPRESA SOCIAL DEL ESTADO REGION DE SALUD SOACHA DE ACUERDO AL REQUERIMIENTO INSTITUCIONAL EN DESARROLLO DE LA RESOLUCION 1032 DE 2024 EMITIDA POR EL MINISTERIO DE SALUD |72   |\n",
      "|PRESTACI√ìN DE SERVICIOS COMO AUXILIAR DE APOYO ADMINISTRATIVO A LA GESTI√ìN Y SEGUIMIENTO DE LAS ACTIVIDADES DE FACTURACI√ìN EN EL √ÅREA ADMINISTRATIVA DENTRO DE LOS DIFERENTES PROCESOS Y PROCEDIMIENTOS DE LA EMPRESA SOCIAL DEL ESTADO REGI√ìN DE SALUD SOACHA DE ACUERDO AL REQUERIMIENTO INSTITUCIONAL.   |66   |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "Distribuci√≥n por tipo de contrato:\n",
      "+------------------------------+-----+\n",
      "|tipo_contrato                 |count|\n",
      "+------------------------------+-----+\n",
      "|Prestaci√≥n de servicios       |81169|\n",
      "|Decreto 092 de 2017           |7814 |\n",
      "|Suministros                   |3413 |\n",
      "|Compraventa                   |2352 |\n",
      "|Otro                          |2319 |\n",
      "|Arrendamiento de inmuebles    |749  |\n",
      "|Obra                          |738  |\n",
      "|Seguros                       |475  |\n",
      "|Consultor√≠a                   |186  |\n",
      "|Interventor√≠a                 |104  |\n",
      "|Arrendamiento de muebles      |56   |\n",
      "|Operaciones de Cr√©dito P√∫blico|23   |\n",
      "|No Especificado               |20   |\n",
      "|Servicios financieros         |19   |\n",
      "|Comodato                      |9    |\n",
      "|Venta muebles                 |6    |\n",
      "|Asociaci√≥n P√∫blico Privada    |4    |\n",
      "|Concesi√≥n                     |2    |\n",
      "+------------------------------+-----+\n",
      "\n",
      "\n",
      "Distribuci√≥n del estado del contrato:\n",
      "+---------------+-----+\n",
      "|estado_contrato|count|\n",
      "+---------------+-----+\n",
      "|En ejecuci√≥n   |37005|\n",
      "|Cerrado        |27760|\n",
      "|Modificado     |21922|\n",
      "|terminado      |11707|\n",
      "|Aprobado       |688  |\n",
      "|cedido         |278  |\n",
      "|Suspendido     |98   |\n",
      "+---------------+-----+\n",
      "\n",
      "\n",
      "Top 10 modalidades de contrataci√≥n:\n",
      "+-------------------------------------------+-----+\n",
      "|modalidad                                  |count|\n",
      "+-------------------------------------------+-----+\n",
      "|Contrataci√≥n directa                       |69957|\n",
      "|Contrataci√≥n r√©gimen especial              |17569|\n",
      "|M√≠nima cuant√≠a                             |6772 |\n",
      "|Selecci√≥n Abreviada de Menor Cuant√≠a       |1607 |\n",
      "|Selecci√≥n abreviada subasta inversa        |964  |\n",
      "|Contrataci√≥n r√©gimen especial (con ofertas)|832  |\n",
      "|Contrataci√≥n Directa (con ofertas)         |745  |\n",
      "|Licitaci√≥n p√∫blica                         |724  |\n",
      "|Concurso de m√©ritos abierto                |186  |\n",
      "|Licitaci√≥n p√∫blica Obra Publica            |58   |\n",
      "+-------------------------------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Entidades\n",
    "print(\"Top 5 entidades:\")\n",
    "df_silver.groupBy(\"entidad\").count().orderBy(desc(\"count\")).show(5, truncate=False)\n",
    "\n",
    "# 2. Departamentos\n",
    "print(\"\\nTop 5 departamentos:\")\n",
    "df_silver.groupBy(\"departamento\").count().orderBy(desc(\"count\")).show(5, truncate=False)\n",
    "\n",
    "# 3. Regi√≥n\n",
    "print(\"\\nDistribuci√≥n por regi√≥n:\")\n",
    "df_silver.groupBy(\"region\").count().orderBy(desc(\"count\")).show(truncate=False)\n",
    "\n",
    "# 4. C√≥digo UNSPSC\n",
    "print(\"\\nTop 10 c√≥digos UNSPSC:\")\n",
    "df_silver.groupBy(\"codigo_unspsc\").count().orderBy(desc(\"count\")).show(10, truncate=False)\n",
    "\n",
    "# 5. Categor√≠a UNSPSC\n",
    "print(\"\\nTop 10 categor√≠as UNSPSC:\")\n",
    "df_silver.groupBy(\"descripcion_categoria\").count().orderBy(desc(\"count\")).show(10, truncate=False)\n",
    "\n",
    "# 6. Tipo de contrato\n",
    "print(\"\\nDistribuci√≥n por tipo de contrato:\")\n",
    "df_silver.groupBy(\"tipo_contrato\").count().orderBy(desc(\"count\")).show(truncate=False)\n",
    "\n",
    "# 7. Estado del contrato\n",
    "print(\"\\nDistribuci√≥n del estado del contrato:\")\n",
    "df_silver.groupBy(\"estado_contrato\").count().orderBy(desc(\"count\")).show(truncate=False)\n",
    "\n",
    "# 8. Modalidad de contrataci√≥n\n",
    "print(\"\\nTop 10 modalidades de contrataci√≥n:\")\n",
    "df_silver.groupBy(\"modalidad\").count().orderBy(desc(\"count\")).show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0131185-211a-4265-9356-7b567e112143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Estad√≠sticas de valor_contrato:\n",
      "+---+----------------+-------------------+--------------------+\n",
      "|min|             max|               mean|                 std|\n",
      "+---+----------------+-------------------+--------------------+\n",
      "|1.0|1.50838540149E11|9.941466321590018E7|1.1521186504414532E9|\n",
      "+---+----------------+-------------------+--------------------+\n",
      "\n",
      "\n",
      "Percentiles de valor_contrato:\n",
      "\n",
      "Estad√≠sticas de duracion_dias:\n",
      "+---+----+-----------------+------------------+\n",
      "|min| max|             mean|               std|\n",
      "+---+----+-----------------+------------------+\n",
      "|  0|4297|82.47422012591348|101.20091534465666|\n",
      "+---+----+-----------------+------------------+\n",
      "\n",
      "\n",
      "Percentiles de duracion_dias:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0, 6.0, 40.0, 125.0, 4297.0]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max, avg, stddev, expr\n",
    "\n",
    "# 10. Valor del contrato\n",
    "print(\"\\nEstad√≠sticas de valor_contrato:\")\n",
    "df_silver.select(\n",
    "    min(\"valor_contrato\").alias(\"min\"),\n",
    "    max(\"valor_contrato\").alias(\"max\"),\n",
    "    avg(\"valor_contrato\").alias(\"mean\"),\n",
    "    stddev(\"valor_contrato\").alias(\"std\")\n",
    ").show()\n",
    "\n",
    "# Percentiles\n",
    "print(\"\\nPercentiles de valor_contrato:\")\n",
    "df_silver.approxQuantile(\"valor_contrato\", [0.01, 0.25, 0.5, 0.75, 0.99], 0.01)\n",
    "\n",
    "# 11. Duraci√≥n en d√≠as\n",
    "print(\"\\nEstad√≠sticas de duracion_dias:\")\n",
    "df_silver.select(\n",
    "    min(\"duracion_dias\").alias(\"min\"),\n",
    "    max(\"duracion_dias\").alias(\"max\"),\n",
    "    avg(\"duracion_dias\").alias(\"mean\"),\n",
    "    stddev(\"duracion_dias\").alias(\"std\")\n",
    ").show()\n",
    "\n",
    "print(\"\\nPercentiles de duracion_dias:\")\n",
    "df_silver.approxQuantile(\"duracion_dias\", [0.01, 0.25, 0.5, 0.75, 0.99], 0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be7ceb3f-a49f-454d-9040-eadd4a513895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top a√±os:\n",
      "+----+-----+\n",
      "|anno|count|\n",
      "+----+-----+\n",
      "|2025|68   |\n",
      "|2024|95797|\n",
      "|2023|3029 |\n",
      "|2022|564  |\n",
      "+----+-----+\n",
      "\n",
      "\n",
      "Contratos por a√±o:\n",
      "+----+-----+\n",
      "|anno|count|\n",
      "+----+-----+\n",
      "|2024|95797|\n",
      "|2023|3029 |\n",
      "|2022|564  |\n",
      "|2025|68   |\n",
      "+----+-----+\n",
      "\n",
      "\n",
      "Top fechas de firma:\n",
      "+-----------+-----+\n",
      "|fecha_firma|count|\n",
      "+-----------+-----+\n",
      "|2024-02-01 |1230 |\n",
      "|2024-03-01 |1119 |\n",
      "|2024-02-02 |860  |\n",
      "|2024-02-05 |815  |\n",
      "|2024-03-22 |794  |\n",
      "|2024-02-09 |789  |\n",
      "|2024-02-16 |744  |\n",
      "|2024-02-06 |723  |\n",
      "|2024-02-12 |715  |\n",
      "|2024-09-02 |686  |\n",
      "+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTop a√±os:\")\n",
    "df_silver.groupBy(\"anno\").count().orderBy(desc(\"anno\")).show(10, truncate=False)\n",
    "\n",
    "print(\"\\nContratos por a√±o:\")\n",
    "df_silver.groupBy(\"anno\").count().orderBy(desc(\"count\")).show(10, truncate=False)\n",
    "\n",
    "print(\"\\nTop fechas de firma:\")\n",
    "df_silver.groupBy(\"fecha_firma\").count().orderBy(desc(\"count\")).show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9095b5d1-039d-4c44-81ad-731451dc1ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PASO 5: GUARDAR EN DELTA LAKE\n",
      "================================================================================\n",
      "\n",
      "üíæ Guardando en: /app/notebooks/delta_lake/silver_contracts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Guardado exitosamente\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 5. GUARDAR EN DELTA LAKE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 5: GUARDAR EN DELTA LAKE\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "DELTA_PATH = \"/app/notebooks/delta_lake/silver_contracts\"\n",
    "\n",
    "print(f\"üíæ Guardando en: {DELTA_PATH}\")\n",
    "\n",
    "df_silver.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .save(DELTA_PATH)\n",
    "\n",
    "print(\"‚úÖ Guardado exitosamente\\n\")\n",
    "\n",
    "# ‚ö†Ô∏è LIBERAR todo\n",
    "df_silver.unpersist()\n",
    "spark.catalog.clearCache()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fa9b8d-dcf7-4962-8755-3993314ee402",
   "metadata": {},
   "source": [
    "## Fase 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f353f8e1-7564-40cc-ae3f-bc39a496b980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PASO 1: CARGAR DATOS DESDE SILVER\n",
      "================================================================================\n",
      "\n",
      "üìä Cargando: /app/notebooks/delta_lake/silver_contracts\n",
      "‚úì Registros: 99,706\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PASO 1: CARGAR DATOS DESDE SILVER\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "SILVER_PATH = \"/app/notebooks/delta_lake/silver_contracts\"\n",
    "print(f\"üìä Cargando: {SILVER_PATH}\")\n",
    "\n",
    "df_silver = spark.read.format(\"delta\").load(SILVER_PATH)\n",
    "df_silver = df_silver.cache()\n",
    "total_records = df_silver.count()\n",
    "\n",
    "print(f\"‚úì Registros: {total_records:,}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18f418b5-05f7-4432-a3e9-ca4bc34e5141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PASO 2: LIMPIEZA DE TEXTO\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:===========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Registros despu√©s de limpieza: 99,706\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PASO 2: LIMPIEZA DE TEXTO\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 2: LIMPIEZA DE TEXTO\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "src_chars = \"√°√©√≠√≥√∫√º√±\"\n",
    "dst_chars = \"aeiouun\"\n",
    "\n",
    "df_prepared = df_silver.withColumn(\n",
    "    \"objeto_limpio\",\n",
    "    trim(\n",
    "        regexp_replace(\n",
    "            regexp_replace(\n",
    "                translate(\n",
    "                    lower(col(\"objeto_contrato\")),\n",
    "                    src_chars,\n",
    "                    dst_chars\n",
    "                ),\n",
    "                \"[^a-z0-9\\\\s]\", \" \"\n",
    "            ),\n",
    "            \"\\\\s+\", \" \"\n",
    "        )\n",
    "    )\n",
    ").filter(length(col(\"objeto_limpio\")) >= 10)\n",
    "\n",
    "print(f\"‚úì Registros despu√©s de limpieza: {df_prepared.count():,}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb8b89d1-bb12-449c-b504-a243d1b8e33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PASO 3: TOKENIZACI√ìN\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:===========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Registros despu√©s de filtrado: 99,706\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PASO 3: TOKENIZACI√ìN Y STOPWORDS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 3: TOKENIZACI√ìN\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "stopwords_es = [\n",
    "    \"el\", \"la\", \"de\", \"que\", \"y\", \"a\", \"en\", \"un\", \"ser\", \"se\", \"no\",\n",
    "    \"por\", \"con\", \"su\", \"para\", \"como\", \"estar\", \"tener\", \"le\", \"lo\",\n",
    "    \"pero\", \"hacer\", \"o\", \"este\", \"otro\", \"ese\", \"si\", \"ya\", \"ver\",\n",
    "    \"dar\", \"muy\", \"sin\", \"sobre\", \"tambi√©n\", \"hasta\", \"a√±o\", \"entre\",\n",
    "    \"del\", \"al\", \"los\", \"las\", \"uno\", \"una\", \"unos\", \"unas\",\n",
    "    \"contrato\", \"contratos\", \"objeto\", \"prestacion\", \"prestaci√≥n\",\n",
    "    \"servicio\", \"servicios\", \"suministro\", \"ejecucion\", \"ejecuci√≥n\"\n",
    "]\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"objeto_limpio\", outputCol=\"palabras\")\n",
    "df_tokenized = tokenizer.transform(df_prepared)\n",
    "\n",
    "remover = StopWordsRemover(\n",
    "    inputCol=\"palabras\",\n",
    "    outputCol=\"palabras_sin_stopwords\",\n",
    "    stopWords=stopwords_es\n",
    ")\n",
    "df_filtered_words = remover.transform(df_tokenized)\n",
    "\n",
    "# Filtrar palabras cortas\n",
    "def clean_words(words):\n",
    "    if not words:\n",
    "        return []\n",
    "    return [w for w in words if len(w) >= 3]\n",
    "\n",
    "clean_udf = udf(clean_words, ArrayType(StringType()))\n",
    "\n",
    "df_filtered = df_filtered_words.withColumn(\n",
    "    \"palabras_filtradas\",\n",
    "    clean_udf(col(\"palabras_sin_stopwords\"))\n",
    ").filter(size(col(\"palabras_filtradas\")) > 0)\n",
    "\n",
    "print(f\"‚úì Registros despu√©s de filtrado: {df_filtered.count():,}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bc2043b-4721-4da7-a242-b0e4bf879ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PASO 4: WORD2VEC\n",
      "================================================================================\n",
      "\n",
      "‚è≥ Entrenando Word2Vec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Vocabulario: 14,468 palabras\n",
      "‚úì Embeddings¬†generados\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PASO 4: WORD2VEC\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 4: WORD2VEC\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "word2vec = Word2Vec(\n",
    "    vectorSize=100,\n",
    "    minCount=2,\n",
    "    maxIter=10,\n",
    "    seed=42,\n",
    "    inputCol=\"palabras_filtradas\",\n",
    "    outputCol=\"embedding_raw\"\n",
    ")\n",
    "\n",
    "print(\"‚è≥ Entrenando Word2Vec...\")\n",
    "word2vec_model = word2vec.fit(df_filtered)\n",
    "df_embeddings = word2vec_model.transform(df_filtered)\n",
    "\n",
    "vocab_size = len(word2vec_model.getVectors().collect())\n",
    "print(f\"‚úì Vocabulario: {vocab_size:,} palabras\")\n",
    "print(f\"‚úì Embeddings¬†generados\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbde6876-943f-4de2-8206-dd5afc874ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Transformaciones categ√≥ricas (sin target)...\n",
      "   OneHot para tipo_contrato...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   OneHot para estado_contrato...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   OneHot para modalidad...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   Frequency Encoding para 'entidad'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# 3. Transformaciones INDEPENDIENTES del target\n",
    "# ----------------------------------------------------------------\n",
    "print(\"\\n2. Transformaciones categ√≥ricas (sin target)...\")\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# A. OneHot para variables de baja cardinalidad\n",
    "low_card_cols = [\"tipo_contrato\", \"estado_contrato\", \"modalidad\"]\n",
    "\n",
    "for col_name in low_card_cols:\n",
    "    print(f\"   OneHot para {col_name}...\")\n",
    "    \n",
    "    indexer = StringIndexer(\n",
    "        inputCol=col_name,\n",
    "        outputCol=f\"{col_name}_idx\",\n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "    df_embeddings = indexer.fit(df_embeddings).transform(df_embeddings)\n",
    "    \n",
    "    encoder = OneHotEncoder(\n",
    "        inputCol=f\"{col_name}_idx\",\n",
    "        outputCol=f\"{col_name}_ohe\",\n",
    "        dropLast=True\n",
    "    )\n",
    "    df_embeddings = encoder.fit(df_embeddings).transform(df_embeddings)\n",
    "\n",
    "# B. Frequency Encoding para 'entidad'\n",
    "print(\"\\n   Frequency Encoding para 'entidad'...\")\n",
    "entidad_freq = df_embeddings.groupBy(\"entidad\").count()\n",
    "total_count = df_embeddings.count()\n",
    "entidad_freq = entidad_freq.withColumn(\n",
    "    \"entidad_freq\",\n",
    "    col(\"count\") / total_count\n",
    ").select(\"entidad\", \"entidad_freq\")\n",
    "\n",
    "df_embeddings = df_embeddings.join(entidad_freq, \"entidad\", \"left\")\n",
    "\n",
    "# C. Eliminar variables sin varianza\n",
    "df_embeddings = df_embeddings.drop(\"departamento\", \"region\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd6d8a14-d303-454a-ae88-91f3c95a00b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Divisi√≥n temporal train/test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Fecha de corte: 2024-09-30 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Train: 79,800 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 128:==========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Test:  19,906 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# 4. DIVISI√ìN TEMPORAL (80/20)\n",
    "# ----------------------------------------------------------------\n",
    "print(\"\\n3. Divisi√≥n temporal train/test...\")\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# Convertir fecha para calcular percentil\n",
    "df_temp = df_embeddings.withColumn(\n",
    "    \"fecha_num\",\n",
    "    col(\"fecha_firma\").cast(\"timestamp\").cast(\"long\")\n",
    ")\n",
    "\n",
    "# Calcular percentil 80\n",
    "q = df_temp.approxQuantile(\"fecha_num\", [0.8], 0.01)\n",
    "split_ts = q[0]\n",
    "split_date = datetime.utcfromtimestamp(split_ts)\n",
    "\n",
    "print(f\"   Fecha de corte: {split_date}\")\n",
    "\n",
    "# Crear datasets de train y test\n",
    "df_train_raw = df_embeddings.filter(col(\"fecha_firma\") <= split_date)\n",
    "df_test_raw = df_embeddings.filter(col(\"fecha_firma\") > split_date)\n",
    "\n",
    "print(f\"   Train: {df_train_raw.count():,} registros\")\n",
    "print(f\"   Test:  {df_test_raw.count():,} registros\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f280937-d4cd-4c08-8970-e427d5537aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. Guardando datasets en Delta Lake...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úì Train guardado en: /app/notebooks/delta_lake/train_raw_v3\n",
      "   ‚úì Test guardado en:  /app/notebooks/delta_lake/test_raw_v3\n",
      "\n",
      "5. Guardando modelos de transformaci√≥n...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelos guardados\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# 5. Guardar datasets en Delta Lake\n",
    "# ----------------------------------------------------------------\n",
    "print(\"\\n4. Guardando datasets en Delta Lake...\")\n",
    "\n",
    "# Ruta para los datasets preprocesados\n",
    "TRAIN_RAW_PATH = \"/app/notebooks/delta_lake/train_raw_v3\"\n",
    "TEST_RAW_PATH = \"/app/notebooks/delta_lake/test_raw_v3\"\n",
    "\n",
    "# Guardar train\n",
    "df_train_raw.write.format(\"delta\").mode(\"overwrite\").save(TRAIN_RAW_PATH)\n",
    "\n",
    "# Guardar test\n",
    "df_test_raw.write.format(\"delta\").mode(\"overwrite\").save(TEST_RAW_PATH)\n",
    "\n",
    "print(f\"   ‚úì Train guardado en: {TRAIN_RAW_PATH}\")\n",
    "print(f\"   ‚úì Test guardado en:  {TEST_RAW_PATH}\")\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 6. Guardar tambi√©n los modelos de transformaci√≥n\n",
    "# ----------------------------------------------------------------\n",
    "print(\"\\n5. Guardando modelos de transformaci√≥n...\")\n",
    "\n",
    "MODELS_PATH = \"/app/notebooks/models_v3\"\n",
    "\n",
    "# Guardar Word2Vec model (si lo tienes)\n",
    "if 'word2vec_model' in locals():\n",
    "    word2vec_model.save(f\"{MODELS_PATH}/word2vec_model\")\n",
    "\n",
    "# Guardar StringIndexer models para referencia\n",
    "for col_name in low_card_cols:\n",
    "    indexer_model_path = f\"{MODELS_PATH}/indexer_{col_name}\"\n",
    "    # Necesitar√≠as extraer el modelo del pipeline o guardar los mapeos\n",
    "\n",
    "print(\"Modelos guardados\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bcc194-9db1-4aac-9f0e-06077ca5a1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# 7. Informe final\n",
    "# ----------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESUMEN FASE 3\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"‚úÖ Preprocesamiento completado\")\n",
    "print(f\"üìä Train: {df_train_raw.count():,} registros\")\n",
    "print(f\"üìä Test:  {df_test_raw.count():,} registros\")\n",
    "print()\n",
    "print(\"üéØ Variables disponibles:\")\n",
    "print(f\"  - embedding_raw: Word2Vec embeddings\")\n",
    "for col in low_card_cols:\n",
    "    print(f\"  - {col}_ohe: OneHot encoded\")\n",
    "print(f\"  - entidad_freq: Frequency encoding\")\n",
    "print(f\"  - valor_contrato: Target variable\")\n",
    "print(f\"  - duracion_dias: Variable num√©rica\")\n",
    "print()\n",
    "print(\"üìà Listo para Fase 4: Target Encoding y Modelado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81211544-f4a9-481c-87a2-b1b3a4b98958",
   "metadata": {},
   "source": [
    "## Fase 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07f621bb-6131-43ed-8375-261de8517517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import builtins  # <-- IMPORTANTE: Importar builtins\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor, GBTRegressor, DecisionTreeRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d89ed5d-9cda-4234-9cef-3322c5bfe2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FASE 4: MODELADO CON TRANSFORMACI√ìN LOGAR√çTMICA\n",
      "================================================================================\n",
      "1. Cargando datasets preprocesados...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úì Train: 79,800 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:==============================================>           (4 + 1) / 5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úì Test:  19,906 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# FASE 4: MODELADO Y REGISTRO EN MLFLOW (CON LOG TRANSFORM)\n",
    "# ================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FASE 4: MODELADO CON TRANSFORMACI√ìN LOGAR√çTMICA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 1. Cargar datasets preprocesados\n",
    "# ----------------------------------------------------------------\n",
    "print(\"1. Cargando datasets preprocesados...\")\n",
    "\n",
    "TRAIN_RAW_PATH = \"/app/notebooks/delta_lake/train_raw_v3\"\n",
    "TEST_RAW_PATH = \"/app/notebooks/delta_lake/test_raw_v3\"\n",
    "\n",
    "train_raw = spark.read.format(\"delta\").load(TRAIN_RAW_PATH).cache()\n",
    "test_raw = spark.read.format(\"delta\").load(TEST_RAW_PATH).cache()\n",
    "\n",
    "print(f\"   ‚úì Train: {train_raw.count():,} registros\")\n",
    "print(f\"   ‚úì Test:  {test_raw.count():,} registros\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb6d7eca-ebcc-4e37-8dfe-bae7e5850657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Aplicando transformaci√≥n logar√≠tmica al target...\n",
      "   Estad√≠sticas del target original vs logar√≠tmico:\n",
      "   Original: mean=$81,720,270.17, std=$983,090,380.87\n",
      "   Log: mean=16.73, std=1.12\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# 2. TRANSFORMACI√ìN LOGAR√çTMICA DEL TARGET\n",
    "# ----------------------------------------------------------------\n",
    "import pyspark.sql.functions as F\n",
    "print(\"\\n2. Aplicando transformaci√≥n logar√≠tmica al target...\")\n",
    "\n",
    "# Usamos log1p = log(1 + x) para evitar problemas con valores peque√±os\n",
    "train_data = train_raw.withColumn(\"log_valor_contrato\", F.log1p(col(\"valor_contrato\")))\n",
    "test_data = test_raw.withColumn(\"log_valor_contrato\", F.log1p(col(\"valor_contrato\")))\n",
    "\n",
    "# Verificar estad√≠sticas antes/despu√©s\n",
    "print(\"   Estad√≠sticas del target original vs logar√≠tmico:\")\n",
    "train_stats = train_data.select(\n",
    "    F.mean(\"valor_contrato\").alias(\"mean_original\"),\n",
    "    F.stddev(\"valor_contrato\").alias(\"std_original\"),\n",
    "    F.mean(\"log_valor_contrato\").alias(\"mean_log\"),\n",
    "    F.stddev(\"log_valor_contrato\").alias(\"std_log\")\n",
    ").first()\n",
    "\n",
    "print(f\"   Original: mean=${train_stats['mean_original']:,.2f}, std=${train_stats['std_original']:,.2f}\")\n",
    "print(f\"   Log: mean={train_stats['mean_log']:.2f}, std={train_stats['std_log']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86e24322-a3fd-4171-b2ae-7c069a62e034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Target Encoding para 'codigo_unspsc' (usando target log)...\n",
      "   ‚úì codigo_unspsc_te_log creado (en escala log)\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# 3. Target Encoding usando el TARGET LOGAR√çTMICO\n",
    "# ----------------------------------------------------------------\n",
    "print(\"\\n3. Target Encoding para 'codigo_unspsc' (usando target log)...\")\n",
    "\n",
    "def safe_target_encoding_log(train_df, test_df, cat_col, target_log_col=\"log_valor_contrato\", m=50):\n",
    "    \"\"\"\n",
    "    Target encoding usando el target en escala logar√≠tmica\n",
    "    \"\"\"\n",
    "    # Calcular media global del LOG en train\n",
    "    global_mean_log = train_df.agg(F.mean(target_log_col)).first()[0]\n",
    "    \n",
    "    # Calcular estad√≠sticas por categor√≠a en train (usando LOG)\n",
    "    stats = train_df.groupBy(cat_col).agg(\n",
    "        F.mean(target_log_col).alias(\"cat_mean_log\"),\n",
    "        F.count(target_log_col).alias(\"cat_count\")\n",
    "    )\n",
    "    \n",
    "    # Aplicar smoothing en escala logar√≠tmica\n",
    "    stats = stats.withColumn(\n",
    "        f\"{cat_col}_te_log\",\n",
    "        (F.col(\"cat_count\") * F.col(\"cat_mean_log\") + m * global_mean_log) / \n",
    "        (F.col(\"cat_count\") + m)\n",
    "    ).select(cat_col, f\"{cat_col}_te_log\")\n",
    "    \n",
    "    # Aplicar a train\n",
    "    train_encoded = train_df.join(stats, cat_col, \"left\")\n",
    "    \n",
    "    # Aplicar a test\n",
    "    test_encoded = test_df.join(stats, cat_col, \"left\")\n",
    "    \n",
    "    # Para categor√≠as no vistas en train, usar global mean log\n",
    "    test_encoded = test_encoded.fillna({f\"{cat_col}_te_log\": global_mean_log})\n",
    "    \n",
    "    return train_encoded, test_encoded\n",
    "\n",
    "# Aplicar target encoding con target logar√≠tmico\n",
    "train_data, test_data = safe_target_encoding_log(\n",
    "    train_data, test_data,\n",
    "    cat_col=\"codigo_unspsc\",\n",
    "    target_log_col=\"log_valor_contrato\",\n",
    "    m=50\n",
    ")\n",
    "\n",
    "print(\"   ‚úì codigo_unspsc_te_log creado (en escala log)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01c10bf4-32e6-4fed-ae19-23f56f58c816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. Ensamblando features...\n",
      "   Features: 7 dimensiones\n",
      "   - embedding_raw\n",
      "   - tipo_contrato_ohe\n",
      "   - estado_contrato_ohe\n",
      "   - modalidad_ohe\n",
      "   - entidad_freq\n",
      "   - codigo_unspsc_te_log\n",
      "   - duracion_dias\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# 4. Ensamblar features (incluyendo el target encoding logar√≠tmico)\n",
    "# ----------------------------------------------------------------\n",
    "print(\"\\n4. Ensamblando features...\")\n",
    "\n",
    "feature_cols = [\n",
    "    \"embedding_raw\",\n",
    "    \"tipo_contrato_ohe\",\n",
    "    \"estado_contrato_ohe\", \n",
    "    \"modalidad_ohe\",\n",
    "    \"entidad_freq\",\n",
    "    \"codigo_unspsc_te_log\",  # ¬°Usamos la versi√≥n logar√≠tmica!\n",
    "]\n",
    "\n",
    "if \"duracion_dias\" in train_data.columns:\n",
    "    train_data = train_data.fillna({\"duracion_dias\": 0})\n",
    "    test_data = test_data.fillna({\"duracion_dias\": 0})\n",
    "    feature_cols.append(\"duracion_dias\")\n",
    "\n",
    "print(f\"   Features: {len(feature_cols)} dimensiones\")\n",
    "for feat in feature_cols:\n",
    "    print(f\"   - {feat}\")\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features_raw\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "\n",
    "train_features = assembler.transform(train_data)\n",
    "test_features = assembler.transform(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "216023ac-4227-46c8-8777-09c2df8ff6f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. Normalizando y validando...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# 5. Normalizaci√≥n (CON VALIDACI√ìN)\n",
    "# ----------------------------------------------------------------\n",
    "print(\"\\n5. Normalizando y validando...\")\n",
    "\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features_raw\",\n",
    "    outputCol=\"features_scaled\",\n",
    "    withStd=True,\n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "scaler_model = scaler.fit(train_features)\n",
    "train_scaled = scaler_model.transform(train_features)\n",
    "test_scaled = scaler_model.transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e20c37f5-b0a1-48b8-a453-465ce975f4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6. Aplicando PCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Componentes para 95% varianza: 1\n",
      "   Reducci√≥n: 141 ‚Üí 1 dimensiones\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# 6. PCA (CORREGIDO)\n",
    "# ----------------------------------------------------------------\n",
    "import builtins\n",
    "print(\"\\n6. Aplicando PCA...\")\n",
    "\n",
    "initial_dims = len(train_scaled.select(\"features_scaled\").first()[0])\n",
    "\n",
    "# CORRECCI√ìN: Usar builtins.min en lugar de min\n",
    "pca = PCA(\n",
    "    k=builtins.min(50, initial_dims),  # <-- ¬°CORRECCI√ìN AQU√ç!\n",
    "    inputCol=\"features_scaled\", \n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "pca_model = pca.fit(train_scaled)\n",
    "train_pca = pca_model.transform(train_scaled)\n",
    "test_pca = pca_model.transform(test_scaled)\n",
    "\n",
    "# Analizar varianza\n",
    "explained_variance = pca_model.explainedVariance.toArray()\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "n_95 = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "\n",
    "print(f\"   Componentes para 95% varianza: {n_95}\")\n",
    "print(f\"   Reducci√≥n: {initial_dims} ‚Üí {n_95} dimensiones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c3a1d2e-b655-4f0b-ac0a-ca192edbf533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç DIAGN√ìSTICO DEL PCA Y FEATURES (VERSI√ìN CORREGIDA):\n",
      "================================================================================\n",
      "1. Verificando tipo de datos de features_scaled...\n",
      "   Tipo: VectorUDT()\n",
      "   Es vector: False\n",
      "\n",
      "2. Verificando dimensiones...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Dimensiones iniciales: 141\n",
      "\n",
      "3. Calculando estad√≠sticas b√°sicas...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Total registros: 79800\n",
      "   Tomando muestra de: 1000 registros\n",
      "   Muestra analizada: 1000 registros\n",
      "   Media (primeras 5 features): [ 0.26052261 -0.16667773 -0.12854223 -0.01215937 -0.11279548]\n",
      "   Varianza (primeras 5 features): [0.84983428 1.04320726 1.40261202 1.04923529 0.85028533]\n",
      "   Features con varianza < 1e-6: 10/141\n",
      "\n",
      "4. Verificando valores problem√°ticos...\n",
      "   Valores NaN en muestra: 0\n",
      "   Valores Inf en muestra: 0\n",
      "\n",
      "5. Verificando calidad del escalado...\n",
      "   Media absoluta de medias: 0.129635 (deber√≠a ser ~0)\n",
      "   Promedio de varianzas: 1.720219 (deber√≠a ser ~1)\n",
      "   ‚ö†Ô∏è  El escalado podr√≠a no estar centrando bien (media lejos de 0)\n",
      "   ‚ö†Ô∏è  El escalado podr√≠a no estar normalizando bien (varianza promedio=1.72)\n",
      "\n",
      "6. Diagn√≥stico del PCA (CORRECTO)...\n",
      "   Ejecutando PCA con todos los componentes para an√°lisis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   Varianza explicada por componentes principales:\n",
      "   PC1: 0.0676 (6.76%)\n",
      "   PC2: 0.0486 (4.86%)\n",
      "   PC3: 0.0405 (4.05%)\n",
      "   PC4: 0.0383 (3.83%)\n",
      "   PC5: 0.0278 (2.78%)\n",
      "   PC6: 0.0250 (2.50%)\n",
      "   PC7: 0.0211 (2.11%)\n",
      "   PC8: 0.0205 (2.05%)\n",
      "   PC9: 0.0191 (1.91%)\n",
      "   PC10: 0.0173 (1.73%)\n",
      "\n",
      "   Componentes necesarios para diferentes umbrales:\n",
      "   50% varianza: 24 componentes\n",
      "   80% varianza: 64 componentes\n",
      "   90% varianza: 87 componentes\n",
      "   95% varianza: 104 componentes\n",
      "   99% varianza: 126 componentes\n",
      "\n",
      "   Componentes para 95% varianza: 104\n",
      "\n",
      "7. An√°lisis de causa ra√≠z...\n",
      "\n",
      "================================================================================\n",
      "SOLUCI√ìN RECOMENDADA:\n",
      "================================================================================\n",
      "\n",
      "‚úÖ PCA PARECE RAZONABLE:\n",
      "\n",
      "   Se necesitan 104 componentes para 95% varianza.\n",
      "   Esto sugiere una distribuci√≥n saludable de informaci√≥n.\n",
      "\n",
      "üîß ACCI√ìN RECOMENDADA:\n",
      "\n",
      "   Probar con 104 componentes PCA y tambi√©n sin PCA para comparar.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# DIAGN√ìSTICO CORREGIDO - VERSI√ìN SIN CONFLICTOS\n",
    "# ================================================================\n",
    "\n",
    "import builtins  # Para usar min() y abs() de Python sin conflictos\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\nüîç DIAGN√ìSTICO DEL PCA Y FEATURES (VERSI√ìN CORREGIDA):\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 1. Verificar el tipo de dato de features_scaled\n",
    "# ----------------------------------------------------------------\n",
    "print(\"1. Verificando tipo de datos de features_scaled...\")\n",
    "\n",
    "# Obtener schema\n",
    "feature_schema = train_scaled.schema[\"features_scaled\"].dataType\n",
    "print(f\"   Tipo: {feature_schema}\")\n",
    "print(f\"   Es vector: {feature_schema.typeName() == 'vector'}\")\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 2. Verificar dimensiones correctamente\n",
    "# ----------------------------------------------------------------\n",
    "print(\"\\n2. Verificando dimensiones...\")\n",
    "initial_dims = len(train_scaled.select(\"features_scaled\").first()[0])\n",
    "print(f\"   Dimensiones iniciales: {initial_dims}\")\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 3. Calcular estad√≠sticas MANUALMENTE (sin Summarizer problem√°tico)\n",
    "# ----------------------------------------------------------------\n",
    "print(\"\\n3. Calculando estad√≠sticas b√°sicas...\")\n",
    "\n",
    "# Tomar una muestra para an√°lisis - USANDO builtins.min()\n",
    "total_count = train_scaled.count()\n",
    "sample_size = builtins.min(1000, total_count)  # Usar builtins.min() expl√≠citamente\n",
    "print(f\"   Total registros: {total_count}\")\n",
    "print(f\"   Tomando muestra de: {sample_size} registros\")\n",
    "\n",
    "sample_df = train_scaled.select(\"features_scaled\").limit(sample_size).collect()\n",
    "\n",
    "# Convertir a numpy array\n",
    "sample_vectors = np.array([row[\"features_scaled\"].toArray() for row in sample_df])\n",
    "\n",
    "# Calcular estad√≠sticas en la muestra\n",
    "mean_per_feature = np.mean(sample_vectors, axis=0)\n",
    "variance_per_feature = np.var(sample_vectors, axis=0)\n",
    "\n",
    "print(f\"   Muestra analizada: {len(sample_vectors)} registros\")\n",
    "print(f\"   Media (primeras 5 features): {mean_per_feature[:5]}\")\n",
    "print(f\"   Varianza (primeras 5 features): {variance_per_feature[:5]}\")\n",
    "\n",
    "# Contar features con varianza cercana a cero\n",
    "low_variance_count = np.sum(variance_per_feature < 1e-6)\n",
    "print(f\"   Features con varianza < 1e-6: {low_variance_count}/{initial_dims}\")\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 4. Verificar si hay valores NaN o Inf\n",
    "# ----------------------------------------------------------------\n",
    "print(\"\\n4. Verificando valores problem√°ticos...\")\n",
    "\n",
    "nan_count = np.sum(np.isnan(sample_vectors))\n",
    "inf_count = np.sum(np.isinf(sample_vectors))\n",
    "\n",
    "print(f\"   Valores NaN en muestra: {nan_count}\")\n",
    "print(f\"   Valores Inf en muestra: {inf_count}\")\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 5. Verificar escalado - deber√≠a tener media ~0 y varianza ~1\n",
    "# ----------------------------------------------------------------\n",
    "print(\"\\n5. Verificando calidad del escalado...\")\n",
    "\n",
    "mean_abs = np.abs(mean_per_feature).mean()\n",
    "variance_mean = variance_per_feature.mean()\n",
    "\n",
    "print(f\"   Media absoluta de medias: {mean_abs:.6f} (deber√≠a ser ~0)\")\n",
    "print(f\"   Promedio de varianzas: {variance_mean:.6f} (deber√≠a ser ~1)\")\n",
    "\n",
    "if mean_abs > 0.1:\n",
    "    print(\"   ‚ö†Ô∏è  El escalado podr√≠a no estar centrando bien (media lejos de 0)\")\n",
    "if builtins.abs(variance_mean - 1.0) > 0.1:  # Usar builtins.abs()\n",
    "    print(f\"   ‚ö†Ô∏è  El escalado podr√≠a no estar normalizando bien (varianza promedio={variance_mean:.2f})\")\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 6. DIAGN√ìSTICO DEL PCA - LA PARTE IMPORTANTE\n",
    "# ----------------------------------------------------------------\n",
    "print(\"\\n6. Diagn√≥stico del PCA (CORRECTO)...\")\n",
    "\n",
    "# Ejecutar PCA con todos los componentes primero para ver varianza explicada\n",
    "print(\"   Ejecutando PCA con todos los componentes para an√°lisis...\")\n",
    "\n",
    "pca_full = PCA(k=initial_dims, inputCol=\"features_scaled\", outputCol=\"features_pca_full\")\n",
    "pca_model_full = pca_full.fit(train_scaled)\n",
    "explained_variance = pca_model_full.explainedVariance.toArray()\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "print(f\"\\n   Varianza explicada por componentes principales:\")\n",
    "for i in range(builtins.min(10, len(explained_variance))):  # Usar builtins.min()\n",
    "    print(f\"   PC{i+1}: {explained_variance[i]:.4f} ({explained_variance[i]:.2%})\")\n",
    "\n",
    "# Encontrar n√∫mero de componentes para diferentes umbrales\n",
    "thresholds = [0.50, 0.80, 0.90, 0.95, 0.99]\n",
    "print(f\"\\n   Componentes necesarios para diferentes umbrales:\")\n",
    "for threshold in thresholds:\n",
    "    n_components = np.argmax(cumulative_variance >= threshold) + 1\n",
    "    print(f\"   {threshold:.0%} varianza: {n_components} componentes\")\n",
    "\n",
    "n_95 = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "print(f\"\\n   Componentes para 95% varianza: {n_95}\")\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 7. AN√ÅLISIS DE POR QU√â PCA DA SOLO 1 COMPONENTE\n",
    "# ----------------------------------------------------------------\n",
    "print(\"\\n7. An√°lisis de causa ra√≠z...\")\n",
    "\n",
    "if n_95 == 1:\n",
    "    print(\"   üîç PROBLEMA DETECTADO: Solo 1 componente para 95% varianza\")\n",
    "    print(\"\\n   Posibles causas:\")\n",
    "    \n",
    "    # Causa 1: Primera componente domina\n",
    "    first_pc_variance = explained_variance[0] if len(explained_variance) > 0 else 0\n",
    "    print(f\"   1. Primera componente muy dominante: {first_pc_variance:.2%}\")\n",
    "    \n",
    "    # Causa 2: Features con varianza muy desigual\n",
    "    variance_max = variance_per_feature.max()\n",
    "    variance_min = variance_per_feature.min()\n",
    "    variance_ratio = variance_max / variance_min if variance_min > 0 else float('inf')\n",
    "    print(f\"   2. Ratio m√°ximo/min de varianza: {variance_ratio:.2f}\")\n",
    "    \n",
    "    # Causa 3: Alta correlaci√≥n entre features\n",
    "    # Calcular correlaciones en muestra\n",
    "    if sample_vectors.shape[1] > 1:\n",
    "        corr_matrix = np.corrcoef(sample_vectors, rowvar=False)\n",
    "        np.fill_diagonal(corr_matrix, 0)  # Quitar diagonal\n",
    "        max_corr = np.max(np.abs(corr_matrix))\n",
    "        high_corr_count = np.sum(np.abs(corr_matrix) > 0.9)\n",
    "        total_corr_pairs = (corr_matrix.shape[0] * (corr_matrix.shape[0] - 1)) / 2\n",
    "        print(f\"   3. Correlaci√≥n m√°xima entre features: {max_corr:.3f}\")\n",
    "        print(f\"      Pares con |corr| > 0.9: {high_corr_count}/{int(total_corr_pairs)}\")\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 8. SOLUCI√ìN RECOMENDADA\n",
    "# ----------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SOLUCI√ìN RECOMENDADA:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if n_95 == 1:\n",
    "    print(\"\"\"\n",
    "‚ö†Ô∏è  PROBLEMA CR√çTICO DETECTADO:\n",
    "   \n",
    "   PCA est√° capturando el 95% de la varianza con solo 1 componente.\n",
    "   Esto significa que:\n",
    "   1. Hay una feature o combinaci√≥n que domina completamente\n",
    "   2. Las dem√°s features aportan muy poca informaci√≥n\n",
    "   3. Posible problema con el escalado o preparaci√≥n de features\n",
    "\n",
    "üîß ACCIONES RECOMENDADAS:\n",
    "\n",
    "   1. VERIFICAR EL TARGET ENCODING:\n",
    "      - codigo_unspsc_te_log podr√≠a estar dominando\n",
    "      - Verificar su varianza relativa a otras features\n",
    "      \n",
    "   2. REVISAR WORD2VEC EMBEDDINGS:\n",
    "      - embedding_raw (100 dimensiones) podr√≠a tener varianza muy baja\n",
    "      - Word2Vec puede generar embeddings con poca variabilidad\n",
    "      \n",
    "   3. PROBAR SIN PCA INICIALMENTE:\n",
    "      - Usar todas las features escaladas directamente\n",
    "      - Los √°rboles (RandomForest, GBT) manejan bien multicolinealidad\n",
    "      \n",
    "   4. VERIFICAR ONE-HOT ENCODINGS:\n",
    "      - tipo_contrato_ohe, estado_contrato_ohe, modalidad_ohe\n",
    "      - Podr√≠an tener muchas dimensiones con poca variabilidad\n",
    "\"\"\")\n",
    "else:\n",
    "    print(f\"\"\"\n",
    "‚úÖ PCA PARECE RAZONABLE:\n",
    "   \n",
    "   Se necesitan {n_95} componentes para 95% varianza.\n",
    "   Esto sugiere una distribuci√≥n saludable de informaci√≥n.\n",
    "   \n",
    "üîß ACCI√ìN RECOMENDADA:\n",
    "   \n",
    "   Probar con {n_95} componentes PCA y tambi√©n sin PCA para comparar.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df5b0f10-7080-4f3f-a666-c1f97faeb902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6. Omitiendo PCA - usando features escaladas directamente\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Dimensiones finales: 141\n",
      "   Train: 79,800 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Test:  19,906 registros\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# 6. OMITIR PCA (usar features escaladas directamente)\n",
    "# ----------------------------------------------------------------\n",
    "print(\"\\n6. Omitiendo PCA - usando features escaladas directamente\")\n",
    "\n",
    "# Usar las features escaladas directamente\n",
    "train_final = train_scaled.select(\n",
    "    col(\"log_valor_contrato\").alias(\"label_log\"),\n",
    "    col(\"features_scaled\").alias(\"features\"),\n",
    "    col(\"id_contrato\"),\n",
    "    col(\"fecha_firma\"),\n",
    "    col(\"valor_contrato\")\n",
    ").cache()\n",
    "\n",
    "test_final = test_scaled.select(\n",
    "    col(\"log_valor_contrato\").alias(\"label_log\"),\n",
    "    col(\"features_scaled\").alias(\"features\"),\n",
    "    col(\"id_contrato\"),\n",
    "    col(\"fecha_firma\"),\n",
    "    col(\"valor_contrato\")\n",
    ").cache()\n",
    "\n",
    "dimensiones = len(train_final.select(\"features\").first()[0])\n",
    "print(f\"   Dimensiones finales: {dimensiones}\")\n",
    "print(f\"   Train: {train_final.count():,} registros\")\n",
    "print(f\"   Test:  {test_final.count():,} registros\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2c1562-f998-4af8-8c16-7c1872d556fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7. Probando m√∫ltiples modelos (sin PCA)...\n",
      "\n",
      "   üîç Entrenando LinearRegression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚úì R¬≤ (log): 0.3787\n",
      "      ‚úì R¬≤ (original): 0.0181\n",
      "      ‚úì RMSE: $1,654,832,456.16\n",
      "      ‚úì MAE: $151,444,702.22\n",
      "\n",
      "   üîç Entrenando RandomForest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 100:==============================>                       (28 + 12) / 50]"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# 7. PROBAR M√öLTIPLES MODELOS (SIN PCA)\n",
    "# ----------------------------------------------------------------\n",
    "print(\"\\n7. Probando m√∫ltiples modelos (sin PCA)...\")\n",
    "\n",
    "models_config = [\n",
    "    {\n",
    "        \"name\": \"LinearRegression\",\n",
    "        \"model\": LinearRegression(\n",
    "            featuresCol=\"features\",\n",
    "            labelCol=\"label_log\",\n",
    "            maxIter=100,\n",
    "            regParam=0.1,\n",
    "            elasticNetParam=0.8\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"RandomForest\",\n",
    "        \"model\": RandomForestRegressor(\n",
    "            featuresCol=\"features\",\n",
    "            labelCol=\"label_log\",\n",
    "            numTrees=50,\n",
    "            maxDepth=10,\n",
    "            seed=42\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"GBT\",\n",
    "        \"model\": GBTRegressor(\n",
    "            featuresCol=\"features\",\n",
    "            labelCol=\"label_log\",\n",
    "            maxIter=50,\n",
    "            maxDepth=5,\n",
    "            seed=42\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"DecisionTree\",\n",
    "        \"model\": DecisionTreeRegressor(\n",
    "            featuresCol=\"features\",\n",
    "            labelCol=\"label_log\",\n",
    "            maxDepth=10,\n",
    "            seed=42\n",
    "        )\n",
    "    }\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for config in models_config:\n",
    "    print(f\"\\n   üîç Entrenando {config['name']}...\")\n",
    "    \n",
    "    try:\n",
    "        model = config[\"model\"]\n",
    "        \n",
    "        # Entrenar\n",
    "        trained_model = model.fit(train_final)\n",
    "        \n",
    "        # Predecir\n",
    "        predictions_log = trained_model.transform(test_final)\n",
    "        \n",
    "        # Convertir a escala original\n",
    "        predictions_final = predictions_log.withColumn(\n",
    "            \"prediction_original\", \n",
    "            F.expm1(col(\"prediction\"))\n",
    "        ).withColumnRenamed(\"prediction\", \"prediction_log\")\n",
    "        \n",
    "        # Calcular m√©tricas\n",
    "        evaluator_log = RegressionEvaluator(\n",
    "            labelCol=\"label_log\",\n",
    "            predictionCol=\"prediction_log\",\n",
    "            metricName=\"r2\"\n",
    "        )\n",
    "        \n",
    "        evaluator_original = RegressionEvaluator(\n",
    "            labelCol=\"valor_contrato\",\n",
    "            predictionCol=\"prediction_original\",\n",
    "            metricName=\"r2\"\n",
    "        )\n",
    "        \n",
    "        evaluator_rmse = RegressionEvaluator(\n",
    "            labelCol=\"valor_contrato\",\n",
    "            predictionCol=\"prediction_original\",\n",
    "            metricName=\"rmse\"\n",
    "        )\n",
    "        \n",
    "        evaluator_mae = RegressionEvaluator(\n",
    "            labelCol=\"valor_contrato\",\n",
    "            predictionCol=\"prediction_original\",\n",
    "            metricName=\"mae\"\n",
    "        )\n",
    "        \n",
    "        r2_log = evaluator_log.evaluate(predictions_final)\n",
    "        r2_original = evaluator_original.evaluate(predictions_final)\n",
    "        rmse_original = evaluator_rmse.evaluate(predictions_final)\n",
    "        mae_original = evaluator_mae.evaluate(predictions_final)\n",
    "        \n",
    "        # Guardar resultados\n",
    "        results.append({\n",
    "            \"model\": config[\"name\"],\n",
    "            \"r2_log\": r2_log,\n",
    "            \"r2_original\": r2_original,\n",
    "            \"rmse_original\": rmse_original,\n",
    "            \"mae_original\": mae_original,\n",
    "            \"trained_model\": trained_model,\n",
    "            \"predictions\": predictions_final\n",
    "        })\n",
    "        \n",
    "        print(f\"      ‚úì R¬≤ (log): {r2_log:.4f}\")\n",
    "        print(f\"      ‚úì R¬≤ (original): {r2_original:.4f}\")\n",
    "        print(f\"      ‚úì RMSE: ${rmse_original:,.2f}\")\n",
    "        print(f\"      ‚úì MAE: ${mae_original:,.2f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"      ‚úó Error: {str(e)[:100]}...\")\n",
    "        results.append({\n",
    "            \"model\": config[\"name\"],\n",
    "            \"error\": str(e)\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1215ffd-de37-4046-82c5-84aab30c315b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# 8. COMPARAR RESULTADOS\n",
    "# ----------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARACI√ìN DE MODELOS (SIN PCA)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Mostrar resultados en tabla\n",
    "print(f\"\\n{'Modelo':<20} {'R¬≤ (log)':<12} {'R¬≤ (original)':<15} {'RMSE':<20} {'MAE':<20}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for result in results:\n",
    "    if \"error\" not in result:\n",
    "        rmse_str = f\"${result['rmse_original']:,.2f}\"\n",
    "        mae_str = f\"${result['mae_original']:,.2f}\"\n",
    "        print(f\"{result['model']:<20} {result['r2_log']:<12.4f} {result['r2_original']:<15.4f} {rmse_str:<20} {mae_str:<20}\")\n",
    "    else:\n",
    "        print(f\"{result['model']:<20} {'ERROR':<12} {'ERROR':<15} {'ERROR':<20} {'ERROR':<20}\")\n",
    "\n",
    "# Encontrar el mejor modelo por R¬≤ original\n",
    "valid_results = [r for r in results if \"error\" not in r]\n",
    "if valid_results:\n",
    "    best_model = max(valid_results, key=lambda x: x[\"r2_original\"])\n",
    "    \n",
    "    print(f\"\\nüèÜ MEJOR MODELO: {best_model['model']}\")\n",
    "    print(f\"   R¬≤ (original): {best_model['r2_original']:.4f}\")\n",
    "    print(f\"   RMSE: ${best_model['rmse_original']:,.2f}\")\n",
    "    print(f\"   MAE: ${best_model['mae_original']:,.2f}\")\n",
    "    \n",
    "    # Guardar el mejor modelo para MLflow\n",
    "    best_trained_model = best_model[\"trained_model\"]\n",
    "    best_predictions = best_model[\"predictions\"]\n",
    "    \n",
    "    # Comparar con resultado anterior CON PCA\n",
    "    print(f\"\\nüìà COMPARACI√ìN CON PCA:\")\n",
    "    print(f\"   CON PCA (anterior): R¬≤ = -0.0043\")\n",
    "    print(f\"   SIN PCA (ahora):    R¬≤ = {best_model['r2_original']:.4f}\")\n",
    "    \n",
    "    if best_model['r2_original'] > -0.0043:\n",
    "        print(\"   ‚úÖ ¬°MEJOR√çA SIGNIFICATIVA!\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  Resultado similar o peor\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No hay modelos v√°lidos para comparar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9e4c3fb-f2b1-4929-998c-bc89a69626ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7. Preparando datos para modelado (target logar√≠tmico)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Train final: 79,800 registros\n",
      "   Test final:  19,906 registros\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# 7. Preparar datos para modelado (usando LOG como target)\n",
    "# ----------------------------------------------------------------\n",
    "print(\"\\n7. Preparando datos para modelado (target logar√≠tmico)...\")\n",
    "\n",
    "# Entrenaremos el modelo para predecir el LOG del valor\n",
    "train_final = train_pca.select(\n",
    "    col(\"log_valor_contrato\").alias(\"label_log\"),  # Target logar√≠tmico\n",
    "    col(\"features\"),\n",
    "    col(\"id_contrato\"),\n",
    "    col(\"fecha_firma\"),\n",
    "    col(\"valor_contrato\")  # Mantenemos el original tambi√©n\n",
    ")\n",
    "\n",
    "test_final = test_pca.select(\n",
    "    col(\"log_valor_contrato\").alias(\"label_log\"),  # Target logar√≠tmico\n",
    "    col(\"features\"),\n",
    "    col(\"id_contrato\"),\n",
    "    col(\"fecha_firma\"),\n",
    "    col(\"valor_contrato\")  # Mantenemos el original tambi√©n\n",
    ")\n",
    "\n",
    "print(f\"   Train final: {train_final.count():,} registros\")\n",
    "print(f\"   Test final:  {test_final.count():,} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a535b42-bb3b-4f33-876b-9f718629393b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "8. Entrenando modelo para predecir log(valor_contrato)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# 8. Entrenar modelo para predecir LOG\n",
    "# ----------------------------------------------------------------\n",
    "print(\"\\n8. Entrenando modelo para predecir log(valor_contrato)...\")\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Modelo para predecir el log\n",
    "lr = LinearRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label_log\",  # Target logar√≠tmico\n",
    "    maxIter=100,\n",
    "    regParam=0.1,\n",
    "    elasticNetParam=0.8\n",
    ")\n",
    "\n",
    "lr_model = lr.fit(train_final)\n",
    "predictions_log = lr_model.transform(test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eda9f634-a4cc-4c2e-9c6f-827fda39f59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "9. Convirtiendo predicciones a escala original...\n",
      "   ‚úì Predicciones convertidas a escala original\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# 9. Convertir predicciones logar√≠tmicas a escala original\n",
    "# ----------------------------------------------------------------\n",
    "print(\"\\n9. Convirtiendo predicciones a escala original...\")\n",
    "\n",
    "# Convertir de vuelta: exp(predicci√≥n_log) - 1\n",
    "predictions_final = predictions_log.withColumn(\n",
    "    \"prediction_original\", \n",
    "    F.expm1(col(\"prediction\"))  # exp(x) - 1\n",
    ")\n",
    "\n",
    "# Para referencia, tambi√©n mantenemos la predicci√≥n logar√≠tmica\n",
    "predictions_final = predictions_final.withColumnRenamed(\"prediction\", \"prediction_log\")\n",
    "\n",
    "print(\"   ‚úì Predicciones convertidas a escala original\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "144c52c1-0f7c-467d-a3c2-7a1201eecbba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10. Calculando m√©tricas...\n",
      "\n",
      "üìä M√âTRICAS EN ESCALA LOGAR√çTMICA:\n",
      "   RMSE (log): 1.4145\n",
      "   R¬≤ (log):   0.2997\n",
      "\n",
      "üìä M√âTRICAS EN ESCALA ORIGINAL:\n",
      "   RMSE: $1,673,553,489.93\n",
      "   MAE:  $162,062,709.30\n",
      "   R¬≤:   -0.0043\n",
      "   Sigma: $1,667,071,830.58\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# 10. CALCULAR M√âTRICAS EN AMBAS ESCALAS\n",
    "# ----------------------------------------------------------------\n",
    "print(\"\\n10. Calculando m√©tricas...\")\n",
    "\n",
    "# M√©tricas en escala logar√≠tmica\n",
    "evaluator_log_rmse = RegressionEvaluator(\n",
    "    labelCol=\"label_log\", \n",
    "    predictionCol=\"prediction_log\", \n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "evaluator_log_r2 = RegressionEvaluator(\n",
    "    labelCol=\"label_log\", \n",
    "    predictionCol=\"prediction_log\", \n",
    "    metricName=\"r2\"\n",
    ")\n",
    "\n",
    "rmse_log = evaluator_log_rmse.evaluate(predictions_final)\n",
    "r2_log = evaluator_log_r2.evaluate(predictions_final)\n",
    "\n",
    "# M√©tricas en escala original\n",
    "evaluator_original_rmse = RegressionEvaluator(\n",
    "    labelCol=\"valor_contrato\", \n",
    "    predictionCol=\"prediction_original\", \n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "evaluator_original_r2 = RegressionEvaluator(\n",
    "    labelCol=\"valor_contrato\", \n",
    "    predictionCol=\"prediction_original\", \n",
    "    metricName=\"r2\"\n",
    ")\n",
    "evaluator_original_mae = RegressionEvaluator(\n",
    "    labelCol=\"valor_contrato\", \n",
    "    predictionCol=\"prediction_original\", \n",
    "    metricName=\"mae\"\n",
    ")\n",
    "\n",
    "rmse_original = evaluator_original_rmse.evaluate(predictions_final)\n",
    "r2_original = evaluator_original_r2.evaluate(predictions_final)\n",
    "mae_original = evaluator_original_mae.evaluate(predictions_final)\n",
    "\n",
    "# Calcular sigma en escala original\n",
    "predictions_with_error = predictions_final.withColumn(\n",
    "    \"error_original\", \n",
    "    col(\"prediction_original\") - col(\"valor_contrato\")\n",
    ")\n",
    "sigma_original = predictions_with_error.select(\n",
    "    stddev(\"error_original\").alias(\"sigma\")\n",
    ").first()[\"sigma\"]\n",
    "\n",
    "print(f\"\\nüìä M√âTRICAS EN ESCALA LOGAR√çTMICA:\")\n",
    "print(f\"   RMSE (log): {rmse_log:.4f}\")\n",
    "print(f\"   R¬≤ (log):   {r2_log:.4f}\")\n",
    "\n",
    "print(f\"\\nüìä M√âTRICAS EN ESCALA ORIGINAL:\")\n",
    "print(f\"   RMSE: ${rmse_original:,.2f}\")\n",
    "print(f\"   MAE:  ${mae_original:,.2f}\")\n",
    "print(f\"   R¬≤:   {r2_original:.4f}\")\n",
    "print(f\"   Sigma: ${sigma_original:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dc727e-587c-4e34-af8d-4f323d62229d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# 11. REGISTRAR EN MLFLOW (TODAS LAS M√âTRICAS)\n",
    "# ----------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"REGISTRO EN MLFLOW\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Configurar MLflow\n",
    "MLFLOW_TRACKING_URI = \"http://172.17.0.1:5000\"\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "mlflow.set_experiment(\"contract_value_regression_log\")\n",
    "\n",
    "# Iniciar run de MLflow\n",
    "with mlflow.start_run(run_name=f\"linear_regression_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"):\n",
    "    \n",
    "    # ========== 11.1 Registrar par√°metros ==========\n",
    "    print(\"üìù Registrando par√°metros...\")\n",
    "    \n",
    "    mlflow.log_param(\"model_type\", \"LinearRegression\")\n",
    "    mlflow.log_param(\"target_transform\", \"log1p\")\n",
    "    mlflow.log_param(\"regParam\", 0.1)\n",
    "    mlflow.log_param(\"elasticNetParam\", 0.8)\n",
    "    mlflow.log_param(\"maxIter\", 100)\n",
    "    mlflow.log_param(\"train_size\", train_final.count())\n",
    "    mlflow.log_param(\"test_size\", test_final.count())\n",
    "    mlflow.log_param(\"features_count\", len(feature_cols))\n",
    "    mlflow.log_param(\"pca_components\", pca.getK())\n",
    "    mlflow.log_param(\"target_encoding_smoothing\", 50)\n",
    "    mlflow.log_param(\"target_encoding_scale\", \"logarithmic\")\n",
    "    \n",
    "    print(\"   ‚úì Par√°metros registrados\")\n",
    "    \n",
    "    # ========== 11.2 Registrar m√©tricas ==========\n",
    "    print(\"üìà Registrando m√©tricas...\")\n",
    "    \n",
    "    # M√©tricas en escala logar√≠tmica\n",
    "    mlflow.log_metric(\"test_rmse_log\", rmse_log)\n",
    "    mlflow.log_metric(\"test_r2_log\", r2_log)\n",
    "    \n",
    "    # M√©tricas en escala original\n",
    "    mlflow.log_metric(\"test_rmse_original\", rmse_original)\n",
    "    mlflow.log_metric(\"test_mae_original\", mae_original)\n",
    "    mlflow.log_metric(\"test_r2_original\", r2_original)\n",
    "    mlflow.log_metric(\"sigma_original\", sigma_original)\n",
    "    mlflow.log_metric(\"anomaly_threshold_2.8sigma\", 2.8 * sigma_original)\n",
    "    \n",
    "    # M√©tricas adicionales\n",
    "    mean_valor = train_final.agg(F.mean(\"valor_contrato\")).first()[0]\n",
    "    mlflow.log_metric(\"rmse_to_mean_ratio\", rmse_original / mean_valor)\n",
    "    mlflow.log_metric(\"mean_valor_contrato\", mean_valor)\n",
    "    \n",
    "    print(\"   ‚úì M√©tricas registradas\")\n",
    "    \n",
    "    # ========== 11.3 Registrar modelo ==========\n",
    "    print(\"üíæ Registrando modelo...\")\n",
    "    \n",
    "    # Registrar el modelo que predice LOG\n",
    "    mlflow.spark.log_model(\n",
    "        spark_model=lr_model,\n",
    "        artifact_path=\"model_log\",\n",
    "        registered_model_name=\"contract_value_predictor_log_v3\"\n",
    "    )\n",
    "    \n",
    "    # Tambi√©n guardar funci√≥n de conversi√≥n\n",
    "    def convert_to_original_udf():\n",
    "        \"\"\"Funci√≥n para convertir predicciones logar√≠tmicas a originales\"\"\"\n",
    "        return F.udf(lambda x: float(np.expm1(x)) if x is not None else None)\n",
    "    \n",
    "    print(\"   ‚úì Modelo registrado\")\n",
    "    \n",
    "    # ========== 11.4 Registrar artifacts ==========\n",
    "    print(\"üìé Registrando artifacts...\")\n",
    "    \n",
    "    # Guardar m√©tricas en JSON\n",
    "    metrics_dict = {\n",
    "        \"model_type\": \"LinearRegression\",\n",
    "        \"target_transform\": \"log1p\",\n",
    "        \"test_rmse_log\": float(rmse_log),\n",
    "        \"test_r2_log\": float(r2_log),\n",
    "        \"test_rmse_original\": float(rmse_original),\n",
    "        \"test_r2_original\": float(r2_original),\n",
    "        \"test_mae_original\": float(mae_original),\n",
    "        \"sigma_original\": float(sigma_original),\n",
    "        \"train_size\": int(train_final.count()),\n",
    "        \"test_size\": int(test_final.count()),\n",
    "        \"features\": feature_cols,\n",
    "        \"run_timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    metrics_path = \"/tmp/model_metrics_log.json\"\n",
    "    with open(metrics_path, 'w') as f:\n",
    "        json.dump(metrics_dict, f, indent=2)\n",
    "    \n",
    "    mlflow.log_artifact(metrics_path, \"metrics\")\n",
    "    \n",
    "    # Guardar sample de predicciones (originales y log)\n",
    "    predictions_sample = predictions_final.select(\n",
    "        \"id_contrato\", \n",
    "        \"valor_contrato\", \n",
    "        \"prediction_log\",\n",
    "        \"prediction_original\",\n",
    "        \"fecha_firma\"\n",
    "    ).limit(100).toPandas()\n",
    "    \n",
    "    predictions_path = \"/tmp/predictions_sample_log.csv\"\n",
    "    predictions_sample.to_csv(predictions_path, index=False)\n",
    "    mlflow.log_artifact(predictions_path, \"predictions\")\n",
    "    \n",
    "    print(\"   ‚úì Artifacts registrados\")\n",
    "    \n",
    "    # ========== 11.5 Registrar tags ==========\n",
    "    print(\"üè∑Ô∏è  Registrando tags...\")\n",
    "    \n",
    "    mlflow.set_tag(\"framework\", \"PySpark\")\n",
    "    mlflow.set_tag(\"spark_version\", spark.version)\n",
    "    mlflow.set_tag(\"model_version\", \"v3.0_log\")\n",
    "    mlflow.set_tag(\"data_source\", \"contratos_publicos\")\n",
    "    mlflow.set_tag(\"target_variable\", \"valor_contrato\")\n",
    "    mlflow.set_tag(\"target_transform\", \"log1p\")\n",
    "    mlflow.set_tag(\"encoding_strategy\", \"hybrid_log\")\n",
    "    \n",
    "    print(\"   ‚úì Tags registrados\")\n",
    "    \n",
    "    # ========== 11.6 Informaci√≥n del run ==========\n",
    "    run_id = mlflow.active_run().info.run_id\n",
    "    experiment_id = mlflow.active_run().info.experiment_id\n",
    "    \n",
    "    print(f\"\\n‚úÖ RUN COMPLETADO:\")\n",
    "    print(f\"   Run ID: {run_id}\")\n",
    "    print(f\"   Experiment ID: {experiment_id}\")\n",
    "    print(f\"   MLflow UI: {MLFLOW_TRACKING_URI}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
