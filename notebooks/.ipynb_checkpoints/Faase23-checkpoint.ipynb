{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a00643f-51d0-49ec-9aaa-0a6ac15581a1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Fase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28ef8e94-16fd-4be8-bf98-8cd5a51ae9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-9a1c4cee-7c26-4670-af12-676610dc2648;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.3 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "\tfound io.delta#delta-spark_2.12;3.0.0 in central\n",
      "\tfound io.delta#delta-storage;3.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 364ms :: artifacts dl 11ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tio.delta#delta-spark_2.12;3.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.0.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   14  |   0   |   0   |   0   ||   14  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-9a1c4cee-7c26-4670-af12-676610dc2648\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 14 already retrieved (0kB/9ms)\n",
      "25/12/10 00:49:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Spark 3.5.1 iniciado\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FASE 2 - OPTIMIZADO PARA SPARK 3.5.1 + DELTA LAKE 3.0\n",
    "# ============================================================================\n",
    "\n",
    "# PASO 0: REINICIAR SPARK CON VERSIONES CORRECTAS\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "import time\n",
    "time.sleep(3)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, lower, regexp_replace, translate, length, trim\n",
    "\n",
    "from pyspark.ml.feature import (\n",
    "    Tokenizer, StopWordsRemover, Word2Vec, \n",
    "    StringIndexer, OneHotEncoder, VectorAssembler,\n",
    "    StandardScaler, PCA\n",
    ")\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.stat import Correlation\n",
    "import numpy as np\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Bronze_to_Silver_Optimized\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1,\"\n",
    "            \"io.delta:delta-spark_2.12:3.0.0\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config(\"spark.executor.memory\", \"4g\")\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"50\")\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "print(f\" Spark {spark.version} iniciado\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fd17a2e-563a-48c1-8f15-3c3ec7900ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PASO 1: LECTURA DE KAFKA\n",
      "================================================================================\n",
      "\n",
      "Leyendo Kafka...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mensajes: 50,349\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 1. LECTURA DE KAFKA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 1: LECTURA DE KAFKA\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "contract_schema = StructType([\n",
    "    StructField(\"id_contrato\", StringType()),\n",
    "    StructField(\"objeto_contrato\", StringType()),\n",
    "    StructField(\"entidad\", StringType()),\n",
    "    StructField(\"departamento\", StringType()),\n",
    "    StructField(\"municipio\", StringType()),\n",
    "    StructField(\"region\", StringType()),\n",
    "    StructField(\"codigo_unspsc\", StringType()),\n",
    "    StructField(\"descripcion_categoria\", StringType()),\n",
    "    StructField(\"valor_contrato\", DoubleType()),\n",
    "    StructField(\"duracion_dias\", IntegerType()),\n",
    "    StructField(\"fecha_firma\", StringType()),\n",
    "    StructField(\"tipo_contrato\", StringType()),\n",
    "    StructField(\"estado_contrato\", StringType()),\n",
    "    StructField(\"modalidad\", StringType()),\n",
    "    StructField(\"anno\", IntegerType()),\n",
    "    StructField(\"id_interno_sistema\", StringType()),\n",
    "    StructField(\"campo_vacio\", StringType()),\n",
    "    StructField(\"constante_1\", StringType()),\n",
    "    StructField(\"constante_2\", IntegerType()),\n",
    "    StructField(\"duplicate_id\", StringType()),\n",
    "    StructField(\"timestamp_carga\", StringType())\n",
    "])\n",
    "\n",
    "print(\"Leyendo Kafka...\")\n",
    "\n",
    "df_kafka = spark.read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", \"contratos-publicos\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "df_bronze = df_kafka.select(\n",
    "    from_json(col(\"value\").cast(\"string\"), contract_schema).alias(\"data\")\n",
    ").select(\"data.*\")\n",
    "\n",
    "df_bronze = df_bronze.cache()\n",
    "total_kafka = df_bronze.count()\n",
    "\n",
    "print(f\" Mensajes: {total_kafka:,}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f952457-ec33-4c40-b285-f2d1e68c3357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PASO 2: ELIMINAR REDUNDANTES\n",
      "================================================================================\n",
      "\n",
      " Eliminando 6 columnas redundantes\n",
      " Columnas restantes: 15\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[id_contrato: string, objeto_contrato: string, entidad: string, departamento: string, municipio: string, region: string, codigo_unspsc: string, descripcion_categoria: string, valor_contrato: double, duracion_dias: int, fecha_firma: string, tipo_contrato: string, estado_contrato: string, modalidad: string, anno: int, id_interno_sistema: string, campo_vacio: string, constante_1: string, constante_2: int, duplicate_id: string, timestamp_carga: string]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 2. ELIMINAR REDUNDANTES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 2: ELIMINAR REDUNDANTES\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "redundant_columns = [\n",
    "    \"id_interno_sistema\", \"campo_vacio\", \"constante_1\",\n",
    "    \"constante_2\", \"duplicate_id\", \"timestamp_carga\"\n",
    "]\n",
    "\n",
    "print(f\" Eliminando {len(redundant_columns)} columnas redundantes\")\n",
    "\n",
    "df_cleaned = df_bronze.drop(*redundant_columns)\n",
    "\n",
    "print(f\" Columnas restantes: {len(df_cleaned.columns)}\\n\")\n",
    "\n",
    "#  LIBERAR bronze, ya no lo necesitamos\n",
    "df_bronze.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "816b1e7b-d82b-439c-acbf-057230ec288d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PASO 3: LIMPIEZA\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Registros: 50,349\n",
      "\n",
      "Columnas con nulos:\n",
      "   duracion_dias: 50,349 (100.0%)\n",
      "\n",
      " Limpieza:\n",
      "   Antes: 50,349\n",
      "   DespuÃ©s: 50,058\n",
      "   Descartados: 291\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[id_contrato: string, objeto_contrato: string, entidad: string, departamento: string, municipio: string, region: string, codigo_unspsc: string, descripcion_categoria: string, valor_contrato: double, duracion_dias: int, fecha_firma: string, tipo_contrato: string, estado_contrato: string, modalidad: string, anno: int]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 3. LIMPIEZA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 3: LIMPIEZA\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "df_cleaned = df_cleaned.cache()\n",
    "total_cleaned = df_cleaned.count()\n",
    "\n",
    "print(f\" Registros: {total_cleaned:,}\\n\")\n",
    "\n",
    "# AnÃ¡lisis de nulos optimizado\n",
    "null_counts = df_cleaned.select([\n",
    "    count(when(col(c).isNull(), c)).alias(c) \n",
    "    for c in df_cleaned.columns\n",
    "]).collect()[0].asDict()\n",
    "\n",
    "print(\"Columnas con nulos:\")\n",
    "for col_name, null_count in sorted(null_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    if null_count > 0:\n",
    "        pct = (null_count / total_cleaned) * 100\n",
    "        print(f\"   {col_name}: {null_count:,} ({pct:.1f}%)\")\n",
    "\n",
    "# Limpieza\n",
    "df_silver = df_cleaned \\\n",
    "    .filter(col(\"id_contrato\").isNotNull()) \\\n",
    "    .filter(col(\"objeto_contrato\").isNotNull()) \\\n",
    "    .filter(col(\"valor_contrato\").isNotNull()) \\\n",
    "    .filter(col(\"valor_contrato\") > 0) \\\n",
    "    .filter(col(\"fecha_firma\").isNotNull()) \\\n",
    "    .withColumn(\"fecha_firma\", to_date(col(\"fecha_firma\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "df_silver = df_silver.cache()\n",
    "total_silver = df_silver.count()\n",
    "\n",
    "print(f\"\\n Limpieza:\")\n",
    "print(f\"   Antes: {total_cleaned:,}\")\n",
    "print(f\"   DespuÃ©s: {total_silver:,}\")\n",
    "print(f\"   Descartados: {total_cleaned - total_silver:,}\\n\")\n",
    "\n",
    "df_cleaned.unpersist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82037c71-8bfc-4e99-b7fe-f3f24287fe73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PASO 4: ESTADÃSTICAS\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Por regiÃ³n:\n",
      "+--------------+-----+\n",
      "|        region|count|\n",
      "+--------------+-----+\n",
      "|Centro-Oriente|50058|\n",
      "+--------------+-----+\n",
      "\n",
      "\n",
      "ðŸ“Š Top 5 entidades:\n",
      "+-------------------------------------------------+-----+\n",
      "|entidad                                          |count|\n",
      "+-------------------------------------------------+-----+\n",
      "|MUNICIPIO DE SOACHA.                             |3184 |\n",
      "|ALCALDÃA MUNICIPAL COTA                          |1995 |\n",
      "|ESE MUNICIPAL DE SOACHA JULIO CESAR PEÃ‘ALOZA*    |1919 |\n",
      "|CUNDINAMARCA-ALCALDIA MUNICIPIO MOSQUERA         |1879 |\n",
      "|empresa social del estado regiÃ³n de salud soacha.|1579 |\n",
      "+-------------------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 4. ESTADÃSTICAS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 4: ESTADÃSTICAS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"ðŸ“Š Por regiÃ³n:\")\n",
    "df_silver.groupBy(\"region\").count().orderBy(desc(\"count\")).show(5)\n",
    "\n",
    "print(\"\\nðŸ“Š Top 5 entidades:\")\n",
    "df_silver.groupBy(\"entidad\").count().orderBy(desc(\"count\")).show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af77614e-c5ff-4b86-91e2-23ce1d5a253d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PASO 5: GUARDAR EN DELTA LAKE\n",
      "================================================================================\n",
      "\n",
      "ðŸ’¾ Guardando en: /app/notebooks/delta_lake/silver_contracts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 34:====================================================>   (47 + 3) / 50]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Guardado exitosamente\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 5. GUARDAR EN DELTA LAKE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 5: GUARDAR EN DELTA LAKE\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "DELTA_PATH = \"/app/notebooks/delta_lake/silver_contracts\"\n",
    "\n",
    "print(f\"ðŸ’¾ Guardando en: {DELTA_PATH}\")\n",
    "\n",
    "df_silver.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .save(DELTA_PATH)\n",
    "\n",
    "print(\"âœ… Guardado exitosamente\\n\")\n",
    "\n",
    "# âš ï¸ LIBERAR todo\n",
    "df_silver.unpersist()\n",
    "spark.catalog.clearCache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b60ee146-c2f4-4f23-ae53-f98ded14fb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.1\n"
     ]
    }
   ],
   "source": [
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "244fd81e-3b09-4646-99b4-d88f61d8abd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "VERIFICACIÃ“N FINAL\n",
      "================================================================================\n",
      "\n",
      "âœ… Registros verificados: 50,058\n",
      "+------------+----------------------------------------------------+--------------+-----------+\n",
      "|id_contrato |entidad                                             |valor_contrato|fecha_firma|\n",
      "+------------+----------------------------------------------------+--------------+-----------+\n",
      "|011-2024    |ESE HOSPITAL SALAZAR DE VILLETA                     |2.9991E7      |2024-01-01 |\n",
      "|CPS 012-2024|E.S.E HOSPITAL NUESTRA SEÃ‘ORA DEL CARMEN DEL COLEGIO|1.07844E7     |2024-01-01 |\n",
      "|CPS 017-2024|E.S.E HOSPITAL NUESTRA SEÃ‘ORA DEL CARMEN DEL COLEGIO|1.07844E7     |2024-01-01 |\n",
      "|004-2024    |ESE HOSPITAL SALAZAR DE VILLETA                     |2.016E7       |2024-01-01 |\n",
      "|CPS-042-2024|empresa social del estado regiÃ³n de salud soacha.   |7.5323616E7   |2024-01-01 |\n",
      "+------------+----------------------------------------------------+--------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "ðŸŽ¯ Fase 2 completada. Siguiente: Fase 3 - Embeddings\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 6. VERIFICACIÃ“N\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"VERIFICACIÃ“N FINAL\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "df_verify = spark.read.format(\"delta\").load(DELTA_PATH)\n",
    "print(f\"âœ… Registros verificados: {df_verify.count():,}\")\n",
    "\n",
    "df_verify.select(\"id_contrato\", \"entidad\", \"valor_contrato\", \"fecha_firma\") \\\n",
    "    .show(5, truncate=False)\n",
    "\n",
    "print(\"\\nðŸŽ¯ Fase 2 completada. Siguiente: Fase 3 - Embeddings\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fa9b8d-dcf7-4962-8755-3993314ee402",
   "metadata": {},
   "source": [
    "## Fase 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f353f8e1-7564-40cc-ae3f-bc39a496b980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PASO 1: CARGAR DATOS DESDE SILVER\n",
      "================================================================================\n",
      "\n",
      "ðŸ“– Cargando datos desde: /app/notebooks/delta_lake/silver_contracts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:=============================>                             (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Registros cargados: 50,058\n",
      "\n",
      "ðŸ“‹ Esquema de datos:\n",
      "root\n",
      " |-- id_contrato: string (nullable = true)\n",
      " |-- objeto_contrato: string (nullable = true)\n",
      " |-- entidad: string (nullable = true)\n",
      " |-- departamento: string (nullable = true)\n",
      " |-- municipio: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- codigo_unspsc: string (nullable = true)\n",
      " |-- descripcion_categoria: string (nullable = true)\n",
      " |-- valor_contrato: double (nullable = true)\n",
      " |-- duracion_dias: integer (nullable = true)\n",
      " |-- fecha_firma: date (nullable = true)\n",
      " |-- tipo_contrato: string (nullable = true)\n",
      " |-- estado_contrato: string (nullable = true)\n",
      " |-- modalidad: string (nullable = true)\n",
      " |-- anno: integer (nullable = true)\n",
      "\n",
      "\n",
      "ðŸ“Š Muestra de datos:\n",
      "+------------+---------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------+-------------+--------------+-------------+\n",
      "|id_contrato |objeto_contrato                                                                                                                        |entidad                                             |codigo_unspsc|valor_contrato|duracion_dias|\n",
      "+------------+---------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------+-------------+--------------+-------------+\n",
      "|011-2024    |PRESTACION DE SERVICIOS ASISTENCIALES COMO MEDICO GENERAL EN LOS PROCESOS Y SUBPROCESOS DE LA ESE HOSPITAL SALAZAR DE VILLETA          |ESE HOSPITAL SALAZAR DE VILLETA                     |             |2.9991E7      |NULL         |\n",
      "|CPS 012-2024|PRESTAR APOYO AL PROCESO ASISTENCIAL EN EL ÃREA DE AUXILIAR DE ENFERMERÃA A LA E.S.E. HOSPITAL NUESTRA SEÃ‘ORA DEL CARMEN DE EL COLEGIO.|E.S.E HOSPITAL NUESTRA SEÃ‘ORA DEL CARMEN DEL COLEGIO|             |1.07844E7     |NULL         |\n",
      "|CPS 017-2024|PRESTAR APOYO AL PROCESO ASISTENCIAL EN EL ÃREA DE FARMACIA-TÃ‰CNICA PARA LA E.S.E. HOSPITAL NUESTRA SEÃ‘ORA DEL CARMEN DE EL COLEGIO.   |E.S.E HOSPITAL NUESTRA SEÃ‘ORA DEL CARMEN DEL COLEGIO|             |1.07844E7     |NULL         |\n",
      "+------------+---------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------+-------------+--------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 1. CARGAR DATOS DESDE SILVER\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PASO 1: CARGAR DATOS DESDE SILVER\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "SILVER_PATH = \"/app/notebooks/delta_lake/silver_contracts\"\n",
    "\n",
    "print(f\"ðŸ“– Cargando datos desde: {SILVER_PATH}\")\n",
    "\n",
    "df_silver = spark.read.format(\"delta\").load(SILVER_PATH)\n",
    "df_silver = df_silver.cache()\n",
    "\n",
    "total_records = df_silver.count()\n",
    "print(f\"âœ… Registros cargados: {total_records:,}\\n\")\n",
    "\n",
    "print(\"ðŸ“‹ Esquema de datos:\")\n",
    "df_silver.printSchema()\n",
    "\n",
    "print(\"\\nðŸ“Š Muestra de datos:\")\n",
    "df_silver.select(\n",
    "    \"id_contrato\", \"objeto_contrato\", \"entidad\", \n",
    "    \"codigo_unspsc\", \"valor_contrato\", \"duracion_dias\"\n",
    ").show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18f418b5-05f7-4432-a3e9-ca4bc34e5141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PASO 2: LIMPIEZA Y PREPARACIÃ“N DE TEXTO (NORMALIZADO)\n",
      "================================================================================\n",
      "\n",
      "ðŸ§¹ Limpiando columna 'objeto_contrato' y eliminando tildes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:=============================>                            (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Registros despuÃ©s de limpieza: 50,058\n",
      "\n",
      " Ejemplo de texto limpio (Sin tildes):\n",
      "+--------------------------------------------------+--------------------------------------------------+\n",
      "|                                   objeto_contrato|                                     objeto_limpio|\n",
      "+--------------------------------------------------+--------------------------------------------------+\n",
      "|PRESTACION DE SERVICIOS ASISTENCIALES COMO MEDI...|prestacion de servicios asistenciales como medi...|\n",
      "|PRESTAR APOYO AL PROCESO ASISTENCIAL EN EL ÃREA...|prestar apoyo al proceso asistencial en el area...|\n",
      "|PRESTAR APOYO AL PROCESO ASISTENCIAL EN EL ÃREA...|prestar apoyo al proceso asistencial en el area...|\n",
      "|APOYO PROFESIONAL A LA GESTIÃ“N ASISTENCIAL COMO...|apoyo profesional a la gestion asistencial como...|\n",
      "|PRESTACIÃ“N DE SERVICIOS COMO PROFESIONAL EN MED...|prestacion de servicios como profesional en med...|\n",
      "+--------------------------------------------------+--------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PASO 2: LIMPIEZA Y PREPARACIÃ“N DE TEXTO (NORMALIZADO)\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"ðŸ§¹ Limpiando columna 'objeto_contrato' y eliminando tildes...\")\n",
    "\n",
    "# Definimos los caracteres a reemplazar\n",
    "# Nota: IncluÃ­ la 'Ã±' -> 'n' y la 'Ã¼' -> 'u'. \n",
    "# Si quieres CONSERVAR la Ã±, quÃ­tala de estas cadenas.\n",
    "src_chars = \"Ã¡Ã©Ã­Ã³ÃºÃ¼Ã±\"\n",
    "dst_chars = \"aeiouun\"\n",
    "\n",
    "df_prepared = df_silver.withColumn(\n",
    "    \"objeto_limpio\",\n",
    "    trim(                                           # 4. Quitar espacios al inicio/final\n",
    "        regexp_replace(                             # 3. Colapsar espacios mÃºltiples\n",
    "            regexp_replace(                         # 2. Eliminar caracteres especiales\n",
    "                translate(                          # 1. Reemplazar tildes\n",
    "                    lower(col(\"objeto_contrato\")),  # 0. Convertir a minÃºsculas\n",
    "                    src_chars, \n",
    "                    dst_chars\n",
    "                ),\n",
    "                \"[^a-z0-9\\\\s]\", \" \" # Solo deja letras a-z (sin tildes), nÃºmeros y espacios\n",
    "            ),\n",
    "            \"\\\\s+\", \" \"\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# Filtrar textos muy cortos\n",
    "df_prepared = df_prepared.filter(length(col(\"objeto_limpio\")) >= 10)\n",
    "\n",
    "print(f\" Registros despuÃ©s de limpieza: {df_prepared.count():,}\")\n",
    "\n",
    "print(\"\\n Ejemplo de texto limpio (Sin tildes):\")\n",
    "df_prepared.select(\"objeto_contrato\", \"objeto_limpio\").show(5, truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb8b89d1-bb12-449c-b504-a243d1b8e33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¤ PASO 3.1: TokenizaciÃ³n y Stopwords\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "   â€¢ Stopwords definidos: 65 (bÃ¡sicos + contratos)\n",
      "   âœ… TokenizaciÃ³n y stopwords completados\n",
      "\n",
      "ðŸ“‹ Ejemplo:\n",
      "+----------------------------------------------------------------------+----------------------------------------------------------------------+\n",
      "|                                                         objeto_limpio|                                                palabras_sin_stopwords|\n",
      "+----------------------------------------------------------------------+----------------------------------------------------------------------+\n",
      "|prestacion de servicios asistenciales como medico general en los pr...|[asistenciales, medico, general, procesos, subprocesos, hospital, s...|\n",
      "|prestar apoyo al proceso asistencial en el area de auxiliar de enfe...|[prestar, apoyo, proceso, asistencial, area, auxiliar, enfermeria, ...|\n",
      "+----------------------------------------------------------------------+----------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PASO 3.1: DEFINIR STOPWORDS Y TOKENIZAR\n",
    "# ============================================================================\n",
    "\n",
    "print(\"ðŸ”¤ PASO 3.1: TokenizaciÃ³n y Stopwords\")\n",
    "print(\"-\" * 80 + \"\\n\")\n",
    "\n",
    "# Stopwords especializados para contratos\n",
    "stopwords_es = [\n",
    "    # BÃ¡sicos espaÃ±ol\n",
    "    \"el\", \"la\", \"de\", \"que\", \"y\", \"a\", \"en\", \"un\", \"ser\", \"se\", \"no\",\n",
    "    \"por\", \"con\", \"su\", \"para\", \"como\", \"estar\", \"tener\", \"le\", \"lo\",\n",
    "    \"pero\", \"hacer\", \"o\", \"este\", \"otro\", \"ese\", \"si\", \"ya\", \"ver\",\n",
    "    \"dar\", \"muy\", \"sin\", \"sobre\", \"tambiÃ©n\", \"hasta\", \"aÃ±o\", \"entre\",\n",
    "    \"del\", \"al\", \"los\", \"las\", \"uno\", \"una\", \"unos\", \"unas\",\n",
    "    # EspecÃ­ficos de contratos pÃºblicos\n",
    "    \"contrato\", \"contratos\", \"objeto\", \"prestacion\", \"prestaciÃ³n\",\n",
    "    \"servicio\", \"servicios\", \"suministro\", \"ejecucion\", \"ejecuciÃ³n\",\n",
    "    \"acuerdo\", \"establecido\", \"pliego\", \"condiciones\", \"especificaciones\",\n",
    "    \"entidad\", \"contratante\", \"contratista\", \"plazo\", \"termino\"\n",
    "]\n",
    "\n",
    "print(f\"   â€¢ Stopwords definidos: {len(stopwords_es)} (bÃ¡sicos + contratos)\")\n",
    "\n",
    "# TokenizaciÃ³n\n",
    "tokenizer = Tokenizer(inputCol=\"objeto_limpio\", outputCol=\"palabras\")\n",
    "df_tokenized = tokenizer.transform(df_prepared)\n",
    "\n",
    "# Remover stopwords\n",
    "remover = StopWordsRemover(\n",
    "    inputCol=\"palabras\", \n",
    "    outputCol=\"palabras_sin_stopwords\",\n",
    "    stopWords=stopwords_es\n",
    ")\n",
    "df_filtered = remover.transform(df_tokenized)\n",
    "\n",
    "print(\"   âœ… TokenizaciÃ³n y stopwords completados\\n\")\n",
    "\n",
    "print(\"ðŸ“‹ Ejemplo:\")\n",
    "df_filtered.select(\"objeto_limpio\", \"palabras_sin_stopwords\").show(2, truncate=70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bc2043b-4721-4da7-a242-b0e4bf879ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” PASO 3.2: Limpieza y Filtrado\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "   â€¢ Filtro aplicado: palabras >= 3 caracteres\n",
      "   â€¢ Documentos vacÃ­os eliminados\n",
      "   âœ… Limpieza completada\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PASO 3.2: LIMPIEZA Y FILTRADO\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nðŸ” PASO 3.2: Limpieza y Filtrado\")\n",
    "print(\"-\" * 80 + \"\\n\")\n",
    "\n",
    "# Filtrar palabras muy cortas (ruido)\n",
    "def clean_words(words):\n",
    "    \"\"\"Mantener solo palabras >= 3 caracteres\"\"\"\n",
    "    if not words:\n",
    "        return []\n",
    "    return [w for w in words if len(w) >= 3]\n",
    "\n",
    "clean_udf = udf(clean_words, ArrayType(StringType()))\n",
    "\n",
    "df_filtered = df_filtered.withColumn(\n",
    "    \"palabras_filtradas\",\n",
    "    clean_udf(col(\"palabras_sin_stopwords\"))\n",
    ")\n",
    "\n",
    "# Eliminar documentos vacÃ­os\n",
    "df_filtered = df_filtered.filter(size(col(\"palabras_filtradas\")) > 0)\n",
    "\n",
    "print(\"   â€¢ Filtro aplicado: palabras >= 3 caracteres\")\n",
    "print(\"   â€¢ Documentos vacÃ­os eliminados\")\n",
    "print(\"   âœ… Limpieza completada\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbde6876-943f-4de2-8206-dd5afc874ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PASO 4: GENERACIÃ“N DE EMBEDDINGS CON WORD2VEC\n",
      "================================================================================\n",
      "\n",
      "ðŸ”¢ Entrenando modelo Word2Vec...\n",
      "   - Vector size: 100 dimensiones\n",
      "   - Min word count: 2 (palabras que aparecen al menos 2 veces)\n",
      "   - Iterations: 10\n",
      "\n",
      "â³ Entrenando (esto puede tardar 1-2 minutos)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Word2Vec entrenado y aplicado\n",
      "\n",
      "ðŸ“Š TamaÃ±o del vocabulario: 9,249 palabras Ãºnicas\n",
      "\n",
      "ðŸ“‹ Ejemplo de embeddings (primeros 10 valores):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|id_contrato |embedding_muestra                                                                                                                                                                                                       |\n",
      "+------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|011-2024    |[0.199337643571198, -0.11567123653367162, -0.06517249078024179, -0.3478297144174576, 0.1609452865086496, 0.04875763412564993, -0.3183848101180047, -0.17065105272922665, 0.07957734051160514, -0.03430204698815942]     |\n",
      "|CPS 012-2024|[-0.08867999973396459, -0.09253484794559577, -0.19816266000270844, -0.4655261902759472, -0.03023647105631729, 0.011915724103649456, -0.11066972992072502, 0.35061018068032956, 0.13556776424714673, 0.23267638124525547]|\n",
      "+------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "\n",
      "ðŸ“ Verificando dimensiÃ³n del embedding:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 52:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+\n",
      "| id_contrato|dimension_embedding|\n",
      "+------------+-------------------+\n",
      "|    011-2024|                100|\n",
      "|CPS 012-2024|                100|\n",
      "|CPS 017-2024|                100|\n",
      "|    004-2024|                100|\n",
      "|CPS-042-2024|                100|\n",
      "+------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 4. GENERACIÃ“N DE EMBEDDINGS CON WORD2VEC\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PASO 4: GENERACIÃ“N DE EMBEDDINGS CON WORD2VEC\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"ðŸ”¢ Entrenando modelo Word2Vec...\")\n",
    "print(\"   - Vector size: 100 dimensiones\")\n",
    "print(\"   - Min word count: 2 (palabras que aparecen al menos 2 veces)\")\n",
    "print(\"   - Iterations: 10\\n\")\n",
    "\n",
    "# Configurar Word2Vec\n",
    "word2vec = Word2Vec(\n",
    "    vectorSize=100,\n",
    "    minCount=2,\n",
    "    maxIter=10,\n",
    "    seed=42,\n",
    "    inputCol=\"palabras_filtradas\",\n",
    "    outputCol=\"embedding_raw\"\n",
    ")\n",
    "\n",
    "# Entrenar Word2Vec\n",
    "print(\"â³ Entrenando (esto puede tardar 1-2 minutos)...\")\n",
    "word2vec_model = word2vec.fit(df_filtered)\n",
    "\n",
    "# Aplicar el modelo\n",
    "df_embeddings = word2vec_model.transform(df_filtered)\n",
    "\n",
    "print(\"âœ… Word2Vec entrenado y aplicado\")\n",
    "\n",
    "# EstadÃ­sticas del vocabulario\n",
    "vocab_size = len(word2vec_model.getVectors().collect())\n",
    "print(f\"\\nðŸ“Š TamaÃ±o del vocabulario: {vocab_size:,} palabras Ãºnicas\")\n",
    "\n",
    "print(\"\\nðŸ“‹ Ejemplo de embeddings (primeros 10 valores):\")\n",
    "\n",
    "# SOLUCIÃ“N: Crear UDF para convertir Vector a Array\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "from pyspark.ml.linalg import VectorUDT, Vectors\n",
    "\n",
    "# UDF para convertir Vector de Spark ML a Array\n",
    "def vector_to_array(vector):\n",
    "    return vector.toArray().tolist() if vector is not None else None\n",
    "\n",
    "vector_to_array_udf = udf(vector_to_array, ArrayType(DoubleType()))\n",
    "\n",
    "# Convertir el embedding a array y mostrar primeros 10 valores\n",
    "df_embeddings.select(\n",
    "    \"id_contrato\",\n",
    "    slice(vector_to_array_udf(col(\"embedding_raw\")), 1, 10).alias(\"embedding_muestra\")\n",
    ").show(2, truncate=False)\n",
    "\n",
    "# Opcional: Ver el tamaÃ±o completo del embedding\n",
    "print(\"\\nðŸ“ Verificando dimensiÃ³n del embedding:\")\n",
    "df_embeddings.select(\n",
    "    \"id_contrato\",\n",
    "    size(vector_to_array_udf(col(\"embedding_raw\"))).alias(\"dimension_embedding\")\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd6d8a14-d303-454a-ae88-91f3c95a00b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def target_encode_smooth(df, cat_col, target_col, m=50):\n",
    "    \"\"\"\n",
    "    Jamesâ€“Stein Smoothed Target Encoding para PySpark.\n",
    "    df: DataFrame\n",
    "    cat_col: columna categÃ³rica (string)\n",
    "    target_col: variable objetivo (numÃ©rica continua)\n",
    "    m: parÃ¡metro de suavizado (entre 10 y 200)\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Promedio global del target\n",
    "    global_mean = df.agg(F.mean(target_col)).first()[0]\n",
    "\n",
    "    # 2. EstadÃ­sticas por categorÃ­a\n",
    "    stats = (\n",
    "        df.groupBy(cat_col)\n",
    "          .agg(\n",
    "              F.mean(target_col).alias(\"cat_mean\"),\n",
    "              F.count(target_col).alias(\"cat_count\")\n",
    "          )\n",
    "          .withColumn(\n",
    "              f\"{cat_col}_te\",\n",
    "              (F.col(\"cat_count\") * F.col(\"cat_mean\") + m * F.lit(global_mean)) \n",
    "              / (F.col(\"cat_count\") + m)\n",
    "          )\n",
    "          .select(cat_col, f\"{cat_col}_te\")\n",
    "    )\n",
    "    # 3. Join de vuelta al DataFrame\n",
    "    df_encoded = df.join(stats, on=cat_col, how=\"left\")\n",
    "\n",
    "    return df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc240153-d1f5-454f-afd7-e96616fc2ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PASO 5: TARGET ENCODING (Suavizado Jamesâ€“Stein)\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPASO 5: TARGET ENCODING (Suavizado Jamesâ€“Stein)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m df_te = \u001b[43mdf_clean\u001b[49m  \u001b[38;5;66;03m# viene del mÃ³dulo anterior\u001b[39;00m\n\u001b[32m      7\u001b[39m categorical_cols = [\u001b[33m\"\u001b[39m\u001b[33mentidad\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdescripcion_categoria\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmunicipio\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodalidad\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfecha_firma\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mestado_contrato\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col_name \u001b[38;5;129;01min\u001b[39;00m categorical_cols:\n",
      "\u001b[31mNameError\u001b[39m: name 'df_clean' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PASO 5: TARGET ENCODING (Suavizado Jamesâ€“Stein)\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "df_te = df_clean  # viene del mÃ³dulo anterior\n",
    "\n",
    "categorical_cols = [\"entidad\", \"descripcion_categoria\", \"municipio\", \"modalidad\", \"fecha_firma\",\"estado_contrato\"]\n",
    "\n",
    "for col_name in categorical_cols:\n",
    "    print(f\"â³ Codificando {col_name}...\")\n",
    "    df_te = target_encode_smooth(df_te, col_name, target_col=\"valor_contrato\", m=50)\n",
    "    print(f\"âœ… {col_name} codificado -> {col_name}_te\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ CodificaciÃ³nÂ completada!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e53513ee-6977-4c8c-bd92-4009f93dc1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Verificando columnas de Target Encoding...\n",
      "âœ“ Todas las columnas TE disponibles: ['entidad_te', 'codigo_unspsc_te', 'departamento_te', 'modalidad_te']\n",
      "\n",
      "ðŸ“Š EstadÃ­sticas de Target Encoding:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  entidad_te:\n",
      "    Min : 15491513.5024\n",
      "    Max : 655285394.1297\n",
      "    Mean: 102528222.0446\n",
      "    Std : 97687942.7203\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  codigo_unspsc_te:\n",
      "    Min : 100309896.7452\n",
      "    Max : 100309896.7452\n",
      "    Mean: 100309896.7451\n",
      "    Std : 0.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  departamento_te:\n",
      "    Min : 100309896.7452\n",
      "    Max : 100309896.7452\n",
      "    Mean: 100309896.7451\n",
      "    Std : 0.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 816:============================>                            (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  modalidad_te:\n",
      "    Min : 32414588.3887\n",
      "    Max : 3635469506.6244\n",
      "    Mean: 92542889.5217\n",
      "    Std : 322995855.6501\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# VERIFICAR QUE TENEMOS LAS COLUMNAS DE TARGET ENCODING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"ðŸ“Š Verificando columnas de Target Encoding...\")\n",
    "\n",
    "# Columnas que deberÃ­an existir despuÃ©s del Target Encoding\n",
    "te_columns = [\"entidad_te\", \"codigo_unspsc_te\", \"departamento_te\", \"modalidad_te\"]\n",
    "\n",
    "missing_cols = [col for col in te_columns if col not in df_te.columns]\n",
    "\n",
    "if missing_cols:\n",
    "    print(f\"âš   ERROR: Faltan columnas: {missing_cols}\")\n",
    "    print(\"   Ejecuta primero el PASO 5 del cÃ³digo original (Target Encoding)\\n\")\n",
    "else:\n",
    "    print(f\"âœ“ Todas las columnas TE disponibles: {te_columns}\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# MOSTRAR ESTADÃSTICAS DE TARGET ENCODING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"ðŸ“Š EstadÃ­sticas de Target Encoding:\\n\")\n",
    "\n",
    "for col in te_columns:\n",
    "    stats = df_te.select(\n",
    "        F.min(col).alias(\"min\"),\n",
    "        F.max(col).alias(\"max\"),\n",
    "        F.mean(col).alias(\"mean\"),\n",
    "        F.stddev(col).alias(\"std\")\n",
    "    ).collect()[0]\n",
    "\n",
    "    print(f\"  {col}:\")\n",
    "    print(f\"    Min : {stats['min']:.4f}\")\n",
    "    print(f\"    Max : {stats['max']:.4f}\")\n",
    "    print(f\"    Mean: {stats['mean']:.4f}\")\n",
    "    print(f\"    Std : {stats['std']:.4f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "944a1233-5f79-4b1a-8c52-e22518d060f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PASO 6 MODIFICADO: ENSAMBLAR FEATURES CON TARGET ENCODING\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Features a ensamblar:\n",
      "  âœ“ embedding_raw (100 dims)\n",
      "  âœ“ entidad_te (Target Encoding)\n",
      "  âœ“ codigo_unspsc_te (Target Encoding)\n",
      "  âœ“ departamento_te (Target Encoding)\n",
      "  âœ“ modalidad_te (Target Encoding)\n",
      "  âœ“ duracion_dias (numÃ©rica)\n",
      "\n",
      "ðŸ“Š Total columnas a ensamblar: 6\n",
      "   Input cols: ['embedding_raw', 'entidad_te', 'codigo_unspsc_te', 'departamento_te', 'modalidad_te', 'duracion_dias']\n",
      "\n",
      "ðŸ“Š Ensamblando features...\n",
      "âœ“ Features ensambladas en un solo vector\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š RESUMEN DE DIMENSIONALIDAD:\n",
      "============================================================\n",
      "  - Embedding Word2Vec:        100 dimensiones\n",
      "  - Target Encoding (4 vars):    4 dimensiones\n",
      "  - duracion_dias:               1 dimensiÃ³n\n",
      "============================================================\n",
      "  TOTAL:                       105 dimensiones\n",
      "  (vs 183 dimensiones anteriores con One-Hot)\n",
      "  ReducciÃ³n: 42.6%\n",
      "============================================================\n",
      "\n",
      "ðŸ“Š VerificaciÃ³n de calidad:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ“ Features nulos: 0 (debe ser 0)\n",
      "\n",
      "ðŸ“Š Muestra de datos ensamblados:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 859:>                                                        (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------------------+--------------------+--------------------+-------------+\n",
      "|id_contrato |entidad_te          |codigo_unspsc_te    |departamento_te     |modalidad_te        |duracion_dias|\n",
      "+------------+--------------------+--------------------+--------------------+--------------------+-------------+\n",
      "|011-2024    |2.7179094349785384E7|1.0030989674523553E8|1.0030989674523553E8|4.9149497466489606E7|0            |\n",
      "|CPS 012-2024|2.605177241000303E7 |1.0030989674523553E8|1.0030989674523553E8|4.9149497466489606E7|0            |\n",
      "|CPS 017-2024|2.605177241000303E7 |1.0030989674523553E8|1.0030989674523553E8|4.9149497466489606E7|0            |\n",
      "|004-2024    |2.7179094349785384E7|1.0030989674523553E8|1.0030989674523553E8|4.9149497466489606E7|0            |\n",
      "|CPS-042-2024|1.5150127860666776E8|1.0030989674523553E8|1.0030989674523553E8|4.9149497466489606E7|0            |\n",
      "+------------+--------------------+--------------------+--------------------+--------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PASO 6 MODIFICADO: ENSAMBLAR FEATURES USANDO TARGET ENCODING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 6 MODIFICADO: ENSAMBLAR FEATURES CON TARGET ENCODING\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Renombrar df_te a df_encoded para mantener compatibilidad\n",
    "df_encoded = df_te\n",
    "\n",
    "# Manejar duracion_dias nulos (imputar con 0)\n",
    "df_encoded = df_encoded.fillna({\"duracion_dias\": 0})\n",
    "\n",
    "# ============================================================================\n",
    "# CONSTRUIR LISTA DE COLUMNAS - USANDO TARGET ENCODING\n",
    "# ============================================================================\n",
    "\n",
    "input_cols = [\"embedding_raw\"]  # Siempre incluimos el embedding\n",
    "\n",
    "print(\"ðŸ“Š Features a ensamblar:\")\n",
    "print(f\"  âœ“ embedding_raw (100 dims)\")\n",
    "\n",
    "# 1. AGREGAR TARGET ENCODING\n",
    "target_encoding_cols = [\"entidad_te\", \"codigo_unspsc_te\", \"departamento_te\", \"modalidad_te\"]\n",
    "\n",
    "for col in target_encoding_cols:\n",
    "    input_cols.append(col)\n",
    "    print(f\"  âœ“ {col} (Target Encoding)\")\n",
    "\n",
    "# 2. AGREGAR VARIABLE NUMÃ‰RICA\n",
    "input_cols.append(\"duracion_dias\")\n",
    "print(f\"  âœ“ duracion_dias (numÃ©rica)\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Total columnas a ensamblar: {len(input_cols)}\")\n",
    "print(f\"   Input cols: {input_cols}\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# ENSAMBLAR FEATURES\n",
    "# ============================================================================\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=input_cols,\n",
    "    outputCol=\"features_raw\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "\n",
    "print(\"ðŸ“Š Ensamblando features...\")\n",
    "df_assembled = assembler.transform(df_encoded)\n",
    "print(\"âœ“ Features ensambladas en un solo vector\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# VERIFICAR DIMENSIÃ“N DEL VECTOR\n",
    "# ============================================================================\n",
    "\n",
    "sample_features = df_assembled.select(\"features_raw\").first()[0]\n",
    "feature_dim = len(sample_features)\n",
    "\n",
    "print(\"ðŸ“Š RESUMEN DE DIMENSIONALIDAD:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  - Embedding Word2Vec:        100 dimensiones\")\n",
    "print(f\"  - Target Encoding (4 vars):    4 dimensiones\")\n",
    "print(f\"  - duracion_dias:               1 dimensiÃ³n\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  TOTAL:                       {feature_dim} dimensiones\")\n",
    "print(f\"  (vs 183 dimensiones anteriores con One-Hot)\")\n",
    "print(f\"  ReducciÃ³n: {(1 - feature_dim/183)*100:.1f}%\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ============================================================================\n",
    "# VERIFICACIÃ“N DE CALIDAD\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nðŸ“Š VerificaciÃ³n de calidad:\")\n",
    "\n",
    "# 1. Verificar que no hay valores nulos en features\n",
    "null_features = df_assembled.filter(F.col(\"features_raw\").isNull()).count()\n",
    "print(f\"  âœ“ Features nulos: {null_features} (debe ser 0)\")\n",
    "\n",
    "# 2. Mostrar muestra\n",
    "print(\"\\nðŸ“Š Muestra de datos ensamblados:\")\n",
    "df_assembled.select(\n",
    "    \"id_contrato\",\n",
    "    \"entidad_te\",\n",
    "    \"codigo_unspsc_te\",\n",
    "    \"departamento_te\", \n",
    "    \"modalidad_te\",\n",
    "    \"duracion_dias\"\n",
    ").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5961ad0a-2858-4338-9c71-86d3657b1abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PASO 7: NORMALIZACIÃ“N (StandardScaler)\n",
      "================================================================================\n",
      "\n",
      "ðŸ“ Aplicando StandardScaler para normalizar features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2166:============================>                           (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Features normalizadas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 7. NORMALIZACIÃ“N DE FEATURES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PASO 7: NORMALIZACIÃ“N (StandardScaler)\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"ðŸ“ Aplicando StandardScaler para normalizar features...\")\n",
    "\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "# Normalizar todas las features numÃ©ricas incluidas las embeddings\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features_raw\",\n",
    "    outputCol=\"features_scaled\",\n",
    "    withStd=True,\n",
    "    withMean=True  # Centrar los datos para PCA\n",
    ")\n",
    "\n",
    "scaler_model = scaler.fit(df_assembled)\n",
    "df_scaled = scaler_model.transform(df_assembled)\n",
    "\n",
    "print(\"âœ… Features normalizadas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1db2f9cf-6bb2-431d-a24d-5fd896d54833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PASO 4: ANÃLISIS DE CORRELACIÃ“N EFICIENTE\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š MÃ©todo: Calcular correlaciÃ³n usando vectores completos\n",
      "   (Evita expandir 100 columnas â†’ mucho mÃ¡s rÃ¡pido)\n",
      "\n",
      "â³ CorrelaciÃ³n de variables categÃ³ricas...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  entidad_te                â†’  0.0977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  codigo_unspsc_te          â†’     nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  departamento_te           â†’     nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  modalidad_te              â†’  0.3198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2249:============================>                           (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  duracion_dias             â†’     nan\n",
      "\n",
      "âœ“ Correlaciones categÃ³ricasÂ calculadas\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PASO 4: CORRELACIÃ“N EFICIENTE (SIN EXPANDIR COLUMNAS)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 4: ANÃLISIS DE CORRELACIÃ“N EFICIENTE\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"ðŸ“Š MÃ©todo: Calcular correlaciÃ³n usando vectores completos\")\n",
    "print(\"   (Evita expandir 100 columnas â†’ mucho mÃ¡s rÃ¡pido)\\n\")\n",
    "\n",
    "# Crear dataset con features + target\n",
    "df_with_target = df_scaled.withColumn(\n",
    "    \"target_vector\",\n",
    "    F.array(F.col(\"valor_contrato\"))\n",
    ")\n",
    "\n",
    "# Calcular correlaciÃ³n solo de variables categÃ³ricas (rÃ¡pido)\n",
    "print(\"â³ CorrelaciÃ³n de variables categÃ³ricas...\")\n",
    "\n",
    "cat_correlations = {}\n",
    "\n",
    "for var in te_columns + [\"duracion_dias\"]:\n",
    "    assembler_pair = VectorAssembler(\n",
    "        inputCols=[var, \"valor_contrato\"],\n",
    "        outputCol=\"features_pair\",\n",
    "        handleInvalid=\"skip\"\n",
    "    )\n",
    "    \n",
    "    df_pair = assembler_pair.transform(df_scaled)\n",
    "    corr_matrix = Correlation.corr(df_pair, \"features_pair\", \"pearson\").collect()[0][0]\n",
    "    corr_value = corr_matrix.toArray()[0, 1]\n",
    "    \n",
    "    cat_correlations[var] = corr_value\n",
    "    print(f\"  {var:<25} â†’ {corr_value:>7.4f}\")\n",
    "\n",
    "print(\"\\nâœ“ Correlaciones categÃ³ricasÂ calculadas\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7ede34fc-65ab-4ec1-8064-0df60e778119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PASO 5: CORRELACIÃ“N DEL EMBEDDING (MÃ©todo Muestral)\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Estrategia: Calcular correlaciÃ³n en una MUESTRA pequeÃ±a\n",
      "   (Mucho mÃ¡s rÃ¡pido, resultados representativos)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ Muestra: 4,964 registros (10%)\n",
      "â³ Calculando correlaciones de embedding...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Procesadas 20/100 dimensiones...\n",
      "  Procesadas 40/100 dimensiones...\n",
      "  Procesadas 60/100 dimensiones...\n",
      "  Procesadas 80/100 dimensiones...\n",
      "  Procesadas 100/100 dimensiones...\n",
      "\n",
      "âœ“ Correlaciones de embedding calculadas\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PASO 5: CORRELACIÃ“N DEL EMBEDDING (MÃ‰TODO EFICIENTE)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 5: CORRELACIÃ“N DEL EMBEDDING (MÃ©todo Muestral)\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"ðŸ“Š Estrategia: Calcular correlaciÃ³n en una MUESTRA pequeÃ±a\")\n",
    "print(\"   (Mucho mÃ¡s rÃ¡pido, resultados representativos)\\n\")\n",
    "\n",
    "# Tomar muestra del 10% (suficiente para correlaciones)\n",
    "SAMPLE_FRACTION = 0.1\n",
    "df_sample = df_scaled.sample(withReplacement=False, fraction=SAMPLE_FRACTION, seed=42)\n",
    "\n",
    "sample_size = df_sample.count()\n",
    "print(f\"â³ Muestra: {sample_size:,} registros ({SAMPLE_FRACTION*100:.0f}%)\")\n",
    "\n",
    "# Convertir a Pandas para cÃ¡lculo eficiente\n",
    "print(\"â³ Calculando correlaciones de embedding...\\n\")\n",
    "\n",
    "# Extraer embeddings y target a arrays numpy\n",
    "data_sample = df_sample.select(\"embedding_raw\", \"valor_contrato\").collect()\n",
    "\n",
    "embeddings_array = np.array([row[\"embedding_raw\"].toArray() for row in data_sample])\n",
    "target_array = np.array([row[\"valor_contrato\"] for row in data_sample])\n",
    "\n",
    "# Calcular correlaciÃ³n de cada dimensiÃ³n con el target\n",
    "embedding_correlations = {}\n",
    "\n",
    "for i in range(100):\n",
    "    correlation = np.corrcoef(embeddings_array[:, i], target_array)[0, 1]\n",
    "    embedding_correlations[f\"emb_{i}\"] = correlation\n",
    "    \n",
    "    if (i + 1) % 20 == 0:\n",
    "        print(f\"  Procesadas {i + 1}/100 dimensiones...\")\n",
    "\n",
    "print(\"\\nâœ“ Correlaciones de embedding calculadas\\n\")\n",
    "\n",
    "# Consolidar todas las correlaciones\n",
    "all_correlations = {**cat_correlations, **embedding_correlations}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bc243579-820c-4873-a597-564580a79207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PASO 6: SELECCIÃ“N DE VARIABLES\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š EstadÃ­sticas de correlaciones:\n",
      "  - Min:    -0.1043\n",
      "  - Max:    0.3198\n",
      "  - Mean:   -0.0034\n",
      "  - Median: -0.0068\n",
      "\n",
      "ðŸ“Š Top 15 variables con mayor correlaciÃ³n absoluta:\n",
      "   1. codigo_unspsc_te          â†’     nan\n",
      "   2. departamento_te           â†’     nan\n",
      "   3. modalidad_te              â†’  0.3198\n",
      "   4. duracion_dias             â†’     nan\n",
      "   5. emb_1                     â†’  0.1071\n",
      "   6. emb_36                    â†’ -0.1043\n",
      "   7. emb_43                    â†’ -0.1027\n",
      "   8. entidad_te                â†’  0.0977\n",
      "   9. emb_90                    â†’ -0.0923\n",
      "  10. emb_95                    â†’ -0.0912\n",
      "  11. emb_59                    â†’  0.0840\n",
      "  12. emb_83                    â†’  0.0792\n",
      "  13. emb_81                    â†’  0.0787\n",
      "  14. emb_11                    â†’  0.0749\n",
      "  15. emb_15                    â†’ -0.0728\n",
      "\n",
      "ðŸ“Š Umbral de selecciÃ³n: |r| >= 0.05\n",
      "\n",
      "âœ… Variables SELECCIONADAS: 26\n",
      "âŒ Variables RECHAZADAS: 79\n",
      "\n",
      "  CategÃ³ricas/NumÃ©ricas: 2\n",
      "  Embeddings: 24\n",
      "\n",
      "ðŸ“Š Variables categÃ³ricas/numÃ©ricas:\n",
      "  âœ“ entidad_te                â†’  0.0977\n",
      "  âœ“ modalidad_te              â†’  0.3198\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PASO 6: ANÃLISIS Y SELECCIÃ“N\n",
    "# ============================================================================\n",
    "\n",
    "import builtins  # para asegurar abs() nativo\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 6: SELECCIÃ“N DE VARIABLES\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# EstadÃ­sticas\n",
    "corr_values = list(all_correlations.values())\n",
    "print(\"ðŸ“Š EstadÃ­sticas de correlaciones:\")\n",
    "print(f\"  - Min:    {np.nanmin(corr_values):.4f}\")\n",
    "print(f\"  - Max:    {np.nanmax(corr_values):.4f}\")\n",
    "print(f\"  - Mean:   {np.nanmean(corr_values):.4f}\")\n",
    "print(f\"  - Median: {np.nanmedian(corr_values):.4f}\")\n",
    "\n",
    "# Top 15\n",
    "print(\"\\nðŸ“Š Top 15 variables con mayor correlaciÃ³n absoluta:\")\n",
    "sorted_all = sorted(\n",
    "    all_correlations.items(),\n",
    "    key=lambda x: builtins.abs(x[1]),\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "for i, (var, corr) in enumerate(sorted_all[:15], 1):\n",
    "    print(f\"  {i:2d}. {var:<25} â†’ {corr:>7.4f}\")\n",
    "\n",
    "# Umbral\n",
    "THRESHOLD = 0.05\n",
    "print(f\"\\nðŸ“Š Umbral de selecciÃ³n: |r| >= {THRESHOLD}\\n\")\n",
    "\n",
    "selected_vars = {var: corr for var, corr in all_correlations.items() \n",
    "                 if builtins.abs(corr) >= THRESHOLD and not np.isnan(corr)}\n",
    "rejected_vars = {var: corr for var, corr in all_correlations.items() \n",
    "                 if builtins.abs(corr) < THRESHOLD or np.isnan(corr)}\n",
    "\n",
    "print(f\"âœ… Variables SELECCIONADAS: {len(selected_vars)}\")\n",
    "print(f\"âŒ Variables RECHAZADAS: {len(rejected_vars)}\")\n",
    "\n",
    "# Desglosar\n",
    "selected_cat = [v for v in selected_vars.keys() if not v.startswith(\"emb_\")]\n",
    "selected_emb = [v for v in selected_vars.keys() if v.startswith(\"emb_\")]\n",
    "\n",
    "print(f\"\\n  CategÃ³ricas/NumÃ©ricas: {len(selected_cat)}\")\n",
    "print(f\"  Embeddings: {len(selected_emb)}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Variables categÃ³ricas/numÃ©ricas:\")\n",
    "for var in selected_cat:\n",
    "    print(f\"  {'âœ“' if var in selected_vars else 'âœ—'} {var:<25} â†’ {all_correlations[var]:>7.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1c26c545-de11-499e-a2bd-d4d310a1af29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PASO 7: FILTRAR FEATURES\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Dimensiones de embedding seleccionadas: 24/100\n",
      "\n",
      "ðŸ“Š Features finales:\n",
      "  - Embedding: 24 dims\n",
      "  - Otras: 2 dims\n",
      "  - TOTAL: 26 dims\n",
      "  - ReducciÃ³n: 75.2%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2303:>                                                       (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Features filtradasÂ ensambladas\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PASO 7: FILTRAR EMBEDDING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PASO 7: FILTRAR FEATURES\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "if len(selected_emb) > 0:\n",
    "    selected_emb_indices = sorted([int(var.split(\"_\")[1]) for var in selected_emb])\n",
    "    \n",
    "    print(f\"ðŸ“Š Dimensiones de embedding seleccionadas: {len(selected_emb_indices)}/100\")\n",
    "    \n",
    "    # UDF para filtrar embedding (eficiente)\n",
    "    from pyspark.ml.linalg import VectorUDT\n",
    "    \n",
    "    def filter_embedding_udf(indices):\n",
    "        def filter_func(vector):\n",
    "            if vector is None:\n",
    "                return Vectors.dense([0.0] * len(indices))\n",
    "            return Vectors.dense([float(vector[i]) for i in indices])\n",
    "        return F.udf(filter_func, VectorUDT())\n",
    "    \n",
    "    df_filtered = df_scaled.withColumn(\n",
    "        \"embedding_filtered\",\n",
    "        filter_embedding_udf(selected_emb_indices)(F.col(\"embedding_raw\"))\n",
    "    )\n",
    "    \n",
    "    embedding_dim = len(selected_emb_indices)\n",
    "    input_cols_filtered = [\"embedding_filtered\"] + selected_cat\n",
    "else:\n",
    "    print(\"âš   Ninguna dimensiÃ³n de embedding supera el umbral\")\n",
    "    df_filtered = df_scaled\n",
    "    embedding_dim = 0\n",
    "    input_cols_filtered = selected_cat\n",
    "\n",
    "print(f\"\\nðŸ“Š Features finales:\")\n",
    "print(f\"  - Embedding: {embedding_dim} dims\")\n",
    "print(f\"  - Otras: {len(selected_cat)} dims\")\n",
    "print(f\"  - TOTAL: {embedding_dim + len(selected_cat)} dims\")\n",
    "print(f\"  - ReducciÃ³n: {(1 - (embedding_dim + len(selected_cat))/feature_dim)*100:.1f}%\\n\")\n",
    "\n",
    "# Ensamblar features filtradas\n",
    "assembler_filtered = VectorAssembler(\n",
    "    inputCols=input_cols_filtered,\n",
    "    outputCol=\"features_selected\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "\n",
    "df_assembled_filtered = assembler_filtered.transform(df_filtered)\n",
    "selected_dim = len(df_assembled_filtered.select(\"features_selected\").first()[0])\n",
    "\n",
    "print(\"âœ“ Features filtradasÂ ensambladas\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "befbabd2-aa93-4576-942d-7f9bd494b80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PASO 8: NORMALIZAR FEATURES FILTRADAS\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2316:============================>                           (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Features filtradas normalizadas\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PASO 8: NORMALIZAR FEATURES FILTRADAS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 8: NORMALIZAR FEATURES FILTRADAS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "scaler_filtered = StandardScaler(\n",
    "    inputCol=\"features_selected\",\n",
    "    outputCol=\"features_scaled_filtered\",\n",
    "    withStd=True,\n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "scaler_model_filtered = scaler_filtered.fit(df_assembled_filtered)\n",
    "df_scaled_filtered = scaler_model_filtered.transform(df_assembled_filtered)\n",
    "\n",
    "print(\"âœ“ Features filtradas normalizadas\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "792c9b93-926e-48a1-8b33-5553c975d6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PASO 9: PCA SOBRE FEATURES FILTRADAS\n",
      "================================================================================\n",
      "\n",
      "â³ Entrenando PCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2335:============================>                           (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ PCA aplicado\n",
      "\n",
      "ðŸ“Š Componentes para 95% varianza: 23\n",
      "\n",
      "ðŸ“Š Varianza explicada:\n",
      "  - PC1: 13.02%\n",
      "  - PC2: 9.70%\n",
      "  - PC3: 6.45%\n",
      "  - PC4: 5.84%\n",
      "  - PC5: 5.48%\n",
      "  - PC6: 5.11%\n",
      "  - PC7: 4.53%\n",
      "  - PC8: 4.37%\n",
      "  - PC9: 4.10%\n",
      "  - PC10: 3.83%\n",
      "\n",
      "ðŸ“Š Varianza acumulada:\n",
      "  -   5 componentes: 40.49%\n",
      "  -  10 componentes: 62.44%\n",
      "  -  20 componentes: 90.21%\n",
      "  -  26 componentes: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PASO 9: PCA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 9: PCA SOBRE FEATURES FILTRADAS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "pca = PCA(\n",
    "    k=selected_dim,\n",
    "    inputCol=\"features_scaled_filtered\",\n",
    "    outputCol=\"features_pca\"\n",
    ")\n",
    "\n",
    "print(\"â³ Entrenando PCA...\")\n",
    "pca_model = pca.fit(df_scaled_filtered)\n",
    "df_pca = pca_model.transform(df_scaled_filtered)\n",
    "print(\"âœ“ PCA aplicado\\n\")\n",
    "\n",
    "# Analizar varianza\n",
    "explained_variance = pca_model.explainedVariance.toArray()\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "n_components_95 = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "\n",
    "print(f\"ðŸ“Š Componentes para 95% varianza: {n_components_95}\")\n",
    "\n",
    "print(\"\\nðŸ“Š Varianza explicada:\")\n",
    "for i in range(builtins.min(10, selected_dim)):\n",
    "    print(f\"  - PC{i+1}: {explained_variance[i]:.2%}\")\n",
    "\n",
    "print(\"\\nðŸ“Š Varianza acumulada:\")\n",
    "thresholds = [5, 10, 20, 30, 50, selected_dim]\n",
    "for i in thresholds:\n",
    "    if i <= len(cumulative_variance):\n",
    "        print(f\"  - {i:3d} componentes: {cumulative_variance[i-1]:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7823f64c-5e9c-45e5-a067-cf45a9c4b2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PASO 8: REDUCCIÃ“N DE DIMENSIONALIDAD CON PCA\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Calculando componentes principales...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2130:============================>                           (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ PCA aplicado\n",
      "\n",
      "ðŸ“Š Componentes necesarios para 95% varianza: 77\n",
      "\n",
      "ðŸ“Š Varianza explicada por componentes:\n",
      "  - PC1: 8.62%\n",
      "  - PC2: 6.30%\n",
      "  - PC3: 5.18%\n",
      "  - PC4: 4.61%\n",
      "  - PC5: 3.80%\n",
      "  - PC6: 3.33%\n",
      "  - PC7: 2.76%\n",
      "  - PC8: 2.58%\n",
      "  - PC9: 2.42%\n",
      "  - PC10: 2.39%\n",
      "\n",
      "ðŸ“Š Varianza acumulada:\n",
      "  -   5 componentes: 28.50%\n",
      "  -  10 componentes: 41.98%\n",
      "  -  20 componentes: 58.67%\n",
      "  -  30 componentes: 69.83%\n",
      "  -  50 componentes: 84.26%\n",
      "  -  60 componentes: 89.17%\n",
      "  -  77 componentes: 95.21%\n",
      "  - 105 componentes: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PASO 8: PCA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 8: REDUCCIÃ“N DE DIMENSIONALIDAD CON PCA\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# PCA con todos los componentes para anÃ¡lisis\n",
    "pca = PCA(\n",
    "    k=feature_dim,\n",
    "    inputCol=\"features_scaled\",\n",
    "    outputCol=\"features_pca\"\n",
    ")\n",
    "\n",
    "print(\"ðŸ“Š Calculando componentes principales...\")\n",
    "pca_model = pca.fit(df_scaled)\n",
    "df_pca = pca_model.transform(df_scaled)\n",
    "print(\"âœ“ PCA aplicado\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# ANALIZAR VARIANZA EXPLICADA\n",
    "# ============================================================================\n",
    "\n",
    "explained_variance = pca_model.explainedVariance.toArray()\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "# Encontrar componentes para 95% varianza\n",
    "n_components_95 = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "\n",
    "print(f\"ðŸ“Š Componentes necesarios para 95% varianza: {n_components_95}\")\n",
    "\n",
    "print(\"\\nðŸ“Š Varianza explicada por componentes:\")\n",
    "import builtins  # Usar min de Python, no de PySpark\n",
    "for i in range(builtins.min(10, feature_dim)):\n",
    "    print(f\"  - PC{i+1}: {explained_variance[i]:.2%}\")\n",
    "\n",
    "print(\"\\nðŸ“Š Varianza acumulada:\")\n",
    "thresholds = [5, 10, 20, 30, 50,60,77, feature_dim]\n",
    "for i in thresholds:\n",
    "    if i <= len(cumulative_variance):\n",
    "        print(f\"  - {i:3d} componentes: {cumulative_variance[i-1]:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6416b277-203f-4436-9ff1-76bdaae05851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PASO 9: PREPARAR DATASET FINAL\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2143:============================>                           (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset final preparado: 50,058 registros\n",
      "\n",
      "ðŸ“‹ Esquema del dataset final:\n",
      "root\n",
      " |-- id_contrato: string (nullable = true)\n",
      " |-- objeto_contrato: string (nullable = true)\n",
      " |-- entidad: string (nullable = false)\n",
      " |-- departamento: string (nullable = false)\n",
      " |-- region: string (nullable = true)\n",
      " |-- codigo_unspsc: string (nullable = false)\n",
      " |-- valor_contrato: double (nullable = true)\n",
      " |-- duracion_dias: integer (nullable = false)\n",
      " |-- fecha_firma: date (nullable = true)\n",
      " |-- features_pca: vector (nullable = true)\n",
      " |-- features_scaled: vector (nullable = true)\n",
      "\n",
      "\n",
      "ðŸ“Š EstadÃ­sticas del target (valor_contrato):\n",
      "+-------+--------------------+\n",
      "|summary|      valor_contrato|\n",
      "+-------+--------------------+\n",
      "|  count|               50058|\n",
      "|   mean|1.0030989674523553E8|\n",
      "| stddev|1.1680313663790998E9|\n",
      "|    min|                 1.0|\n",
      "|    max|    1.50838540149E11|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 9. PREPARAR DATASET FINAL PARA MODELADO\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PASO 9: PREPARAR DATASET FINAL\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Seleccionar columnas necesarias para el modelo\n",
    "df_final = df_pca.select(\n",
    "    \"id_contrato\",\n",
    "    \"objeto_contrato\",\n",
    "    \"entidad\",\n",
    "    \"departamento\",\n",
    "    \"region\",\n",
    "    \"codigo_unspsc\",\n",
    "    \"valor_contrato\",        # TARGET\n",
    "    \"duracion_dias\",\n",
    "    \"fecha_firma\",\n",
    "    \"features_pca\",          # FEATURES para el modelo\n",
    "    \"features_scaled\"        # Backup sin PCA\n",
    ")\n",
    "\n",
    "# Cachear para operaciones posteriores\n",
    "df_final = df_final.cache()\n",
    "total_final = df_final.count()\n",
    "\n",
    "print(f\"âœ… Dataset final preparado: {total_final:,} registros\")\n",
    "\n",
    "print(\"\\nðŸ“‹ Esquema del dataset final:\")\n",
    "df_final.printSchema()\n",
    "\n",
    "print(\"\\nðŸ“Š EstadÃ­sticas del target (valor_contrato):\")\n",
    "df_final.select(\"valor_contrato\").describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "07d0999f-dc3c-4818-a8f4-d1755df0a73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PASO 10: ANÃLISIS DE CORRELACIONES\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Calculando correlaciones de features PCA con valor_contrato...\n",
      "â³ Calculando matriz de correlaciÃ³n...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2153:============================>                           (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Correlaciones de PCA con valor_contrato:\n",
      "   - Componente mÃ¡s correlacionado: PC103 (nan)\n",
      "   - Top 5 componentes:\n",
      "     PC105: nan\n",
      "     PC103: nan\n",
      "     PC104: nan\n",
      "     PC24: -0.149\n",
      "     PC29: -0.120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 10. ANÃLISIS DE CORRELACIONES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PASO 10: ANÃLISIS DE CORRELACIONES\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"ðŸ“Š Calculando correlaciones de features PCA con valor_contrato...\")\n",
    "\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf as spark_udf, col\n",
    "\n",
    "# FunciÃ³n para agregar target al vector PCA\n",
    "def add_target_to_vector(features, target):\n",
    "    return Vectors.dense(list(features.toArray()) + [float(target)])\n",
    "\n",
    "add_target_udf = spark_udf(add_target_to_vector, VectorUDT())\n",
    "\n",
    "df_corr = df_final.withColumn(\n",
    "    \"features_with_target\",\n",
    "    add_target_udf(col(\"features_pca\"), col(\"valor_contrato\"))\n",
    ")\n",
    "\n",
    "# Calcular matriz de correlaciÃ³n\n",
    "print(\"â³ Calculando matriz de correlaciÃ³n...\")\n",
    "correlation_matrix = Correlation.corr(df_corr, \"features_with_target\", \"pearson\")\n",
    "\n",
    "# Extraer matriz como array numpy\n",
    "corr_array = correlation_matrix.collect()[0][0].toArray()\n",
    "\n",
    "# Correlaciones del target (Ãºltima fila, excepto Ãºltimo elemento)\n",
    "target_correlations = corr_array[-1, :-1]\n",
    "\n",
    "print(\"\\nðŸ“Š Correlaciones de PCA con valor_contrato:\")\n",
    "max_idx = np.argmax(np.abs(target_correlations))\n",
    "\n",
    "print(f\"   - Componente mÃ¡s correlacionado: PC{max_idx+1} ({target_correlations[max_idx]:.3f})\")\n",
    "print(f\"   - Top 5 componentes:\")\n",
    "\n",
    "top_5_indices = np.argsort(np.abs(target_correlations))[-5:][::-1]\n",
    "for idx in top_5_indices:\n",
    "    print(f\"     PC{idx+1}: {target_correlations[idx]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4965d4e-1661-4859-96a6-435bc5fa2a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 11. GUARDAR DATOS PROCESADOS (GOLD LAYER)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PASO 11: GUARDAR EN DELTA LAKE (GOLD LAYER)\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "GOLD_PATH = \"/app/notebooks/delta_lake/gold_features\"\n",
    "\n",
    "print(f\"ðŸ’¾ Guardando en: {GOLD_PATH}\")\n",
    "\n",
    "df_final.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .save(GOLD_PATH)\n",
    "\n",
    "print(\"âœ… Datos guardados exitosamente\")\n",
    "\n",
    "# Liberar cache\n",
    "df_silver.unpersist()\n",
    "df_final.unpersist()\n",
    "\n",
    "# ============================================================================\n",
    "# 12. GUARDAR MODELOS DE TRANSFORMACIÃ“N\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PASO 12: GUARDAR MODELOS DE TRANSFORMACIÃ“N\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "MODELS_PATH = \"/app/notebooks/models\"\n",
    "\n",
    "print(f\"ðŸ’¾ Guardando modelos en: {MODELS_PATH}\")\n",
    "\n",
    "# Guardar Word2Vec model\n",
    "word2vec_model.save(f\"{MODELS_PATH}/word2vec_model\")\n",
    "print(\"   âœ… Word2Vec model guardado\")\n",
    "\n",
    "# Guardar PCA model\n",
    "pca_model.save(f\"{MODELS_PATH}/pca_model\")\n",
    "print(\"   âœ… PCA model guardado\")\n",
    "\n",
    "# Guardar Scaler model\n",
    "scaler_model.save(f\"{MODELS_PATH}/scaler_model\")\n",
    "print(\"   âœ… StandardScaler model guardado\")\n",
    "\n",
    "# ============================================================================\n",
    "# 13. REPORTE FINAL\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… FASE 3 COMPLETADA - REPORTE FINAL\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"ðŸ“Š RESUMEN DEL PROCESAMIENTO:\\n\")\n",
    "print(f\"   âœ… Registros procesados: {total_final:,}\")\n",
    "print(f\"   âœ… Vocabulario Word2Vec: {vocab_size:,} palabras\")\n",
    "print(f\"   âœ… DimensiÃ³n embeddings: 100\")\n",
    "print(f\"   âœ… DimensiÃ³n features total: {feature_dim}\")\n",
    "print(f\"   âœ… DimensiÃ³n despuÃ©s de PCA: {k_components}\")\n",
    "print(f\"   âœ… Varianza explicada: {cumulative_variance[-1]:.2%}\")\n",
    "\n",
    "print(\"\\nðŸ“ ARCHIVOS GENERADOS:\")\n",
    "print(f\"   - Dataset: {GOLD_PATH}\")\n",
    "print(f\"   - Modelos: {MODELS_PATH}/\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ SIGUIENTE PASO:\")\n",
    "print(\"   Fase 4: Entrenar modelo de regresiÃ³n para predecir valor_contrato\")\n",
    "\n",
    "print(\"\\n\" +Â \"=\"*80Â +Â \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
