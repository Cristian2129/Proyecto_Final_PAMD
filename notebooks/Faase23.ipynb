{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a00643f-51d0-49ec-9aaa-0a6ac15581a1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Fase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28ef8e94-16fd-4be8-bf98-8cd5a51ae9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-c3c061e6-e049-4b5e-a5d8-7d2b37e59976;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.3 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "\tfound io.delta#delta-spark_2.12;3.0.0 in central\n",
      "\tfound io.delta#delta-storage;3.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 368ms :: artifacts dl 12ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tio.delta#delta-spark_2.12;3.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.0.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   14  |   0   |   0   |   0   ||   14  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-c3c061e6-e049-4b5e-a5d8-7d2b37e59976\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 14 already retrieved (0kB/7ms)\n",
      "25/12/11 14:13:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Spark 3.5.1 iniciado\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FASE 2 - OPTIMIZADO PARA SPARK 3.5.1 + DELTA LAKE 3.0\n",
    "# ============================================================================\n",
    "\n",
    "# PASO 0: REINICIAR SPARK CON VERSIONES CORRECTAS\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "import time\n",
    "time.sleep(3)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, lower, regexp_replace, translate, length, trim\n",
    "\n",
    "from pyspark.ml.feature import (\n",
    "    Tokenizer, StopWordsRemover, Word2Vec, \n",
    "    StringIndexer, OneHotEncoder, VectorAssembler,\n",
    "    StandardScaler, PCA\n",
    ")\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.stat import Correlation\n",
    "import numpy as np\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Bronze_to_Silver_Optimized\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1,\"\n",
    "            \"io.delta:delta-spark_2.12:3.0.0\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config(\"spark.executor.memory\", \"4g\")\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"50\")\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "print(f\" Spark {spark.version} iniciado\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fd17a2e-563a-48c1-8f15-3c3ec7900ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PASO 1: LECTURA DE KAFKA\n",
      "================================================================================\n",
      "\n",
      "Leyendo Kafka...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 52:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mensajes: 100,698\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 1. LECTURA DE KAFKA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 1: LECTURA DE KAFKA\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "contract_schema = StructType([\n",
    "    StructField(\"id_contrato\", StringType()),\n",
    "    StructField(\"objeto_contrato\", StringType()),\n",
    "    StructField(\"entidad\", StringType()),\n",
    "    StructField(\"departamento\", StringType()),\n",
    "    StructField(\"municipio\", StringType()),\n",
    "    StructField(\"region\", StringType()),\n",
    "    StructField(\"codigo_unspsc\", StringType()),\n",
    "    StructField(\"descripcion_categoria\", StringType()),\n",
    "    StructField(\"valor_contrato\", DoubleType()),\n",
    "    StructField(\"duracion_dias\", IntegerType()),\n",
    "    StructField(\"fecha_firma\", StringType()),\n",
    "    StructField(\"tipo_contrato\", StringType()),\n",
    "    StructField(\"estado_contrato\", StringType()),\n",
    "    StructField(\"modalidad\", StringType()),\n",
    "    StructField(\"anno\", IntegerType()),\n",
    "    StructField(\"id_interno_sistema\", StringType()),\n",
    "    StructField(\"campo_vacio\", StringType()),\n",
    "    StructField(\"constante_1\", StringType()),\n",
    "    StructField(\"constante_2\", IntegerType()),\n",
    "    StructField(\"duplicate_id\", StringType()),\n",
    "    StructField(\"timestamp_carga\", StringType())\n",
    "])\n",
    "\n",
    "print(\"Leyendo Kafka...\")\n",
    "\n",
    "df_kafka = spark.read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", \"contratos-publicos\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "df_bronze = df_kafka.select(\n",
    "    from_json(col(\"value\").cast(\"string\"), contract_schema).alias(\"data\")\n",
    ").select(\"data.*\")\n",
    "\n",
    "df_bronze = df_bronze.cache()\n",
    "total_kafka = df_bronze.count()\n",
    "\n",
    "print(f\" Mensajes: {total_kafka:,}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f952457-ec33-4c40-b285-f2d1e68c3357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PASO 2: ELIMINAR REDUNDANTES Y PREPARAR DATOS\n",
      "================================================================================\n",
      "\n",
      " Eliminando 6 columnas redundantes...\n",
      "Columnas restantes: 15\n",
      "\n",
      " Preparando campo fecha_firma...\n",
      "   Formato recibido: ISO timestamp (2024-01-04T00:00:00.000)\n",
      "   Convirtiendo a: date (2024-01-04)\n",
      " Fecha convertida correctamente\n",
      "\n",
      " Liberando memoria de df_bronze...\n",
      " Memoria liberada\n",
      "\n",
      "================================================================================\n",
      " Dataset preparado: 15 columnas\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 2. ELIMINAR REDUNDANTES Y PREPARAR DATOS\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 2: ELIMINAR REDUNDANTES Y PREPARAR DATOS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Columnas redundantes a eliminar\n",
    "redundant_columns = [\n",
    "    \"id_interno_sistema\",\n",
    "    \"campo_vacio\",\n",
    "    \"constante_1\",\n",
    "    \"constante_2\",\n",
    "    \"duplicate_id\",\n",
    "    \"timestamp_carga\"\n",
    "]\n",
    "\n",
    "print(f\" Eliminando {len(redundant_columns)} columnas redundantes...\")\n",
    "df_cleaned = df_bronze.drop(*redundant_columns)\n",
    "\n",
    "print(f\"Columnas restantes: {len(df_cleaned.columns)}\")\n",
    "print()\n",
    "\n",
    "print(\" Preparando campo fecha_firma...\")\n",
    "print(\"   Formato recibido: ISO timestamp (2024-01-04T00:00:00.000)\")\n",
    "print(\"   Convirtiendo a: date (2024-01-04)\")\n",
    "\n",
    "df_cleaned = (\n",
    "    df_cleaned\n",
    "    .withColumn(\"fecha_firma_temp\", to_timestamp(col(\"fecha_firma\")))\n",
    "    .withColumn(\"fecha_firma\", to_date(col(\"fecha_firma_temp\")))\n",
    "    .drop(\"fecha_firma_temp\")\n",
    ")\n",
    "\n",
    "print(\" Fecha convertida correctamente\\n\")\n",
    "\n",
    "# Liberar bronze ahora que ya no lo necesitamos\n",
    "print(\" Liberando memoria de df_bronze...\")\n",
    "df_bronze.unpersist()\n",
    "print(\" Memoria liberada\\n\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\" Dataset preparado: {len(df_cleaned.columns)} columnas\")\n",
    "print(\"=\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "816b1e7b-d82b-439c-acbf-057230ec288d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PASO 3: LIMPIEZA - PREPARACIÃ“N\n",
      "================================================================================\n",
      "\n",
      "Cacheando datos para anÃ¡lisis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 56:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Registros totales: 100,698\n",
      "\n",
      " Columnas: 15\n",
      " Datos cacheadosÂ enÂ memoria\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELDA 1: PREPARACIÃ“N Y CONTEO INICIAL\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 3: LIMPIEZA - PREPARACIÃ“N\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"Cacheando datos para anÃ¡lisis...\")\n",
    "df_cleaned = df_cleaned.cache()\n",
    "total_cleaned = df_cleaned.count()\n",
    "\n",
    "print(f\" Registros totales: {total_cleaned:,}\\n\")\n",
    "print(f\" Columnas: {len(df_cleaned.columns)}\")\n",
    "print(f\" Datos cacheadosÂ enÂ memoria\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82037c71-8bfc-4e99-b7fe-f3f24287fe73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ANÃLISIS DE CALIDAD DE DATOS\n",
      "================================================================================\n",
      "\n",
      "Analizando valores nulos en columnas crÃ­ticas...\n",
      "ðŸ“Š Valores nulos en columnas crÃ­ticas:\n",
      "\n",
      "   âš   fecha_firma: 695 (0.7%)\n",
      "   âš   duracion_dias: 50,350 (50.0%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELDA 2: ANÃLISIS DE NULOS (OPTIMIZADO)\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"ANÃLISIS DE CALIDAD DE DATOS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"Analizando valores nulos en columnas crÃ­ticas...\")\n",
    "\n",
    "# Solo analizar columnas crÃ­ticas para ahorrar memoria\n",
    "critical_columns = [\n",
    "    \"id_contrato\",\n",
    "    \"objeto_contrato\", \n",
    "    \"valor_contrato\",\n",
    "    \"fecha_firma\",\n",
    "    \"entidad\",\n",
    "    \"departamento\",\n",
    "    \"duracion_dias\"\n",
    "]\n",
    "\n",
    "# AnÃ¡lisis optimizado solo de columnas crÃ­ticas\n",
    "null_analysis = df_cleaned.select([\n",
    "    count(when(col(c).isNull(), c)).alias(c) \n",
    "    for c in critical_columns if c in df_cleaned.columns\n",
    "]).collect()[0].asDict()\n",
    "\n",
    "print(\"ðŸ“Š Valores nulos en columnas crÃ­ticas:\\n\")\n",
    "has_nulls = False\n",
    "for col_name in critical_columns:\n",
    "    if col_name in null_analysis:\n",
    "        null_count = null_analysis[col_name]\n",
    "        if null_count > 0:\n",
    "            has_nulls = True\n",
    "            pct = (null_count / total_cleaned) * 100\n",
    "            print(f\"   âš   {col_name}: {null_count:,} ({pct:.1f}%)\")\n",
    "\n",
    "if not has_nulls:\n",
    "    print(\"   âœ… No hay valores nulos en columnas crÃ­ticas\")\n",
    "\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af77614e-c5ff-4b86-91e2-23ce1d5a253d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "APLICANDO FILTROS DE CALIDAD\n",
      "================================================================================\n",
      "\n",
      "Aplicando reglas de limpieza:\n",
      "  âœ“ id_contrato no nulo\n",
      "  âœ“ objeto_contrato no nulo\n",
      "  âœ“ valor_contrato no nulo y > 0\n",
      "  âœ“ fecha_firma no nula\n",
      "\n",
      "âœ… Filtros aplicados correctamente\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELDA 3: APLICAR FILTROS DE LIMPIEZA\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"APLICANDO FILTROS DE CALIDAD\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"Aplicando reglas de limpieza:\")\n",
    "print(\"  âœ“ id_contrato no nulo\")\n",
    "print(\"  âœ“ objeto_contrato no nulo\")\n",
    "print(\"  âœ“ valor_contrato no nulo y > 0\")\n",
    "print(\"  âœ“ fecha_firma no nula\")\n",
    "print()\n",
    "\n",
    "# Aplicar filtros paso a paso\n",
    "# NOTA: fecha_firma ya fue convertida a date en el Paso 2\n",
    "df_silver = df_cleaned \\\n",
    "    .filter(col(\"id_contrato\").isNotNull()) \\\n",
    "    .filter(col(\"objeto_contrato\").isNotNull()) \\\n",
    "    .filter(col(\"valor_contrato\").isNotNull()) \\\n",
    "    .filter(col(\"valor_contrato\") > 0) \\\n",
    "    .filter(col(\"fecha_firma\").isNotNull())\n",
    "\n",
    "print(\"âœ… Filtros aplicados correctamente\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b60ee146-c2f4-4f23-ae53-f98ded14fb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.1\n"
     ]
    }
   ],
   "source": [
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "244fd81e-3b09-4646-99b4-d88f61d8abd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FINALIZANDO LIMPIEZA\n",
      "================================================================================\n",
      "\n",
      "Cacheando datos limpios...\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š RESUMEN DE LIMPIEZA\n",
      "================================================================================\n",
      "  Registros iniciales:    100,698\n",
      "  Registros finales:      99,458 (98.8%)\n",
      "  Registros descartados:  1,240 (1.2%)\n",
      "================================================================================\n",
      "\n",
      "Liberando memoria del cache anterior...\n",
      "âœ… LimpiezaÂ completada\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELDA 4: CACHEAR RESULTADOS Y GENERAR REPORTE\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"FINALIZANDO LIMPIEZA\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"Cacheando datos limpios...\")\n",
    "df_silver = df_silver.cache()\n",
    "total_silver = df_silver.count()\n",
    "\n",
    "# Calcular estadÃ­sticas\n",
    "registros_descartados = total_cleaned - total_silver\n",
    "pct_retenido = (total_silver / total_cleaned) * 100 if total_cleaned > 0 else 0\n",
    "pct_descartado = (registros_descartados / total_cleaned) * 100 if total_cleaned > 0 else 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“Š RESUMEN DE LIMPIEZA\")\n",
    "print(\"=\"*80)\n",
    "print(f\"  Registros iniciales:    {total_cleaned:,}\")\n",
    "print(f\"  Registros finales:      {total_silver:,} ({pct_retenido:.1f}%)\")\n",
    "print(f\"  Registros descartados:  {registros_descartados:,} ({pct_descartado:.1f}%)\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Liberar memoria del DataFrame anterior\n",
    "print(\"Liberando memoria del cache anterior...\")\n",
    "df_cleaned.unpersist()\n",
    "print(\"âœ… LimpiezaÂ completada\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b4f2fef-6d09-419f-baae-b612a0c8eef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 entidades:\n",
      "+-------------------------------------------------+-----+\n",
      "|entidad                                          |count|\n",
      "+-------------------------------------------------+-----+\n",
      "|MUNICIPIO DE SOACHA.                             |6356 |\n",
      "|ALCALDÃA MUNICIPAL COTA                          |3988 |\n",
      "|ESE MUNICIPAL DE SOACHA JULIO CESAR PEÃ‘ALOZA*    |3822 |\n",
      "|CUNDINAMARCA-ALCALDIA MUNICIPIO MOSQUERA         |3759 |\n",
      "|empresa social del estado regiÃ³n de salud soacha.|3152 |\n",
      "+-------------------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Top 5 departamentos:\n",
      "+------------+-----+\n",
      "|departamento|count|\n",
      "+------------+-----+\n",
      "|Cundinamarca|99458|\n",
      "+------------+-----+\n",
      "\n",
      "\n",
      "DistribuciÃ³n por regiÃ³n:\n",
      "+--------------+-----+\n",
      "|region        |count|\n",
      "+--------------+-----+\n",
      "|Centro-Oriente|99458|\n",
      "+--------------+-----+\n",
      "\n",
      "\n",
      "Top 10 cÃ³digos UNSPSC:\n",
      "+-------------+-----+\n",
      "|codigo_unspsc|count|\n",
      "+-------------+-----+\n",
      "|             |50058|\n",
      "|V1.80111600  |11391|\n",
      "|V1.80111701  |4329 |\n",
      "|V1.85101600  |2605 |\n",
      "|V1.80111620  |2158 |\n",
      "|V1.80111601  |2092 |\n",
      "|V1.86101710  |1134 |\n",
      "|UNSPECIFIED  |1081 |\n",
      "|V1.80161500  |891  |\n",
      "|V1.80111504  |866  |\n",
      "+-------------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "Top 10 categorÃ­as UNSPSC:\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|descripcion_categoria                                                                                                                                                                                                                                                                                       |count|\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|                                                                                                                                                                                                                                                                                                            |50058|\n",
      "|PRESTACIÃ“N DE SERVICIOS DE APOYO A LA GESTIÃ“N PARA REALIZAR ACTIVIDADES DE ATENCIÃ“N PRIMARIA EN SALUD EN EL MUNICIPIO DE SOACHA EN EL MARCO DE LAS NORMAS VIGENTES; MEDIANTE LA IMPLEMENTACIÃ“N DE EQUIPOS BÃSICOS DE SALUD EN EL CONTEXTO DE LAS ACCIONES DE PROMOCIÃ“N MANTENIMIENTO DE LA SALUD Y LA ATENCI|318  |\n",
      "|PRESTACIÃ“N DE SERVICIOS PROFESIONALES PARA REALIZAR ACTIVIDADES DE ATENCIÃ“N PRIMARIA EN SALUD EN EL MUNICIPIO DE SOACHA EN EL MARCO DE LAS NORMAS VIGENTES; MEDIANTE LA IMPLEMENTACIÃ“N DE EQUIPOS BÃSICOS DE SALUD EN EL CONTEXTO DE LAS ACCIONES DE PROMOCIÃ“N MANTENIMIENTO DE LA SALUD Y LA ATENCIÃ“N INDIV|248  |\n",
      "|Contratar la prestaciÃ³n de los servicios profesionales de manera autÃ³noma e independiente para impartir formaciÃ³n profesional integral en las competencias tÃ©cnicas; claves y transversales a travÃ©s de herramientas pedagÃ³gicas en el programa y modalidad asignada garantizando el cumplimiento del proces|173  |\n",
      "|PRESTACIÃ“N DE SERVICIOS TÃ‰CNICO ASISTENCIAL PARA REALIZAR ACTIVIDADES DE ATENCIÃ“N PRIMARIA EN SALUD EN EL MUNICIPIO DE SOACHA EN EL MARCO DE LAS NORMAS VIGENTES; MEDIANTE LA IMPLEMENTACIÃ“N DE EQUIPOS BÃSICOS EN SALUD EN EL CONTEXTO DE LAS ACCIONES DE PROMOCIÃ“N Y MANTENIMIENTO DE LA SALUD Y LA ATENCI|120  |\n",
      "|Prestar servicios personales de carÃ¡cter temporal para impartir formaciÃ³n profesional integral; de forma presencial y/o virtual; asÃ­ como otras actividades que se deriven de los diferentes programas; niveles y especialidades impartidas por el centro de tecnologÃ­as del transporte del Sena - Regional |92   |\n",
      "|PrestaciÃ³n de servicios profesionales; tecnÃ³logo y/o tÃ©cnico para el desarrollo de las actividades de formaciÃ³n del nivel tÃ©cnico; tecnolÃ³gico y/o complementarÃ­a del programa Regular; atendiendo las polÃ­ticas institucionales y la normatividad vigente; de acuerdo con las necesidades; la programaciÃ³n |87   |\n",
      "|AUNAR ESFUERZOS ENTRE EL DEPARTAMENTO DE CUNDINAMARCA - SECRETARÃA DE EDUCACIÃ“N Y LOS MUNICIPIOS FOCALIZADOS PARA LA ESTRATEGIA DE TRANSPORTE ESCOLAR; DIRIGIDO A ESTUDIANTES DE LAS IED OFICIALES; ASEGURANDO LAS CONDICIONES NECESARIAS PARA SU PERMANENCIA EN EL   SISTEMA EDUCATIVO (SEGÃšN ANEXO DISTRIB|72   |\n",
      "|PRESTACIÃ“N DE SERVICIOS COMO AUXILIAR DE ENFERMERIA EN EL PROCESO DE EQUIPOS BASICOS EN SALUD DE LA SUBGERENCIA COMUNITARIA DE LA EMPRESA SOCIAL DEL ESTADO REGION DE SALUD SOACHA DE ACUERDO AL REQUERIMIENTO INSTITUCIONAL EN DESARROLLO DE LA RESOLUCION 1032 DE 2024 EMITIDA POR EL MINISTERIO DE SALUD |72   |\n",
      "|PRESTACIÃ“N DE SERVICIOS COMO AUXILIAR DE APOYO ADMINISTRATIVO A LA GESTIÃ“N Y SEGUIMIENTO DE LAS ACTIVIDADES DE FACTURACIÃ“N EN EL ÃREA ADMINISTRATIVA DENTRO DE LOS DIFERENTES PROCESOS Y PROCEDIMIENTOS DE LA EMPRESA SOCIAL DEL ESTADO REGIÃ“N DE SALUD SOACHA DE ACUERDO AL REQUERIMIENTO INSTITUCIONAL.   |66   |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "DistribuciÃ³n por tipo de contrato:\n",
      "+------------------------------+-----+\n",
      "|tipo_contrato                 |count|\n",
      "+------------------------------+-----+\n",
      "|PrestaciÃ³n de servicios       |81169|\n",
      "|Decreto 092 de 2017           |7814 |\n",
      "|Suministros                   |3413 |\n",
      "|Compraventa                   |2352 |\n",
      "|Otro                          |2319 |\n",
      "|Arrendamiento de inmuebles    |749  |\n",
      "|Obra                          |738  |\n",
      "|Seguros                       |475  |\n",
      "|ConsultorÃ­a                   |186  |\n",
      "|InterventorÃ­a                 |104  |\n",
      "|Arrendamiento de muebles      |56   |\n",
      "|Operaciones de CrÃ©dito PÃºblico|23   |\n",
      "|No Especificado               |20   |\n",
      "|Servicios financieros         |19   |\n",
      "|Comodato                      |9    |\n",
      "|Venta muebles                 |6    |\n",
      "|AsociaciÃ³n PÃºblico Privada    |4    |\n",
      "|ConcesiÃ³n                     |2    |\n",
      "+------------------------------+-----+\n",
      "\n",
      "\n",
      "DistribuciÃ³n del estado del contrato:\n",
      "+---------------+-----+\n",
      "|estado_contrato|count|\n",
      "+---------------+-----+\n",
      "|En ejecuciÃ³n   |37005|\n",
      "|Cerrado        |27760|\n",
      "|Modificado     |21922|\n",
      "|terminado      |11707|\n",
      "|Aprobado       |688  |\n",
      "|cedido         |278  |\n",
      "|Suspendido     |98   |\n",
      "+---------------+-----+\n",
      "\n",
      "\n",
      "Top 10 modalidades de contrataciÃ³n:\n",
      "+-------------------------------------------+-----+\n",
      "|modalidad                                  |count|\n",
      "+-------------------------------------------+-----+\n",
      "|ContrataciÃ³n directa                       |69957|\n",
      "|ContrataciÃ³n rÃ©gimen especial              |17569|\n",
      "|MÃ­nima cuantÃ­a                             |6772 |\n",
      "|SelecciÃ³n Abreviada de Menor CuantÃ­a       |1607 |\n",
      "|SelecciÃ³n abreviada subasta inversa        |964  |\n",
      "|ContrataciÃ³n rÃ©gimen especial (con ofertas)|832  |\n",
      "|ContrataciÃ³n Directa (con ofertas)         |745  |\n",
      "|LicitaciÃ³n pÃºblica                         |724  |\n",
      "|Concurso de mÃ©ritos abierto                |186  |\n",
      "|LicitaciÃ³n pÃºblica Obra Publica            |58   |\n",
      "+-------------------------------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Entidades\n",
    "print(\"Top 5 entidades:\")\n",
    "df_silver.groupBy(\"entidad\").count().orderBy(desc(\"count\")).show(5, truncate=False)\n",
    "\n",
    "# 2. Departamentos\n",
    "print(\"\\nTop 5 departamentos:\")\n",
    "df_silver.groupBy(\"departamento\").count().orderBy(desc(\"count\")).show(5, truncate=False)\n",
    "\n",
    "# 3. RegiÃ³n\n",
    "print(\"\\nDistribuciÃ³n por regiÃ³n:\")\n",
    "df_silver.groupBy(\"region\").count().orderBy(desc(\"count\")).show(truncate=False)\n",
    "\n",
    "# 4. CÃ³digo UNSPSC\n",
    "print(\"\\nTop 10 cÃ³digos UNSPSC:\")\n",
    "df_silver.groupBy(\"codigo_unspsc\").count().orderBy(desc(\"count\")).show(10, truncate=False)\n",
    "\n",
    "# 5. CategorÃ­a UNSPSC\n",
    "print(\"\\nTop 10 categorÃ­as UNSPSC:\")\n",
    "df_silver.groupBy(\"descripcion_categoria\").count().orderBy(desc(\"count\")).show(10, truncate=False)\n",
    "\n",
    "# 6. Tipo de contrato\n",
    "print(\"\\nDistribuciÃ³n por tipo de contrato:\")\n",
    "df_silver.groupBy(\"tipo_contrato\").count().orderBy(desc(\"count\")).show(truncate=False)\n",
    "\n",
    "# 7. Estado del contrato\n",
    "print(\"\\nDistribuciÃ³n del estado del contrato:\")\n",
    "df_silver.groupBy(\"estado_contrato\").count().orderBy(desc(\"count\")).show(truncate=False)\n",
    "\n",
    "# 8. Modalidad de contrataciÃ³n\n",
    "print(\"\\nTop 10 modalidades de contrataciÃ³n:\")\n",
    "df_silver.groupBy(\"modalidad\").count().orderBy(desc(\"count\")).show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0131185-211a-4265-9356-7b567e112143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EstadÃ­sticas de valor_contrato:\n",
      "+---+----------------+-------------------+--------------------+\n",
      "|min|             max|               mean|                 std|\n",
      "+---+----------------+-------------------+--------------------+\n",
      "|1.0|1.50838540149E11|9.941466321590018E7|1.1521186504414532E9|\n",
      "+---+----------------+-------------------+--------------------+\n",
      "\n",
      "\n",
      "Percentiles de valor_contrato:\n",
      "\n",
      "EstadÃ­sticas de duracion_dias:\n",
      "+---+----+-----------------+------------------+\n",
      "|min| max|             mean|               std|\n",
      "+---+----+-----------------+------------------+\n",
      "|  0|4297|82.47422012591348|101.20091534465666|\n",
      "+---+----+-----------------+------------------+\n",
      "\n",
      "\n",
      "Percentiles de duracion_dias:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0, 6.0, 40.0, 125.0, 4297.0]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max, avg, stddev, expr\n",
    "\n",
    "# 10. Valor del contrato\n",
    "print(\"\\nEstadÃ­sticas de valor_contrato:\")\n",
    "df_silver.select(\n",
    "    min(\"valor_contrato\").alias(\"min\"),\n",
    "    max(\"valor_contrato\").alias(\"max\"),\n",
    "    avg(\"valor_contrato\").alias(\"mean\"),\n",
    "    stddev(\"valor_contrato\").alias(\"std\")\n",
    ").show()\n",
    "\n",
    "# Percentiles\n",
    "print(\"\\nPercentiles de valor_contrato:\")\n",
    "df_silver.approxQuantile(\"valor_contrato\", [0.01, 0.25, 0.5, 0.75, 0.99], 0.01)\n",
    "\n",
    "# 11. DuraciÃ³n en dÃ­as\n",
    "print(\"\\nEstadÃ­sticas de duracion_dias:\")\n",
    "df_silver.select(\n",
    "    min(\"duracion_dias\").alias(\"min\"),\n",
    "    max(\"duracion_dias\").alias(\"max\"),\n",
    "    avg(\"duracion_dias\").alias(\"mean\"),\n",
    "    stddev(\"duracion_dias\").alias(\"std\")\n",
    ").show()\n",
    "\n",
    "print(\"\\nPercentiles de duracion_dias:\")\n",
    "df_silver.approxQuantile(\"duracion_dias\", [0.01, 0.25, 0.5, 0.75, 0.99], 0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be7ceb3f-a49f-454d-9040-eadd4a513895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top aÃ±os:\n",
      "+----+-----+\n",
      "|anno|count|\n",
      "+----+-----+\n",
      "|2025|68   |\n",
      "|2024|95797|\n",
      "|2023|3029 |\n",
      "|2022|564  |\n",
      "+----+-----+\n",
      "\n",
      "\n",
      "Contratos por aÃ±o:\n",
      "+----+-----+\n",
      "|anno|count|\n",
      "+----+-----+\n",
      "|2024|95797|\n",
      "|2023|3029 |\n",
      "|2022|564  |\n",
      "|2025|68   |\n",
      "+----+-----+\n",
      "\n",
      "\n",
      "Top fechas de firma:\n",
      "+-----------+-----+\n",
      "|fecha_firma|count|\n",
      "+-----------+-----+\n",
      "|2024-02-01 |1230 |\n",
      "|2024-03-01 |1119 |\n",
      "|2024-02-02 |860  |\n",
      "|2024-02-05 |815  |\n",
      "|2024-03-22 |794  |\n",
      "|2024-02-09 |789  |\n",
      "|2024-02-16 |744  |\n",
      "|2024-02-06 |723  |\n",
      "|2024-02-12 |715  |\n",
      "|2024-09-02 |686  |\n",
      "+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTop aÃ±os:\")\n",
    "df_silver.groupBy(\"anno\").count().orderBy(desc(\"anno\")).show(10, truncate=False)\n",
    "\n",
    "print(\"\\nContratos por aÃ±o:\")\n",
    "df_silver.groupBy(\"anno\").count().orderBy(desc(\"count\")).show(10, truncate=False)\n",
    "\n",
    "print(\"\\nTop fechas de firma:\")\n",
    "df_silver.groupBy(\"fecha_firma\").count().orderBy(desc(\"count\")).show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9095b5d1-039d-4c44-81ad-731451dc1ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PASO 5: GUARDAR EN DELTA LAKE\n",
      "================================================================================\n",
      "\n",
      "ðŸ’¾ Guardando en: /app/notebooks/delta_lake/silver_contracts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Guardado exitosamente\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 5. GUARDAR EN DELTA LAKE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 5: GUARDAR EN DELTA LAKE\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "DELTA_PATH = \"/app/notebooks/delta_lake/silver_contracts\"\n",
    "\n",
    "print(f\"ðŸ’¾ Guardando en: {DELTA_PATH}\")\n",
    "\n",
    "df_silver.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .save(DELTA_PATH)\n",
    "\n",
    "print(\"âœ… Guardado exitosamente\\n\")\n",
    "\n",
    "# âš ï¸ LIBERAR todo\n",
    "df_silver.unpersist()\n",
    "spark.catalog.clearCache()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fa9b8d-dcf7-4962-8755-3993314ee402",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Fase 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f353f8e1-7564-40cc-ae3f-bc39a496b980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PASO 1: CARGAR DATOS DESDE SILVER\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Cargando: /app/notebooks/delta_lake/silver_contracts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 122:==========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Registros: 99,458\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PASO 1: CARGAR DATOS DESDE SILVER\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "SILVER_PATH = \"/app/notebooks/delta_lake/silver_contracts\"\n",
    "print(f\"ðŸ“Š Cargando: {SILVER_PATH}\")\n",
    "\n",
    "df_silver = spark.read.format(\"delta\").load(SILVER_PATH)\n",
    "df_silver = df_silver.cache()\n",
    "total_records = df_silver.count()\n",
    "\n",
    "print(f\"âœ“ Registros: {total_records:,}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "18f418b5-05f7-4432-a3e9-ca4bc34e5141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PASO 2: LIMPIEZA DE TEXTO\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 126:==========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Registros despuÃ©s de limpieza: 99,458\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PASO 2: LIMPIEZA DE TEXTO\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 2: LIMPIEZA DE TEXTO\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "src_chars = \"Ã¡Ã©Ã­Ã³ÃºÃ¼Ã±\"\n",
    "dst_chars = \"aeiouun\"\n",
    "\n",
    "df_prepared = df_silver.withColumn(\n",
    "    \"objeto_limpio\",\n",
    "    trim(\n",
    "        regexp_replace(\n",
    "            regexp_replace(\n",
    "                translate(\n",
    "                    lower(col(\"objeto_contrato\")),\n",
    "                    src_chars,\n",
    "                    dst_chars\n",
    "                ),\n",
    "                \"[^a-z0-9\\\\s]\", \" \"\n",
    "            ),\n",
    "            \"\\\\s+\", \" \"\n",
    "        )\n",
    "    )\n",
    ").filter(length(col(\"objeto_limpio\")) >= 10)\n",
    "\n",
    "print(f\"âœ“ Registros despuÃ©s de limpieza: {df_prepared.count():,}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bb8b89d1-bb12-449c-b504-a243d1b8e33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PASO 3: TOKENIZACIÃ“N\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 129:==========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Registros despuÃ©s de filtrado: 99,458\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PASO 3: TOKENIZACIÃ“N Y STOPWORDS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 3: TOKENIZACIÃ“N\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "stopwords_es = [\n",
    "    \"el\", \"la\", \"de\", \"que\", \"y\", \"a\", \"en\", \"un\", \"ser\", \"se\", \"no\",\n",
    "    \"por\", \"con\", \"su\", \"para\", \"como\", \"estar\", \"tener\", \"le\", \"lo\",\n",
    "    \"pero\", \"hacer\", \"o\", \"este\", \"otro\", \"ese\", \"si\", \"ya\", \"ver\",\n",
    "    \"dar\", \"muy\", \"sin\", \"sobre\", \"tambiÃ©n\", \"hasta\", \"aÃ±o\", \"entre\",\n",
    "    \"del\", \"al\", \"los\", \"las\", \"uno\", \"una\", \"unos\", \"unas\",\n",
    "    \"contrato\", \"contratos\", \"objeto\", \"prestacion\", \"prestaciÃ³n\",\n",
    "    \"servicio\", \"servicios\", \"suministro\", \"ejecucion\", \"ejecuciÃ³n\"\n",
    "]\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"objeto_limpio\", outputCol=\"palabras\")\n",
    "df_tokenized = tokenizer.transform(df_prepared)\n",
    "\n",
    "remover = StopWordsRemover(\n",
    "    inputCol=\"palabras\",\n",
    "    outputCol=\"palabras_sin_stopwords\",\n",
    "    stopWords=stopwords_es\n",
    ")\n",
    "df_filtered_words = remover.transform(df_tokenized)\n",
    "\n",
    "# Filtrar palabras cortas\n",
    "def clean_words(words):\n",
    "    if not words:\n",
    "        return []\n",
    "    return [w for w in words if len(w) >= 3]\n",
    "\n",
    "clean_udf = udf(clean_words, ArrayType(StringType()))\n",
    "\n",
    "df_filtered = df_filtered_words.withColumn(\n",
    "    \"palabras_filtradas\",\n",
    "    clean_udf(col(\"palabras_sin_stopwords\"))\n",
    ").filter(size(col(\"palabras_filtradas\")) > 0)\n",
    "\n",
    "print(f\"âœ“ Registros despuÃ©s de filtrado: {df_filtered.count():,}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2bc2043b-4721-4da7-a242-b0e4bf879ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PASO 4: WORD2VEC\n",
      "================================================================================\n",
      "\n",
      "â³ Entrenando Word2Vec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Vocabulario: 14,465 palabras\n",
      "âœ“ EmbeddingsÂ generados\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PASO 4: WORD2VEC\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 4: WORD2VEC\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "word2vec = Word2Vec(\n",
    "    vectorSize=100,\n",
    "    minCount=2,\n",
    "    maxIter=10,\n",
    "    seed=42,\n",
    "    inputCol=\"palabras_filtradas\",\n",
    "    outputCol=\"embedding_raw\"\n",
    ")\n",
    "\n",
    "print(\"â³ Entrenando Word2Vec...\")\n",
    "word2vec_model = word2vec.fit(df_filtered)\n",
    "df_embeddings = word2vec_model.transform(df_filtered)\n",
    "\n",
    "vocab_size = len(word2vec_model.getVectors().collect())\n",
    "print(f\"âœ“ Vocabulario: {vocab_size:,} palabras\")\n",
    "print(f\"âœ“ EmbeddingsÂ generados\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cbde6876-943f-4de2-8206-dd5afc874ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PASO 5: TARGET ENCODING\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Codificando entidad...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â†’ entidad_te creado\n",
      "ðŸ“Š Codificando codigo_unspsc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â†’ codigo_unspsc_te creado\n",
      "ðŸ“Š Codificando tipo_contrato...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â†’ tipo_contrato_te creado\n",
      "ðŸ“Š Codificando estado_contrato...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â†’ estado_contrato_te creado\n",
      "ðŸ“Š Codificando modalidad...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 176:==========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â†’ modalidad_te creado\n",
      "\n",
      "âœ“ Target EncodingÂ completado\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PASO 5: TARGET ENCODING (SOBRE df_embeddings)\n",
    "# ============================================================================\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 5: TARGET ENCODING\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "def target_encode_smooth(df, cat_col, target_col, m=50):\n",
    "    \"\"\"Target Encoding suavizado\"\"\"\n",
    "    global_mean = df.agg(F.mean(target_col)).first()[0]\n",
    "    \n",
    "    stats = (\n",
    "        df.groupBy(cat_col)\n",
    "        .agg(\n",
    "            F.mean(target_col).alias(\"cat_mean\"),\n",
    "            F.count(target_col).alias(\"cat_count\")\n",
    "        )\n",
    "        .withColumn(\n",
    "            f\"{cat_col}_te\",\n",
    "            (F.col(\"cat_count\") * F.col(\"cat_mean\") + m * F.lit(global_mean))\n",
    "            / (F.col(\"cat_count\") + m)\n",
    "        )\n",
    "        .select(cat_col, f\"{cat_col}_te\")\n",
    "    )\n",
    "    \n",
    "    return df.join(stats, on=cat_col, how=\"left\")\n",
    "\n",
    "# IMPORTANTE: Aplicar sobre df_embeddings (que tiene embedding_raw)\n",
    "df_te = df_embeddings\n",
    "\n",
    "categorical_cols = [\"entidad\", \"codigo_unspsc\", \"tipo_contrato\", \"estado_contrato\", \"modalidad\"]\n",
    "\n",
    "for col_name in categorical_cols:\n",
    "    print(f\"ðŸ“Š Codificando {col_name}...\")\n",
    "    df_te = target_encode_smooth(df_te, col_name, target_col=\"valor_contrato\", m=50)\n",
    "    print(f\"   â†’ {col_name}_te creado\")\n",
    "\n",
    "print(\"\\nâœ“ Target EncodingÂ completado\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bd6d8a14-d303-454a-ae88-91f3c95a00b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "VERIFICACIÃ“N: Varianza de variables Target Encoded\n",
      "================================================================================\n",
      "\n",
      "Verificando columnas con Target Encoding (_te):\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entidad_te:\n",
      "  Count:    99458\n",
      "  Min:      14406872.390959457\n",
      "  Max:      1105320127.1465836\n",
      "  Variance: 9594105497661858.0\n",
      "  Std:      97949504.83622599\n",
      "  CONSERVAR (tiene varianza)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "codigo_unspsc_te:\n",
      "  Count:    99458\n",
      "  Min:      17057530.61269573\n",
      "  Max:      4494832645.981172\n",
      "  Variance: 1.0081318874149584e+16\n",
      "  Std:      100405771.11973985\n",
      "  CONSERVAR (tiene varianza)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tipo_contrato_te:\n",
      "  Count:    99458\n",
      "  Min:      60305043.46804067\n",
      "  Max:      6341257341.517739\n",
      "  Variance: 2.160509100008716e+16\n",
      "  Std:      146986703.48057732\n",
      "  CONSERVAR (tiene varianza)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estado_contrato_te:\n",
      "  Count:    99458\n",
      "  Min:      28069428.3107082\n",
      "  Max:      2146138087.3567228\n",
      "  Variance: 1.1824545100141602e+16\n",
      "  Std:      108740724.20276408\n",
      "  CONSERVAR (tiene varianza)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 238:==========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modalidad_te:\n",
      "  Count:    99458\n",
      "  Min:      31966280.59774187\n",
      "  Max:      3863781158.8434043\n",
      "  Variance: 1.201264388683272e+17\n",
      "  Std:      346592612.25295496\n",
      "  CONSERVAR (tiene varianza)\n",
      "\n",
      "Resultado: 5/5 variables vÃ¡lidas\n",
      "\n",
      "Variables vÃ¡lidas: ['entidad_te', 'codigo_unspsc_te', 'tipo_contrato_te', 'estado_contrato_te', 'modalidad_te']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# VERIFICAR VARIANZA DE VARIABLES TARGET ENCODED\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"VERIFICACIÃ“N: Varianza de variables Target Encoded\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Columnas numÃ©ricas generadas por Target Encoding\n",
    "te_columns = [\"entidad_te\", \"codigo_unspsc_te\", \"tipo_contrato_te\", \"estado_contrato_te\", \"modalidad_te\"]\n",
    "valid_te_columns = []\n",
    "\n",
    "print(\"Verificando columnas con Target Encoding (_te):\\n\")\n",
    "\n",
    "for col in te_columns:\n",
    "    # Verificar existencia\n",
    "    if col not in df_te.columns:\n",
    "        print(f\"{col}:\")\n",
    "        print(\"  COLUMNA NO EXISTE (Target Encoding fallÃ³)\\n\")\n",
    "        continue\n",
    "    \n",
    "    # Calcular estadÃ­sticas\n",
    "    stats = (\n",
    "        df_te\n",
    "        .select(\n",
    "            F.variance(col).alias(\"variance\"),\n",
    "            F.stddev(col).alias(\"std\"),\n",
    "            F.min(col).alias(\"min\"),\n",
    "            F.max(col).alias(\"max\"),\n",
    "            F.count(col).alias(\"count\")\n",
    "        )\n",
    "        .first()\n",
    "    )\n",
    "    \n",
    "    variance = stats[\"variance\"]\n",
    "    std = stats[\"std\"]\n",
    "    min_val = stats[\"min\"]\n",
    "    max_val = stats[\"max\"]\n",
    "    count = stats[\"count\"]\n",
    "    \n",
    "    print(f\"{col}:\")\n",
    "    print(f\"  Count:    {count}\")\n",
    "    print(f\"  Min:      {min_val if min_val is not None else 'None'}\")\n",
    "    print(f\"  Max:      {max_val if max_val is not None else 'None'}\")\n",
    "    print(f\"  Variance: {variance if variance is not None else 'None'}\")\n",
    "    print(f\"  Std:      {std if std is not None else 'None'}\")\n",
    "    \n",
    "    # Criterio de validez\n",
    "    if variance is None or variance == 0 or std is None or std == 0:\n",
    "        print(\"  ELIMINAR (sin varianza)\\n\")\n",
    "    else:\n",
    "        print(\"  CONSERVAR (tiene varianza)\\n\")\n",
    "        valid_te_columns.append(col)\n",
    "\n",
    "\n",
    "print(f\"Resultado: {len(valid_te_columns)}/{len(te_columns)} variables vÃ¡lidas\\n\")\n",
    "\n",
    "# Si ninguna variable target-encoded tiene varianza\n",
    "if len(valid_te_columns) == 0:\n",
    "    print(\"ADVERTENCIA: Ninguna variable tiene varianza.\")\n",
    "    print(\"Posibles causas:\")\n",
    "    print(\"1. Target Encoding fallÃ³ (todas las categorÃ­as tienen el mismo promedio)\")\n",
    "    print(\"2. Solo hay una categorÃ­a Ãºnica en la variable\")\n",
    "    print(\"3. Los datos estÃ¡n muy balanceados o homogÃ©neos\\n\")\n",
    "    \n",
    "    print(\"Mostrando muestra de datos para diagnÃ³stico:\")\n",
    "    df_te.select(\n",
    "        \"entidad\", \"entidad_te\",\n",
    "        \"modalidad\", \"modalidad_te\",\n",
    "        \"valor_contrato\"\n",
    "    ).show(10, truncate=False)\n",
    "    \n",
    "    print(\"\\nValores Ãºnicos por variable original:\")\n",
    "    for col_orig in [\"entidad\", \"codigo_unspsc\", \"departamento\", \"modalidad\"]:\n",
    "        unique_count = df_te.select(col_orig).distinct().count()\n",
    "        print(f\"- {col_orig}: {unique_count} valores Ãºnicos\\n\")\n",
    "\n",
    "else:\n",
    "    print(f\"Variables vÃ¡lidas: {valid_te_columns}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "944a1233-5f79-4b1a-8c52-e22518d060f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PASO 6: ENSAMBLAR FEATURES\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… duracion_dias incluida\n",
      "\n",
      "ðŸ“Š Features a ensamblar:\n",
      "  âœ“ embedding_raw\n",
      "  âœ“ entidad_te\n",
      "  âœ“ codigo_unspsc_te\n",
      "  âœ“ tipo_contrato_te\n",
      "  âœ“ estado_contrato_te\n",
      "  âœ“ modalidad_te\n",
      "  âœ“ duracion_dias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ DimensiÃ³n: 106 dims\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PASO 6: ENSAMBLAR FEATURES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 6: ENSAMBLAR FEATURES\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Imputar duracion_dias\n",
    "df_te = df_te.fillna({\"duracion_dias\": 0})\n",
    "\n",
    "# Verificar duracion_dias\n",
    "duracion_variance = df_te.select(F.variance(\"duracion_dias\")).first()[0]\n",
    "if duracion_variance and duracion_variance > 0:\n",
    "    valid_te_columns.append(\"duracion_dias\")\n",
    "    print(\"âœ… duracion_dias incluida\\n\")\n",
    "\n",
    "input_cols = [\"embedding_raw\"] + valid_te_columns\n",
    "\n",
    "print(\"ðŸ“Š Features a ensamblar:\")\n",
    "for col in input_cols:\n",
    "    print(f\"  âœ“ {col}\")\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=input_cols,\n",
    "    outputCol=\"features_raw\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "\n",
    "df_assembled = assembler.transform(df_te)\n",
    "feature_dim = len(df_assembled.select(\"features_raw\").first()[0])\n",
    "\n",
    "print(f\"\\nâœ“ DimensiÃ³n: {feature_dim} dims\\n\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5961ad0a-2858-4338-9c71-86d3657b1abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PASO 7: NORMALIZACIÃ“N\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 276:==========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Features normalizadas\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PASO 7: NORMALIZAR\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 7: NORMALIZACIÃ“N\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features_raw\",\n",
    "    outputCol=\"features_scaled\",\n",
    "    withStd=True,\n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "scaler_model = scaler.fit(df_assembled)\n",
    "df_scaled = scaler_model.transform(df_assembled)\n",
    "\n",
    "print(\"âœ“ Features normalizadas\\n\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1db2f9cf-6bb2-431d-a24d-5fd896d54833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PASO 8: ANÃLISIS DE CORRELACIÃ“N\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š CorrelaciÃ³n de variables categÃ³ricas:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  entidad_te                â†’  0.0990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  codigo_unspsc_te          â†’  0.3300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  tipo_contrato_te          â†’  0.2462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  estado_contrato_te        â†’  0.1114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  modalidad_te              â†’  0.3278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  duracion_dias             â†’  0.0268\n",
      "\n",
      "ðŸ“Š CorrelaciÃ³n de embeddings (muestra 10%):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Muestra: 10,059 registros\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Procesadas 25/100 dimensiones...\n",
      "  Procesadas 50/100 dimensiones...\n",
      "  Procesadas 75/100 dimensiones...\n",
      "  Procesadas 100/100 dimensiones...\n",
      "\n",
      "âœ“ Correlaciones calculadas\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PASO 8: ANÃLISIS DE CORRELACIÃ“N\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 8: ANÃLISIS DE CORRELACIÃ“N\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# 8.1: CorrelaciÃ³n de variables categÃ³ricas\n",
    "print(\"ðŸ“Š CorrelaciÃ³n de variables categÃ³ricas:\")\n",
    "\n",
    "cat_correlations = {}\n",
    "\n",
    "for var in valid_te_columns:\n",
    "    try:\n",
    "        assembler_pair = VectorAssembler(\n",
    "            inputCols=[var, \"valor_contrato\"],\n",
    "            outputCol=\"features_pair\",\n",
    "            handleInvalid=\"skip\"\n",
    "        )\n",
    "        \n",
    "        df_pair = assembler_pair.transform(df_scaled)\n",
    "        corr_matrix = Correlation.corr(df_pair, \"features_pair\", \"pearson\").collect()[0][0]\n",
    "        corr_value = corr_matrix.toArray()[0, 1]\n",
    "        \n",
    "        cat_correlations[var] = corr_value\n",
    "        print(f\"  {var:<25} â†’ {corr_value:>7.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  {var:<25} â†’ ERROR\")\n",
    "\n",
    "print()\n",
    "\n",
    "# 8.2: CorrelaciÃ³n de embeddings (muestra)\n",
    "print(\"ðŸ“Š CorrelaciÃ³n de embeddings (muestra 10%):\")\n",
    "\n",
    "SAMPLE_FRACTION = 0.1\n",
    "df_sample = df_scaled.sample(withReplacement=False, fraction=SAMPLE_FRACTION, seed=42)\n",
    "sample_size = df_sample.count()\n",
    "print(f\"  Muestra: {sample_size:,} registros\\n\")\n",
    "\n",
    "data_sample = df_sample.select(\"embedding_raw\", \"valor_contrato\").collect()\n",
    "embeddings_array = np.array([row[\"embedding_raw\"].toArray() for row in data_sample])\n",
    "target_array = np.array([row[\"valor_contrato\"] for row in data_sample])\n",
    "\n",
    "embedding_correlations = {}\n",
    "\n",
    "for i in range(100):\n",
    "    correlation = np.corrcoef(embeddings_array[:, i], target_array)[0, 1]\n",
    "    embedding_correlations[f\"emb_{i}\"] = correlation\n",
    "    \n",
    "    if (i + 1) % 25 == 0:\n",
    "        print(f\"  Procesadas {i + 1}/100 dimensiones...\")\n",
    "\n",
    "print(\"\\nâœ“ Correlaciones calculadas\\n\")\n",
    "\n",
    "# Consolidar\n",
    "all_correlations = {**cat_correlations, **embedding_correlations}\n",
    "valid_correlations = {k: v for k, v in all_correlations.items() if not np.isnan(v)}\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7ede34fc-65ab-4ec1-8064-0df60e778119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PASO 9: SELECCIÃ“N DE VARIABLES\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Umbral: |r| >= 0.05\n",
      "  âœ… Seleccionadas: 19\n",
      "  âŒ Rechazadas: 87\n",
      "\n",
      "  CategÃ³ricas: 5\n",
      "  Embeddings: 14\n",
      "\n",
      "ðŸ“Š Top 10 variables:\n",
      "   1. codigo_unspsc_te          â†’  0.3300\n",
      "   2. modalidad_te              â†’  0.3278\n",
      "   3. tipo_contrato_te          â†’  0.2462\n",
      "   4. estado_contrato_te        â†’  0.1114\n",
      "   5. entidad_te                â†’  0.0990\n",
      "   6. emb_37                    â†’  0.0734\n",
      "   7. emb_68                    â†’  0.0721\n",
      "   8. emb_73                    â†’  0.0690\n",
      "   9. emb_92                    â†’  0.0681\n",
      "  10. emb_75                    â†’  0.0678\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PASO 9: SELECCIÃ“N DE VARIABLES\n",
    "# ============================================================================\n",
    "import builtins\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 9: SELECCIÃ“N DE VARIABLES\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "THRESHOLD = 0.05\n",
    "\n",
    "selected_vars = {var: corr for var, corr in valid_correlations.items() \n",
    "                 if builtins.abs(corr) >= THRESHOLD}\n",
    "\n",
    "print(f\"ðŸ“Š Umbral: |r| >= {THRESHOLD}\")\n",
    "print(f\"  âœ… Seleccionadas: {len(selected_vars)}\")\n",
    "print(f\"  âŒ Rechazadas: {len(valid_correlations) - len(selected_vars)}\")\n",
    "\n",
    "selected_cat = [v for v in selected_vars.keys() if not v.startswith(\"emb_\")]\n",
    "selected_emb = [v for v in selected_vars.keys() if v.startswith(\"emb_\")]\n",
    "\n",
    "print(f\"\\n  CategÃ³ricas: {len(selected_cat)}\")\n",
    "print(f\"  Embeddings: {len(selected_emb)}\")\n",
    "\n",
    "print(\"\\nðŸ“Š Top 10 variables:\")\n",
    "sorted_vars = sorted(selected_vars.items(), key=lambda x: builtins.abs(x[1]), reverse=True)\n",
    "for i, (var, corr) in enumerate(sorted_vars[:10], 1):\n",
    "    print(f\"  {i:2d}. {var:<25} â†’ {corr:>7.4f}\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bc243579-820c-4873-a597-564580a79207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PASO 10: FILTRAR FEATURES\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Features seleccionadas:\n",
      "  - Embedding: 14 dims\n",
      "  - CategÃ³ricas: 5 dims\n",
      "  - TOTAL: 19 dims\n",
      "  - ReducciÃ³n: 82.1%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 472:======================================>                  (2 + 1) / 3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Features filtradas ensambladas\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PASO 10: FILTRAR EMBEDDING\n",
    "# ============================================================================\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PASO 10: FILTRAR FEATURES\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "if len(selected_emb) > 0:\n",
    "    selected_emb_indices = sorted([int(var.split(\"_\")[1]) for var in selected_emb])\n",
    "    \n",
    "    def filter_embedding_udf(indices):\n",
    "        def filter_func(vector):\n",
    "            if vector is None:\n",
    "                return Vectors.dense([0.0] * len(indices))\n",
    "            return Vectors.dense([float(vector[i]) for i in indices])\n",
    "        return F.udf(filter_func, VectorUDT())\n",
    "    \n",
    "    df_filtered = df_scaled.withColumn(\n",
    "        \"embedding_filtered\",\n",
    "        filter_embedding_udf(selected_emb_indices)(F.col(\"embedding_raw\"))\n",
    "    )\n",
    "    \n",
    "    embedding_dim = len(selected_emb_indices)\n",
    "    input_cols_filtered = [\"embedding_filtered\"] + selected_cat\n",
    "else:\n",
    "    df_filtered = df_scaled\n",
    "    embedding_dim = 0\n",
    "    input_cols_filtered = selected_cat\n",
    "\n",
    "total_selected = embedding_dim + len(selected_cat)\n",
    "\n",
    "print(f\"ðŸ“Š Features seleccionadas:\")\n",
    "print(f\"  - Embedding: {embedding_dim} dims\")\n",
    "print(f\"  - CategÃ³ricas: {len(selected_cat)} dims\")\n",
    "print(f\"  - TOTAL: {total_selected} dims\")\n",
    "print(f\"  - ReducciÃ³n: {(1 - total_selected/feature_dim)*100:.1f}%\\n\")\n",
    "\n",
    "assembler_filtered = VectorAssembler(\n",
    "    inputCols=input_cols_filtered,\n",
    "    outputCol=\"features_selected\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "\n",
    "df_assembled_filtered = assembler_filtered.transform(df_filtered)\n",
    "selected_dim = len(df_assembled_filtered.select(\"features_selected\").first()[0])\n",
    "\n",
    "print(\"âœ“ Features filtradas ensambladas\\n\")\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1c26c545-de11-499e-a2bd-d4d310a1af29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PASO 11: NORMALIZAR FEATURES FILTRADAS\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Normalizadas\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PASO 11: NORMALIZAR FILTRADAS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 11: NORMALIZAR FEATURES FILTRADAS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "scaler_filtered = StandardScaler(\n",
    "    inputCol=\"features_selected\",\n",
    "    outputCol=\"features_scaled_filtered\",\n",
    "    withStd=True,\n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "scaler_model_filtered = scaler_filtered.fit(df_assembled_filtered)\n",
    "df_scaled_filtered = scaler_model_filtered.transform(df_assembled_filtered)\n",
    "\n",
    "print(\"âœ“ Normalizadas\\n\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "792c9b93-926e-48a1-8b33-5553c975d6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PASO 9: PCA SOBRE FEATURES FILTRADAS\n",
      "================================================================================\n",
      "\n",
      "â³ Entrenando PCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2335:============================>                           (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ PCA aplicado\n",
      "\n",
      "ðŸ“Š Componentes para 95% varianza: 23\n",
      "\n",
      "ðŸ“Š Varianza explicada:\n",
      "  - PC1: 13.02%\n",
      "  - PC2: 9.70%\n",
      "  - PC3: 6.45%\n",
      "  - PC4: 5.84%\n",
      "  - PC5: 5.48%\n",
      "  - PC6: 5.11%\n",
      "  - PC7: 4.53%\n",
      "  - PC8: 4.37%\n",
      "  - PC9: 4.10%\n",
      "  - PC10: 3.83%\n",
      "\n",
      "ðŸ“Š Varianza acumulada:\n",
      "  -   5 componentes: 40.49%\n",
      "  -  10 componentes: 62.44%\n",
      "  -  20 componentes: 90.21%\n",
      "  -  26 componentes: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PASO 9: PCA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 9: PCA SOBRE FEATURES FILTRADAS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "pca = PCA(\n",
    "    k=selected_dim,\n",
    "    inputCol=\"features_scaled_filtered\",\n",
    "    outputCol=\"features_pca\"\n",
    ")\n",
    "\n",
    "print(\"â³ Entrenando PCA...\")\n",
    "pca_model = pca.fit(df_scaled_filtered)\n",
    "df_pca = pca_model.transform(df_scaled_filtered)\n",
    "print(\"âœ“ PCA aplicado\\n\")\n",
    "\n",
    "# Analizar varianza\n",
    "explained_variance = pca_model.explainedVariance.toArray()\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "n_components_95 = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "\n",
    "print(f\"ðŸ“Š Componentes para 95% varianza: {n_components_95}\")\n",
    "\n",
    "print(\"\\nðŸ“Š Varianza explicada:\")\n",
    "for i in range(builtins.min(10, selected_dim)):\n",
    "    print(f\"  - PC{i+1}: {explained_variance[i]:.2%}\")\n",
    "\n",
    "print(\"\\nðŸ“Š Varianza acumulada:\")\n",
    "thresholds = [5, 10, 20, 30, 50, selected_dim]\n",
    "for i in thresholds:\n",
    "    if i <= len(cumulative_variance):\n",
    "        print(f\"  - {i:3d} componentes: {cumulative_variance[i-1]:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7823f64c-5e9c-45e5-a067-cf45a9c4b2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PASO 12: PCA\n",
      "================================================================================\n",
      "\n",
      "â³ Entrenando PCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ PCA aplicado\n",
      "\n",
      "ðŸ“Š Componentes para 95% varianza: 18\n",
      "ðŸ“Š PC1 varianza: 12.50%\n",
      "ðŸ“Š Top 10 varianza: 68.46%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PASO 12: PCA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 12: PCA\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "pca = PCA(\n",
    "    k=selected_dim,\n",
    "    inputCol=\"features_scaled_filtered\",\n",
    "    outputCol=\"features_pca\"\n",
    ")\n",
    "\n",
    "print(\"â³ Entrenando PCA...\")\n",
    "pca_model = pca.fit(df_scaled_filtered)\n",
    "df_pca = pca_model.transform(df_scaled_filtered)\n",
    "\n",
    "explained_variance = pca_model.explainedVariance.toArray()\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "n_components_95 = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "\n",
    "print(f\"âœ“ PCA aplicado\\n\")\n",
    "print(f\"ðŸ“Š Componentes para 95% varianza: {n_components_95}\")\n",
    "print(f\"ðŸ“Š PC1 varianza: {explained_variance[0]:.2%}\")\n",
    "print(f\"ðŸ“Š Top 10 varianza: {cumulative_variance[9]:.2%}\\n\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6416b277-203f-4436-9ff1-76bdaae05851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PASO 13: DATASET FINAL\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 529:==========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Dataset final: 99,458 registros\n",
      "\n",
      "ðŸ“Š Opciones de features:\n",
      "  1. features_pca: 18 dims\n",
      "  2. features_scaled_filtered: 19 dims\n",
      "  3. features_scaled: 106 dims\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PASO 13: DATASET FINAL\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 13: DATASET FINAL\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "df_final = df_pca.select(\n",
    "    \"id_contrato\",\n",
    "    \"objeto_contrato\",\n",
    "    \"entidad\",\n",
    "    \"departamento\",\n",
    "    \"region\",\n",
    "    \"codigo_unspsc\",\n",
    "    \"valor_contrato\",\n",
    "    \"duracion_dias\",\n",
    "    \"fecha_firma\",\n",
    "    \"features_pca\",\n",
    "    \"features_scaled_filtered\",\n",
    "    \"features_scaled\"\n",
    ")\n",
    "\n",
    "df_final = df_final.cache()\n",
    "total_final = df_final.count()\n",
    "\n",
    "print(f\"âœ“ Dataset final: {total_final:,} registros\\n\")\n",
    "print(\"ðŸ“Š Opciones de features:\")\n",
    "print(f\"  1. features_pca: {n_components_95} dims\")\n",
    "print(f\"  2. features_scaled_filtered: {selected_dim} dims\")\n",
    "print(f\"  3. features_scaled: {feature_dim} dims\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "07d0999f-dc3c-4818-a8f4-d1755df0a73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PASO 10: ANÃLISIS DE CORRELACIONES\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Calculando correlaciones de features PCA con valor_contrato...\n",
      "â³ Calculando matriz de correlaciÃ³n...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2153:============================>                           (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Correlaciones de PCA con valor_contrato:\n",
      "   - Componente mÃ¡s correlacionado: PC103 (nan)\n",
      "   - Top 5 componentes:\n",
      "     PC105: nan\n",
      "     PC103: nan\n",
      "     PC104: nan\n",
      "     PC24: -0.149\n",
      "     PC29: -0.120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 10. ANÃLISIS DE CORRELACIONES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PASO 10: ANÃLISIS DE CORRELACIONES\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"ðŸ“Š Calculando correlaciones de features PCA con valor_contrato...\")\n",
    "\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf as spark_udf, col\n",
    "\n",
    "# FunciÃ³n para agregar target al vector PCA\n",
    "def add_target_to_vector(features, target):\n",
    "    return Vectors.dense(list(features.toArray()) + [float(target)])\n",
    "\n",
    "add_target_udf = spark_udf(add_target_to_vector, VectorUDT())\n",
    "\n",
    "df_corr = df_final.withColumn(\n",
    "    \"features_with_target\",\n",
    "    add_target_udf(col(\"features_pca\"), col(\"valor_contrato\"))\n",
    ")\n",
    "\n",
    "# Calcular matriz de correlaciÃ³n\n",
    "print(\"â³ Calculando matriz de correlaciÃ³n...\")\n",
    "correlation_matrix = Correlation.corr(df_corr, \"features_with_target\", \"pearson\")\n",
    "\n",
    "# Extraer matriz como array numpy\n",
    "corr_array = correlation_matrix.collect()[0][0].toArray()\n",
    "\n",
    "# Correlaciones del target (Ãºltima fila, excepto Ãºltimo elemento)\n",
    "target_correlations = corr_array[-1, :-1]\n",
    "\n",
    "print(\"\\nðŸ“Š Correlaciones de PCA con valor_contrato:\")\n",
    "max_idx = np.argmax(np.abs(target_correlations))\n",
    "\n",
    "print(f\"   - Componente mÃ¡s correlacionado: PC{max_idx+1} ({target_correlations[max_idx]:.3f})\")\n",
    "print(f\"   - Top 5 componentes:\")\n",
    "\n",
    "top_5_indices = np.argsort(np.abs(target_correlations))[-5:][::-1]\n",
    "for idx in top_5_indices:\n",
    "    print(f\"     PC{idx+1}: {target_correlations[idx]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c4965d4e-1661-4859-96a6-435bc5fa2a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GUARDANDO\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Dataset: /app/notebooks/delta_lake/gold_features_v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Modelos: /app/notebooks/models_v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… FASE 3 COMPLETADA\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PASO 14: GUARDAR\n",
    "# ============================================================================\n",
    "\n",
    "GOLD_PATH = \"/app/notebooks/delta_lake/gold_features_v2\"\n",
    "MODELS_PATH = \"/app/notebooks/models_v2\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"GUARDANDO\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(f\"ðŸ“Š Dataset: {GOLD_PATH}\")\n",
    "df_final.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(GOLD_PATH)\n",
    "\n",
    "print(f\"ðŸ“Š Modelos: {MODELS_PATH}\")\n",
    "word2vec_model.save(f\"{MODELS_PATH}/word2vec_model\")\n",
    "pca_model.save(f\"{MODELS_PATH}/pca_model\")\n",
    "scaler_model.save(f\"{MODELS_PATH}/scaler_model\")\n",
    "scaler_model_filtered.save(f\"{MODELS_PATH}/scaler_filtered_model\")\n",
    "\n",
    "print(\"\\nâœ… FASE 3 COMPLETADA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81211544-f4a9-481c-87a2-b1b3a4b98958",
   "metadata": {},
   "source": [
    "## Fase 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d89ed5d-9cda-4234-9cef-3322c5bfe2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PASO 1: CARGA DE DATOS\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Cargando: /app/notebooks/delta_lake/gold_features_v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Registros cargados: 99,458\n",
      "âœ“ Columnas: 12\n",
      "\n",
      "ðŸ“Š Features disponibles:\n",
      "  1. features_pca: 19 dims\n",
      "  2. features_scaled_filtered: 19 dims\n",
      "  3. features_scaled: 106 dims\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PASO 1: CARGAR DATOS DESDE GOLD\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 1: CARGA DE DATOS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "GOLD_PATH = \"/app/notebooks/delta_lake/gold_features_v2\"\n",
    "\n",
    "print(f\"ðŸ“Š Cargando: {GOLD_PATH}\")\n",
    "df_gold = spark.read.format(\"delta\").load(GOLD_PATH)\n",
    "\n",
    "df_gold = df_gold.cache()\n",
    "total_records = df_gold.count()\n",
    "\n",
    "print(f\"âœ“ Registros cargados: {total_records:,}\")\n",
    "print(f\"âœ“ Columnas: {len(df_gold.columns)}\\n\")\n",
    "\n",
    "print(\"ðŸ“Š Features disponibles:\")\n",
    "feature_cols = [c for c in df_gold.columns if \"features\" in c]\n",
    "\n",
    "for i, c in enumerate(feature_cols, 1):\n",
    "    sample_dim = len(df_gold.select(c).first()[0])\n",
    "    print(f\"  {i}. {c}: {sample_dim} dims\")\n",
    "\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb6d7eca-ebcc-4e37-8dfe-bae7e5850657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PASO 2: PREPARACIÃ“N DE DATOS\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Features seleccionadas: features_scaled\n",
      "ðŸ“Š Target: valor_contrato\n",
      "\n",
      "âœ“ Registros vÃ¡lidos: 99,458\n",
      "âœ“ Descartados: 0\n",
      "\n",
      "ðŸ“Š EstadÃ­sticas de valor_contrato:\n",
      "  Min:  $1.00\n",
      "  Max:  $150,838,540,149.00\n",
      "  Mean: $99,414,663.22\n",
      "  Std:  $1,152,118,650.44\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PASO 2: SELECCIÃ“N DE FEATURES Y TARGET\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 2: PREPARACIÃ“N DE DATOS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "FEATURE_COL = \"features_scaled\"\n",
    "TARGET_COL = \"valor_contrato\"\n",
    "\n",
    "print(f\"ðŸ“Š Features seleccionadas: {FEATURE_COL}\")\n",
    "print(f\"ðŸ“Š Target: {TARGET_COL}\\n\")\n",
    "\n",
    "df_model = df_gold.select(\n",
    "    col(FEATURE_COL).alias(\"features\"),\n",
    "    col(TARGET_COL).alias(\"label\"),\n",
    "    \"id_contrato\",\n",
    "    \"fecha_firma\",\n",
    "    \"entidad\"\n",
    ").filter(\n",
    "    col(\"features\").isNotNull() &\n",
    "    col(\"label\").isNotNull() &\n",
    "    (col(\"label\") > 0)\n",
    ")\n",
    "\n",
    "df_model = df_model.cache()\n",
    "total_model = df_model.count()\n",
    "\n",
    "print(f\"âœ“ Registros vÃ¡lidos: {total_model:,}\")\n",
    "print(f\"âœ“ Descartados: {total_records - total_model:,}\\n\")\n",
    "\n",
    "stats = df_model.select(\n",
    "    min(\"label\").alias(\"min\"),\n",
    "    max(\"label\").alias(\"max\"),\n",
    "    avg(\"label\").alias(\"mean\"),\n",
    "    stddev(\"label\").alias(\"std\")\n",
    ").first()\n",
    "\n",
    "print(\"ðŸ“Š EstadÃ­sticas de valor_contrato:\")\n",
    "print(f\"  Min:  ${stats['min']:,.2f}\")\n",
    "print(f\"  Max:  ${stats['max']:,.2f}\")\n",
    "print(f\"  Mean: ${stats['mean']:,.2f}\")\n",
    "print(f\"  Std:  ${stats['std']:,.2f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86e24322-a3fd-4171-b2ae-7c069a62e034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PASO 3: DIVISIÃ“N TRAIN/TEST (CORREGIDO PARA FECHAS)\n",
      "================================================================================\n",
      "\n",
      "ðŸ“… Fecha de corte (percentil 80): 2024-09-27 00:00:00\n",
      "\n",
      "Train: 79,298\n",
      "Test : 20,160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PASO 3: TRAIN/TEST SPLIT\n",
    "# ============================================================================\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 3: DIVISIÃ“N TRAIN/TEST (CORREGIDO PARA FECHAS)\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Convertimos la fecha a nÃºmero (timestamp largo)\n",
    "df_temp = df_model.withColumn(\n",
    "    \"fecha_num\",\n",
    "    col(\"fecha_firma\").cast(\"timestamp\").cast(\"long\")\n",
    ")\n",
    "\n",
    "# Obtenemos el percentil 80\n",
    "q = df_temp.approxQuantile(\"fecha_num\", [0.8], 0.01)\n",
    "split_ts = q[0]\n",
    "\n",
    "# Convertimos el nÃºmero a fecha\n",
    "split_date = datetime.utcfromtimestamp(split_ts)\n",
    "\n",
    "print(f\"ðŸ“… Fecha de corte (percentil 80): {split_date}\\n\")\n",
    "\n",
    "# Dividimos el dataset usando la fecha original\n",
    "train_data = df_model.filter(col(\"fecha_firma\") <= split_date).cache()\n",
    "test_data  = df_model.filter(col(\"fecha_firma\") > split_date).cache()\n",
    "\n",
    "print(f\"Train: {train_data.count():,}\")\n",
    "print(f\"Test : {test_data.count():,}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01c10bf4-32e6-4fed-ae19-23f56f58c816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PASO 4: MODELO BASELINE - REGRESIÃ“N LINEAL\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š RESULTADOS - LINEAR REGRESSION:\n",
      "  RMSE:  $1,535,092,233.12\n",
      "  MAE:   $257,007,310.67\n",
      "  RÂ²:    0.1446\n",
      "  Sigma: $1,535,117,975.89\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PASO 4: REGRESIÃ“N LINEAL\n",
    "# ============================================================================\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col, stddev\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 4: MODELO BASELINE - REGRESIÃ“N LINEAL\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "lr = LinearRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    maxIter=100,\n",
    "    regParam=0.1,\n",
    "    elasticNetParam=0.8\n",
    ")\n",
    "\n",
    "lr_model = lr.fit(train_data)\n",
    "lr_predictions = lr_model.transform(test_data)\n",
    "\n",
    "# Evaluadores\n",
    "evaluator_rmse = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "evaluator_r2   = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "evaluator_mae  = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "\n",
    "lr_rmse = evaluator_rmse.evaluate(lr_predictions)\n",
    "lr_r2   = evaluator_r2.evaluate(lr_predictions)\n",
    "lr_mae  = evaluator_mae.evaluate(lr_predictions)\n",
    "\n",
    "# ===========================\n",
    "# CÃ¡lculo de sigma (Ïƒ)\n",
    "# ===========================\n",
    "\n",
    "lr_with_error = lr_predictions.withColumn(\n",
    "    \"error\", col(\"prediction\") - col(\"label\")\n",
    ")\n",
    "\n",
    "sigma = lr_with_error.select(stddev(\"error\").alias(\"sigma\")).first()[\"sigma\"]\n",
    "\n",
    "print(\"ðŸ“Š RESULTADOS - LINEAR REGRESSION:\")\n",
    "print(f\"  RMSE:  ${lr_rmse:,.2f}\")\n",
    "print(f\"  MAE:   ${lr_mae:,.2f}\")\n",
    "print(f\"  RÂ²:    {lr_r2:.4f}\")\n",
    "print(f\"  Sigma: ${sigma:,.2f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "216023ac-4227-46c8-8777-09c2df8ff6f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PASO 1: CARGA DE DATOS\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Cargando: /app/notebooks/delta_lake/gold_features_v2\n",
      "âœ“ Registros cargados: 99,458\n",
      "âœ“ Columnas: 12\n",
      "\n",
      "ðŸ“Š Features disponibles:\n",
      "  1. features_pca: 19 dims\n",
      "  2. features_scaled_filtered: 19 dims\n",
      "  3. features_scaled: 106 dims\n",
      "\n",
      "================================================================================\n",
      "PASO 2: PREPARACIÃ“N DE DATOS\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Features seleccionadas: features_pca\n",
      "ðŸ“Š Target: valor_contrato\n",
      "\n",
      "âœ“ Registros vÃ¡lidos: 99,458\n",
      "âœ“ Descartados: 0\n",
      "\n",
      "ðŸ“Š EstadÃ­sticas de valor_contrato:\n",
      "  Min:  $1.00\n",
      "  Max:  $150,838,540,149.00\n",
      "  Mean: $99,414,663.22\n",
      "  Std:  $1,152,118,650.44\n",
      "\n",
      "================================================================================\n",
      "PASO 3: DIVISIÃ“N TRAIN/TEST (CORREGIDO PARA FECHAS)\n",
      "================================================================================\n",
      "\n",
      "ðŸ“… Fecha de corte (percentil 80): 2024-09-27 00:00:00\n",
      "\n",
      "Train: 79,298\n",
      "Test : 20,160\n",
      "\n",
      "================================================================================\n",
      "PASO 4: MODELO BASELINE - REGRESIÃ“N LINEAL\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/11 00:49:38 INFO mlflow.tracking.fluent: Experiment with name 'contract_value_regression' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š RESULTADOS - LINEAR REGRESSION:\n",
      "  RMSE:  $1,537,548,264.71\n",
      "  MAE:   $244,431,490.36\n",
      "  RÂ²:    0.1419\n",
      "  Sigma: $1,537,484,096.27\n",
      "\n",
      "================================================================================\n",
      "PASO 9: GUARDAR MODELO CON MLFLOW\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Registrando parÃ¡metros...\n",
      "âœ“ ParÃ¡metros registrados\n",
      "\n",
      "ðŸ“Š Registrando mÃ©tricas...\n",
      "âœ“ MÃ©tricas registradas\n",
      "\n",
      "ðŸ“Š Guardando modelo en MLflow...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'contract_value_predictor'.\n",
      "2025/12/11 00:50:01 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: contract_value_predictor, version 1\n",
      "Created version '1' of model 'contract_value_predictor'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Modelo registrado\n",
      "\n",
      "ðŸ“Š MLflow Run ID: 793a87e9ca594961b0ca30aff71eb8cc\n",
      "\n",
      "ðŸƒ View run linear_regression_v1 at: http://172.17.0.1:5000/#/experiments/2/runs/793a87e9ca594961b0ca30aff71eb8cc\n",
      "ðŸ§ª View experiment at: http://172.17.0.1:5000/#/experiments/2\n",
      "âœ… Modelo guardado exitosamente en MLflow\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS\n",
    "# ============================================================================\n",
    "from pyspark.sql.functions import col, min, max, avg, stddev\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from datetime import datetime\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "import json\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 1: CARGAR DATOS DESDE GOLD\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 1: CARGA DE DATOS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "GOLD_PATH = \"/app/notebooks/delta_lake/gold_features_v2\"\n",
    "\n",
    "print(f\"ðŸ“Š Cargando: {GOLD_PATH}\")\n",
    "df_gold = spark.read.format(\"delta\").load(GOLD_PATH)\n",
    "\n",
    "df_gold = df_gold.cache()\n",
    "total_records = df_gold.count()\n",
    "\n",
    "print(f\"âœ“ Registros cargados: {total_records:,}\")\n",
    "print(f\"âœ“ Columnas: {len(df_gold.columns)}\\n\")\n",
    "\n",
    "print(\"ðŸ“Š Features disponibles:\")\n",
    "feature_cols = [c for c in df_gold.columns if \"features\" in c]\n",
    "\n",
    "for i, c in enumerate(feature_cols, 1):\n",
    "    sample_dim = len(df_gold.select(c).first()[0])\n",
    "    print(f\"  {i}. {c}: {sample_dim} dims\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 2: SELECCIÃ“N DE FEATURES Y TARGET\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 2: PREPARACIÃ“N DE DATOS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "FEATURE_COL = \"features_scaled_filtered\"\n",
    "TARGET_COL = \"valor_contrato\"\n",
    "\n",
    "print(f\"ðŸ“Š Features seleccionadas: {FEATURE_COL}\")\n",
    "print(f\"ðŸ“Š Target: {TARGET_COL}\\n\")\n",
    "\n",
    "df_model = df_gold.select(\n",
    "    col(FEATURE_COL).alias(\"features\"),\n",
    "    col(TARGET_COL).alias(\"label\"),\n",
    "    \"id_contrato\",\n",
    "    \"fecha_firma\",\n",
    "    \"entidad\"\n",
    ").filter(\n",
    "    col(\"features\").isNotNull() &\n",
    "    col(\"label\").isNotNull() &\n",
    "    (col(\"label\") > 0)\n",
    ")\n",
    "\n",
    "df_model = df_model.cache()\n",
    "total_model = df_model.count()\n",
    "\n",
    "print(f\"âœ“ Registros vÃ¡lidos: {total_model:,}\")\n",
    "print(f\"âœ“ Descartados: {total_records - total_model:,}\\n\")\n",
    "\n",
    "stats = df_model.select(\n",
    "    min(\"label\").alias(\"min\"),\n",
    "    max(\"label\").alias(\"max\"),\n",
    "    avg(\"label\").alias(\"mean\"),\n",
    "    stddev(\"label\").alias(\"std\")\n",
    ").first()\n",
    "\n",
    "print(\"ðŸ“Š EstadÃ­sticas de valor_contrato:\")\n",
    "print(f\"  Min:  ${stats['min']:,.2f}\")\n",
    "print(f\"  Max:  ${stats['max']:,.2f}\")\n",
    "print(f\"  Mean: ${stats['mean']:,.2f}\")\n",
    "print(f\"  Std:  ${stats['std']:,.2f}\\n\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 3: TRAIN/TEST SPLIT (CORREGIDO)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 3: DIVISIÃ“N TRAIN/TEST (CORREGIDO PARA FECHAS)\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Convertimos fecha a nÃºmero para calcular percentil\n",
    "df_temp = df_model.withColumn(\n",
    "    \"fecha_num\",\n",
    "    col(\"fecha_firma\").cast(\"timestamp\").cast(\"long\")\n",
    ")\n",
    "\n",
    "q = df_temp.approxQuantile(\"fecha_num\", [0.8], 0.01)\n",
    "split_ts = q[0]\n",
    "split_date = datetime.utcfromtimestamp(split_ts)\n",
    "\n",
    "print(f\"ðŸ“… Fecha de corte (percentil 80): {split_date}\\n\")\n",
    "\n",
    "train_data = df_model.filter(col(\"fecha_firma\") <= split_date).cache()\n",
    "test_data  = df_model.filter(col(\"fecha_firma\") > split_date).cache()\n",
    "\n",
    "print(f\"Train: {train_data.count():,}\")\n",
    "print(f\"Test : {test_data.count():,}\\n\")\n",
    "\n",
    "# Guardar tamaÃ±os\n",
    "train_size = train_data.count()\n",
    "test_size  = test_data.count()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 4: REGRESIÃ“N LINEAL\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 4: MODELO BASELINE - REGRESIÃ“N LINEAL\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "lr = LinearRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    maxIter=100,\n",
    "    regParam=0.1,\n",
    "    elasticNetParam=0.8\n",
    ")\n",
    "\n",
    "lr_model = lr.fit(train_data)\n",
    "lr_predictions = lr_model.transform(test_data)\n",
    "\n",
    "# Evaluadores\n",
    "evaluator_rmse = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "evaluator_r2   = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "evaluator_mae  = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "\n",
    "lr_rmse = evaluator_rmse.evaluate(lr_predictions)\n",
    "lr_r2   = evaluator_r2.evaluate(lr_predictions)\n",
    "lr_mae  = evaluator_mae.evaluate(lr_predictions)\n",
    "\n",
    "# Sigma\n",
    "lr_with_error = lr_predictions.withColumn(\"error\", col(\"prediction\") - col(\"label\"))\n",
    "sigma = lr_with_error.select(stddev(\"error\").alias(\"sigma\")).first()[\"sigma\"]\n",
    "\n",
    "print(\"ðŸ“Š RESULTADOS - LINEAR REGRESSION:\")\n",
    "print(f\"  RMSE:  ${lr_rmse:,.2f}\")\n",
    "print(f\"  MAE:   ${lr_mae:,.2f}\")\n",
    "print(f\"  RÂ²:    {lr_r2:.4f}\")\n",
    "print(f\"  Sigma: ${sigma:,.2f}\\n\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 9: GUARDAR MODELO EN MLFLOW\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 9: GUARDAR MODELO CON MLFLOW\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "MODELS_PATH = \"/app/notebooks/models_v2\"\n",
    "mlflow.set_tracking_uri(\"http://172.17.0.1:5000\")\n",
    "mlflow.set_experiment(\"contract_value_regression\")\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "with mlflow.start_run(run_name=\"linear_regression_v1\"):\n",
    "\n",
    "    print(\"ðŸ“Š Registrando parÃ¡metros...\")\n",
    "    mlflow.log_param(\"model_type\", \"LinearRegression\")\n",
    "    mlflow.log_param(\"regParam\", 0.1)\n",
    "    mlflow.log_param(\"elasticNetParam\", 0.8)\n",
    "    mlflow.log_param(\"maxIter\", 100)\n",
    "    mlflow.log_param(\"train_size\", train_size)\n",
    "    mlflow.log_param(\"test_size\", test_size)\n",
    "    mlflow.log_param(\"random_seed\", RANDOM_SEED)\n",
    "    print(\"âœ“ ParÃ¡metros registrados\\n\")\n",
    "\n",
    "    print(\"ðŸ“Š Registrando mÃ©tricas...\")\n",
    "    mlflow.log_metric(\"test_rmse\", lr_rmse)\n",
    "    mlflow.log_metric(\"test_r2\", lr_r2)\n",
    "    mlflow.log_metric(\"test_mae\", lr_mae)\n",
    "    mlflow.log_metric(\"sigma\", sigma)\n",
    "    mlflow.log_metric(\"anomaly_threshold\", 2.8 * sigma)\n",
    "    print(\"âœ“ MÃ©tricas registradas\\n\")\n",
    "\n",
    "    print(\"ðŸ“Š Guardando modelo en MLflow...\")\n",
    "    mlflow.spark.log_model(\n",
    "        spark_model=lr_model,\n",
    "        artifact_path=\"linear_regression_model\",\n",
    "        registered_model_name=\"contract_value_predictor\"\n",
    "    )\n",
    "    print(\"âœ“ Modelo registrado\\n\")\n",
    "\n",
    "    # Guardar sigma como artifact\n",
    "    sigma_path_temp = \"/tmp/sigma.txt\"\n",
    "    with open(sigma_path_temp, \"w\") as f:\n",
    "        f.write(str(sigma))\n",
    "    mlflow.log_artifact(sigma_path_temp, \"model_artifacts\")\n",
    "\n",
    "    # Guardar metadatos\n",
    "    metadata = {\n",
    "        \"model_type\": \"LinearRegression\",\n",
    "        \"regParam\": 0.1,\n",
    "        \"elasticNetParam\": 0.8,\n",
    "        \"maxIter\": 100,\n",
    "        \"train_size\": train_size,\n",
    "        \"test_size\": test_size,\n",
    "        \"test_rmse\": float(lr_rmse),\n",
    "        \"test_r2\": float(lr_r2),\n",
    "        \"test_mae\": float(lr_mae),\n",
    "        \"sigma\": float(sigma),\n",
    "        \"anomaly_threshold\": float(2.8 * sigma)\n",
    "    }\n",
    "\n",
    "    metadata_path_temp = \"/tmp/model_metadata.json\"\n",
    "    with open(metadata_path_temp, \"w\") as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "\n",
    "    mlflow.log_artifact(metadata_path_temp, \"model_artifacts\")\n",
    "\n",
    "    run_id = mlflow.active_run().info.run_id\n",
    "    print(f\"ðŸ“Š MLflow Run ID: {run_id}\\n\")\n",
    "\n",
    "print(\"âœ… Modelo guardado exitosamente en MLflow\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e4c3fb-f2b1-4929-998c-bc89a69626ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
